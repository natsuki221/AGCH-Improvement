# äº”æŠ˜äº¤å‰é©—è­‰å¿«é€ŸåŸ·è¡ŒæŒ‡å—

> **é€™æ˜¯ç°¡åŒ–ç‰ˆæŒ‡å—ï¼Œå®Œæ•´æ–‡æª”è«‹è¦‹ `siglip2_multimodal_hash_5fold_cv_plan_v2.3_AGCH.md`**

---

## ğŸš€ å¿«é€Ÿé–‹å§‹ï¼ˆ5 æ­¥é©Ÿï¼‰

### æ­¥é©Ÿ 1: ç”Ÿæˆ Fold åˆ‡åˆ†

```bash
cd ~/Documents/Coding/github.com/natsuki221/AGCH-Improvement

# ç”Ÿæˆ 5-fold split
python scripts/create_kfold_split.py

# é©—è­‰è¼¸å‡º
cat data/coco/5fold_split.json | python -m json.tool | head -20
```

**é æœŸè¼¸å‡º**:

```
é–‹ç™¼é›†æ± ç¸½æ•¸: 118287
æ¸¬è©¦é›†ç¸½æ•¸: 5000 (å·²æ’é™¤)
 - fold_0: Train=94,630, Val=23,657
 - fold_1: Train=94,630, Val=23,657
 ...
âœ“ å·²å„²å­˜è‡³: data/coco/5fold_split.json
```

---

### æ­¥é©Ÿ 2: å»ºç«‹é…ç½®æª”æ¡ˆ

```bash
# å»ºç«‹ç›®éŒ„
mkdir -p configs/experiments

# å‰µå»º configs/experiments/cv_experiment.yaml
cat > configs/experiments/cv_experiment.yaml << 'EOF'
defaults:
  - /hardware/rtx5080_16gb

experiment:
  name: "cv_baseline"
  seed: 42

k_fold:
  enabled: true
  num_folds: 5
  current_fold: 0

training:
  batch_size: 32
  gradient_accumulation_steps: 2
  num_epochs: 20  # å¾ 30 é™åˆ° 20
  warmup_epochs: 1
  early_stopping_patience: 3  # å¾ 5 é™åˆ° 3
  val_every_n_epochs: 1

checkpointing:
  save_dir: "./outputs/checkpoints"
  save_top_k: 1  # åªå„²å­˜æœ€ä½³
  save_optimizer: false  # ä¸å„²å­˜ optimizerï¼ˆç¯€çœç©ºé–“ï¼‰
  filename_format: "best_model_mAP{val_mAP:.4f}.pth"

logging:
  use_wandb: true
  wandb_project: "siglip2-5fold-cv"
  wandb_entity: "natsuki221"
EOF
```

---

### æ­¥é©Ÿ 3: æ¸¬è©¦å–®å€‹ Foldï¼ˆé‡è¦ï¼ï¼‰

```bash
# è©¦è·‘ 1 å€‹ epochï¼Œç¢ºä¿ä¸€åˆ‡æ­£å¸¸
python scripts/train.py \
    --config-name cv_experiment \
    k_fold.enabled=true \
    k_fold.current_fold=0 \
    training.num_epochs=1

# æª¢æŸ¥è¼¸å‡ºï¼š
# âœ“ GPU è¨˜æ†¶é«”æ‡‰åœ¨ 10-11 GB
# âœ“ æ¯å€‹ epoch ç´„ 13-15 åˆ†é˜
# âœ“ Loss æ­£å¸¸ä¸‹é™
```

**å¦‚æœæˆåŠŸ**ï¼Œä½ æœƒçœ‹åˆ°ï¼š

```
Epoch 1/1
Train - Loss: 0.4523 (BCE: 0.3821, Cos: 0.0512, Hash: 0.0190)
Val   - Loss: 0.4123, mAP: 0.3245, F1-Micro: 0.4567
âœ“ å„²å­˜æœ€ä½³æ¨¡å‹
```

---

### æ­¥é©Ÿ 4: åŸ·è¡Œå®Œæ•´ 5-Fold

```bash
# è³¦äºˆåŸ·è¡Œæ¬Šé™
chmod +x scripts/run_5fold_cv.sh

# ä½¿ç”¨ tmuxï¼ˆæ¨è–¦ï¼Œå› ç‚ºè¦è·‘å¾ˆä¹…ï¼‰
tmux new -s cv_training

# åœ¨ tmux ä¸­åŸ·è¡Œ
./scripts/run_5fold_cv.sh

# åˆ†é›¢ tmux: æŒ‰ Ctrl+Bï¼Œç„¶å¾ŒæŒ‰ D
# é‡æ–°é€£æ¥: tmux attach -t cv_training
```

**é æœŸæ™‚é•·**: ç´„ 17-22 å°æ™‚ï¼ˆå–æ±ºæ–¼ early stoppingï¼‰

---

### æ­¥é©Ÿ 5: èšåˆçµæœ

```bash
# åŸ·è¡Œçµæœèšåˆ
python scripts/aggregate_cv_results.py \
    --exp_prefix siglip2_cv_run1_fold

# æŸ¥çœ‹çµæœ
cat outputs/checkpoints/cv_results_summary.json
```

**é æœŸè¼¸å‡º**:

```
ğŸ† 5-Fold Cross-Validation æœ€çµ‚çµæœ (mAP)
============================================================
Mean: 0.7193
Std:  0.0082
Min:  0.7098
Max:  0.7311
Median: 0.7189
============================================================

ğŸ“ è«–æ–‡å ±å‘Šæ ¼å¼:
   mAP: 0.72 Â± 0.01
============================================================
```

---

## ğŸ“Š é—œéµæŒ‡æ¨™å°æ¯”

| é …ç›® | å–®æ¬¡è¨“ç·´ (v2.2) | äº”æŠ˜é©—è­‰ (v2.3) |
| ------ | ---------------- | ---------------- |
| **è¨“ç·´æ¬¡æ•¸** | 1 | **5** |
| **ç¸½æ™‚é•·** | ~17.5 å°æ™‚ | **~20 å°æ™‚** |
| **çµæœå½¢å¼** | mAP: 0.72 | **mAP: 0.72 Â± 0.01** |
| **è«–æ–‡åƒ¹å€¼** | ä¸­ç­‰ | **é«˜ï¼ˆé ‚æœƒæ¨™æº–ï¼‰** |
| **ç¡¬ç¢Ÿéœ€æ±‚** | ~5 GB | **~4 GB**ï¼ˆç²¾ç°¡å„²å­˜ï¼‰ |

---

## âš ï¸ æ•…éšœæ’é™¤

### å•é¡Œ 1: FileNotFoundError: 5fold_split.json

**è§£æ±º**: åŸ·è¡Œ `python scripts/create_kfold_split.py`

### å•é¡Œ 2: OOM (è¨˜æ†¶é«”ä¸è¶³)

**è§£æ±º**: é™ä½ batch size

```yaml
# åœ¨ cv_experiment.yaml ä¸­
training:
  batch_size: 16  # å¾ 32 é™åˆ° 16
  gradient_accumulation_steps: 4  # å¾ 2 å¢åˆ° 4
```

### å•é¡Œ 3: è¨“ç·´é€Ÿåº¦æ…¢

**æª¢æŸ¥**:

```bash
nvidia-smi  # GPU åˆ©ç”¨ç‡æ‡‰ > 80%
```

**è§£æ±º**: å¢åŠ  workers

```yaml
dataloader:
  num_workers: 20  # å¾ 16 å¢åˆ° 20
```

---

## ğŸ“ é‡è¦æª”æ¡ˆä½ç½®

```
AGCH-Improvement/
â”œâ”€â”€ data/coco/
â”‚   â””â”€â”€ 5fold_split.json          â† create_kfold_split.py ç”Ÿæˆ
â”‚
â”œâ”€â”€ configs/experiments/
â”‚   â””â”€â”€ cv_experiment.yaml         â† æ‰‹å‹•å‰µå»º
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_kfold_split.py      â† å®Œæ•´æ–‡æª”ä¸­æä¾›
â”‚   â”œâ”€â”€ run_5fold_cv.sh            â† å®Œæ•´æ–‡æª”ä¸­æä¾›
â”‚   â”œâ”€â”€ aggregate_cv_results.py    â† å®Œæ•´æ–‡æª”ä¸­æä¾›
â”‚   â””â”€â”€ train.py                   â† éœ€è¦å°å¹…ä¿®æ”¹
â”‚
â””â”€â”€ outputs/checkpoints/
    â”œâ”€â”€ siglip2_cv_run1_fold0/
    â”œâ”€â”€ siglip2_cv_run1_fold1/
    â”œâ”€â”€ ...
    â””â”€â”€ cv_results_summary.json    â† æœ€çµ‚çµæœ
```

---

## ğŸ¯ è«–æ–‡å ±å‘Šç¯„ä¾‹

```
We evaluate our method using 5-fold cross-validation on the 
MS-COCO dataset (118,287 development images). The results 
demonstrate strong and stable performance:

Results (5-Fold CV):
  - mAP (macro): 0.72 Â± 0.01
  - F1-score (micro): 0.74 Â± 0.01
  - F1-score (macro): 0.65 Â± 0.02

The low standard deviation (< 0.02) across all folds indicates 
robust generalization, validating the effectiveness of our 
approach.
```

---

## ğŸ“ éœ€è¦å¹«åŠ©ï¼Ÿ

è©³ç´°èªªæ˜è«‹åƒè€ƒï¼š

- **å®Œæ•´è¨ˆç•«**: `siglip2_multimodal_hash_5fold_cv_plan_v2.3_AGCH.md`
- **ç« ç¯€ 6**: å®Œæ•´ç¨‹å¼ç¢¼å¯¦ä½œ
- **ç« ç¯€ 10**: è©³ç´°æ•…éšœæ’é™¤

ç¥å¯¦é©—é †åˆ©ï¼ğŸš€
