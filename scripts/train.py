# scripts/train.py
"""
è¨“ç·´è…³æœ¬ - AGCH-Improvement
æ”¯æ´ RTX 5080 16GBï¼Œä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´èˆ‡æ¢¯åº¦ç´¯ç©
"""

import os
import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

import torch
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
import hydra
from omegaconf import DictConfig, OmegaConf
from tqdm import tqdm
import numpy as np
from sklearn.metrics import (
    average_precision_score,
    f1_score,
    roc_auc_score,
    precision_score,
    recall_score,
    hamming_loss,
    coverage_error,
    label_ranking_loss,
    label_ranking_average_precision_score,
)

# æœ¬å°ˆæ¡ˆæ¨¡çµ„
from siglip2_multimodal_hash.model import MultimodalHashKNN
from siglip2_multimodal_hash.dataset import create_dataloader
from siglip2_multimodal_hash.losses import compute_total_loss
from siglip2_multimodal_hash.utils import set_seed, get_gpu_memory_info


def train_epoch(model, train_loader, optimizer, scheduler, scaler, config):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()

    total_losses = {"total": 0, "bce": 0, "cos": 0, "hash": 0}
    num_batches = 0
    accumulation_steps = config.training.gradient_accumulation_steps

    optimizer.zero_grad()

    pbar = tqdm(train_loader, desc="Training")

    for batch_idx, batch in enumerate(pbar):
        # ç§»å‹•è³‡æ–™åˆ° GPU
        pixel_values = batch["pixel_values"].cuda()
        input_ids = batch["input_ids"].cuda()
        attention_mask = batch.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.cuda()
        labels = batch["labels"].cuda()

        # æ··åˆç²¾åº¦å‰å‘å‚³æ’­
        with autocast(enabled=config.memory_optimization.mixed_precision):
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_components=True,
            )

            loss_dict = compute_total_loss(outputs=outputs, labels=labels, config=config.loss)

            loss = loss_dict["total"] / accumulation_steps

        # åå‘å‚³æ’­
        scaler.scale(loss).backward()

        # æ¢¯åº¦ç´¯ç©
        if (batch_idx + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip_norm)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            if scheduler is not None:
                scheduler.step()

        # ç´¯è¨ˆæå¤±
        total_losses["total"] += loss_dict["total"].item()
        total_losses["bce"] += loss_dict["bce"].item()
        total_losses["cos"] += loss_dict["cos"].item()
        total_losses["hash"] += loss_dict["hash"].item()
        num_batches += 1

        # æ›´æ–°é€²åº¦æ¢
        pbar.set_postfix(
            {
                "loss": f"{loss_dict['total'].item():.4f}",
                "GPU": f"{get_gpu_memory_info()['allocated_gb']:.1f}GB",
            }
        )

    # å¹³å‡æå¤±
    for key in total_losses:
        total_losses[key] /= num_batches

    return total_losses


@torch.no_grad()
def validate(model, val_loader, config):
    """é©—è­‰æ¨¡å‹ï¼Œè¨ˆç®—å¤šæ¨™ç±¤åˆ†é¡å¸¸ç”¨æŒ‡æ¨™"""
    model.eval()

    all_preds = []
    all_labels = []
    total_loss = 0
    num_batches = 0

    for batch in tqdm(val_loader, desc="Validation"):
        # ç§»å‹•è³‡æ–™åˆ° GPU
        pixel_values = batch["pixel_values"].cuda()
        input_ids = batch["input_ids"].cuda()
        attention_mask = batch.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.cuda()
        labels = batch["labels"].cuda()

        # å‰å‘å‚³æ’­
        with autocast(enabled=config.memory_optimization.mixed_precision):
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_components=True,
            )

            loss_dict = compute_total_loss(outputs=outputs, labels=labels, config=config.loss)

        total_loss += loss_dict["total"].item()
        num_batches += 1

        # æ”¶é›†é æ¸¬
        probs = torch.sigmoid(outputs["logits"]).cpu().numpy()
        all_preds.append(probs)
        all_labels.append(labels.cpu().numpy())

    # åˆä½µçµæœ
    all_preds = np.concatenate(all_preds, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    pred_binary = (all_preds > 0.5).astype(int)

    # ============================================================
    # å¤šæ¨™ç±¤åˆ†é¡ç ”ç©¶å¸¸ç”¨æŒ‡æ¨™
    # ============================================================

    # 1. Mean Average Precision (mAP) - ä¸»è¦æŒ‡æ¨™
    mAP = average_precision_score(all_labels, all_preds, average="macro")

    # 2. AUC-ROC (macro/micro)
    try:
        auc_macro = roc_auc_score(all_labels, all_preds, average="macro")
        auc_micro = roc_auc_score(all_labels, all_preds, average="micro")
    except ValueError:
        # æŸäº›æ¨™ç±¤å¯èƒ½æ²’æœ‰æ­£ä¾‹
        auc_macro = 0.0
        auc_micro = 0.0

    # 3. F1 Scores
    f1_micro = f1_score(all_labels, pred_binary, average="micro", zero_division=0)
    f1_macro = f1_score(all_labels, pred_binary, average="macro", zero_division=0)

    # 4. Precision & Recall
    precision_micro = precision_score(all_labels, pred_binary, average="micro", zero_division=0)
    precision_macro = precision_score(all_labels, pred_binary, average="macro", zero_division=0)
    recall_micro = recall_score(all_labels, pred_binary, average="micro", zero_division=0)
    recall_macro = recall_score(all_labels, pred_binary, average="macro", zero_division=0)

    # 5. Hamming Loss (è¶Šä½è¶Šå¥½)
    h_loss = hamming_loss(all_labels, pred_binary)

    # 6. Ranking æŒ‡æ¨™
    try:
        # Coverage Error: å¹³å‡éœ€è¦åŒ…å«å¤šå°‘æ¨™ç±¤æ‰èƒ½æ¶µè“‹æ‰€æœ‰çœŸå¯¦æ¨™ç±¤
        cov_error = coverage_error(all_labels, all_preds)
        # Label Ranking Loss: æ’åºæå¤± (è¶Šä½è¶Šå¥½)
        rank_loss = label_ranking_loss(all_labels, all_preds)
        # Label Ranking Average Precision (LRAP)
        lrap = label_ranking_average_precision_score(all_labels, all_preds)
    except ValueError:
        cov_error = 0.0
        rank_loss = 0.0
        lrap = 0.0

    # 7. Mean Absolute Error (é æ¸¬æ©Ÿç‡èˆ‡çœŸå¯¦æ¨™ç±¤å·®è·)
    mae = np.mean(np.abs(all_preds - all_labels))

    return {
        # ä¸»è¦æŒ‡æ¨™
        "loss": total_loss / num_batches,
        "mAP": mAP,
        # AUC
        "auc_macro": auc_macro,
        "auc_micro": auc_micro,
        # F1
        "f1_micro": f1_micro,
        "f1_macro": f1_macro,
        # Precision & Recall
        "precision_micro": precision_micro,
        "precision_macro": precision_macro,
        "recall_micro": recall_micro,
        "recall_macro": recall_macro,
        # å…¶ä»–
        "hamming_loss": h_loss,
        "coverage_error": cov_error,
        "ranking_loss": rank_loss,
        "lrap": lrap,
        "mae": mae,
    }


@hydra.main(config_path="../configs", config_name="hardware/rtx5080_16gb", version_base=None)
def main(raw_config: DictConfig):
    """ä¸»è¨“ç·´å‡½æ•¸"""
    # Hydra æœƒå°‡é…ç½®åŒ…åœ¨è³‡æ–™å¤¾åä¸‹ï¼Œéœ€è¦è§£é–‹
    if "hardware" in raw_config:
        config = raw_config.hardware
    elif "experiments" in raw_config:
        config = raw_config.experiments
    else:
        config = raw_config

    print("=" * 60)
    print("AGCH-Improvement è¨“ç·´è…³æœ¬")
    print("=" * 60)

    # é¡¯ç¤ºé…ç½®
    print(f"\nğŸ“‹ å¯¦é©—: {config.experiment.name}")
    print(
        f"ğŸ“‹ Batch size: {config.training.batch_size} x {config.training.gradient_accumulation_steps} = {config.training.batch_size * config.training.gradient_accumulation_steps}"
    )
    print(f"ğŸ“‹ Epochs: {config.training.num_epochs}")

    # è¨­å®š seed
    set_seed(config.experiment.seed)

    # æª¢æŸ¥ CUDA
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA ä¸å¯ç”¨ï¼æ­¤è…³æœ¬éœ€è¦ GPU")

    print(f"\nğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ–¥ï¸  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # åˆå§‹åŒ– wandb (å¯é¸)
    if config.logging.use_wandb:
        try:
            import wandb

            wandb.init(
                project=config.logging.wandb_project,
                entity=(
                    config.logging.wandb_entity
                    if config.logging.wandb_entity != "your-username"
                    else None
                ),
                config=OmegaConf.to_container(config, resolve=True),
                name=config.experiment.name,
            )
            use_wandb = True
        except Exception as e:
            print(f"âš ï¸  Wandb åˆå§‹åŒ–å¤±æ•—: {e}")
            use_wandb = False
    else:
        use_wandb = False

    # å»ºç«‹æ¨¡å‹
    print("\nğŸ“¦ å»ºç«‹æ¨¡å‹...")
    model = MultimodalHashKNN(config.model).cuda()

    # é¡¯ç¤ºè¨˜æ†¶é«”è³‡è¨Š
    mem_info = get_gpu_memory_info()
    print(
        f"âœ“ æ¨¡å‹è¼‰å…¥å¾Œ GPU è¨˜æ†¶é«”: {mem_info['allocated_gb']:.2f}GB / {mem_info['total_gb']:.1f}GB"
    )

    # è¨ˆç®—å¯è¨“ç·´åƒæ•¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(
        f"âœ“ å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)"
    )

    # å»ºç«‹ DataLoader
    print("\nğŸ“‚ å»ºç«‹ DataLoader...")

    # æª¢æŸ¥æ˜¯å¦ç‚º K-Fold æ¨¡å¼
    use_k_fold = config.get("k_fold", {}).get("enabled", False)
    fold_idx = config.get("k_fold", {}).get("current_fold", None) if use_k_fold else None

    if use_k_fold:
        print(f"ğŸ“‹ K-Fold æ¨¡å¼: Fold {fold_idx}")

    train_loader = create_dataloader(config, split="train", fold_idx=fold_idx)
    val_loader = create_dataloader(config, split="val", fold_idx=fold_idx)
    print(f"âœ“ è¨“ç·´é›†: {len(train_loader)} batches")
    print(f"âœ“ é©—è­‰é›†: {len(val_loader)} batches")

    # å»ºç«‹ optimizer
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.optimizer.lr,
        weight_decay=config.optimizer.weight_decay,
        betas=tuple(config.optimizer.betas),
    )

    # å»ºç«‹ scheduler
    total_steps = (
        len(train_loader)
        * config.training.num_epochs
        // config.training.gradient_accumulation_steps
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=total_steps, eta_min=config.scheduler.min_lr
    )

    # æ··åˆç²¾åº¦ scaler
    scaler = GradScaler(enabled=config.memory_optimization.mixed_precision)

    # å»ºç«‹å„²å­˜ç›®éŒ„ - K-Fold æ¨¡å¼ä½¿ç”¨å¯¦é©—åç¨±ä½œç‚ºå­ç›®éŒ„
    base_save_dir = Path("outputs/checkpoints")
    exp_name = config.experiment.name
    save_dir = base_save_dir / exp_name
    save_dir.mkdir(parents=True, exist_ok=True)

    # è¨“ç·´è¿´åœˆ
    best_val_map = 0
    patience_counter = 0

    print("\nğŸš€ é–‹å§‹è¨“ç·´...")
    for epoch in range(config.training.num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{config.training.num_epochs}")
        print(f"{'='*60}")

        # è¨“ç·´
        train_losses = train_epoch(model, train_loader, optimizer, scheduler, scaler, config)
        print(
            f"Train Loss: {train_losses['total']:.4f} "
            f"(BCE: {train_losses['bce']:.4f}, "
            f"Cos: {train_losses['cos']:.4f}, "
            f"Hash: {train_losses['hash']:.4f})"
        )

        # é©—è­‰
        val_metrics = validate(model, val_loader, config)
        print(
            f"Val Loss: {val_metrics['loss']:.4f}, "
            f"mAP: {val_metrics['mAP']:.4f}, "
            f"AUC: {val_metrics['auc_macro']:.4f}, "
            f"F1-Macro: {val_metrics['f1_macro']:.4f}"
        )
        print(
            f"     Precision: {val_metrics['precision_macro']:.4f}, "
            f"Recall: {val_metrics['recall_macro']:.4f}, "
            f"Hamming: {val_metrics['hamming_loss']:.4f}, "
            f"MAE: {val_metrics['mae']:.4f}"
        )

        # è¨˜éŒ„åˆ° wandb
        if use_wandb:
            wandb.log(
                {
                    "epoch": epoch,
                    "train/loss": train_losses["total"],
                    "train/loss_bce": train_losses["bce"],
                    "train/loss_cos": train_losses["cos"],
                    "train/loss_hash": train_losses["hash"],
                    "val/loss": val_metrics["loss"],
                    "val/mAP": val_metrics["mAP"],
                    "val/auc_macro": val_metrics["auc_macro"],
                    "val/auc_micro": val_metrics["auc_micro"],
                    "val/f1_micro": val_metrics["f1_micro"],
                    "val/f1_macro": val_metrics["f1_macro"],
                    "val/precision_macro": val_metrics["precision_macro"],
                    "val/recall_macro": val_metrics["recall_macro"],
                    "val/hamming_loss": val_metrics["hamming_loss"],
                    "val/ranking_loss": val_metrics["ranking_loss"],
                    "val/lrap": val_metrics["lrap"],
                    "val/mae": val_metrics["mae"],
                    "lr": optimizer.param_groups[0]["lr"],
                    "gpu_memory_gb": get_gpu_memory_info()["allocated_gb"],
                }
            )

        # å„²å­˜æœ€ä½³æ¨¡å‹
        if val_metrics["mAP"] > best_val_map:
            best_val_map = val_metrics["mAP"]
            patience_counter = 0

            checkpoint = {
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict(),
                "val_metrics": val_metrics,  # æ‰€æœ‰è©•ä¼°æŒ‡æ¨™
                "val_mAP": val_metrics["mAP"],  # å‘å¾Œå…¼å®¹
                "config": OmegaConf.to_container(config, resolve=True),
            }
            checkpoint_path = save_dir / "best_model.pth"
            torch.save(checkpoint, checkpoint_path)
            print(f"âœ“ å„²å­˜æœ€ä½³æ¨¡å‹: {checkpoint_path} (mAP: {val_metrics['mAP']:.4f})")
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= config.training.early_stopping_patience:
            print(f"\nâš ï¸  Early stopping triggered after {epoch+1} epochs")
            break

    print("\n" + "=" * 60)
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print(f"æœ€ä½³ Val mAP: {best_val_map:.4f}")
    print("=" * 60)

    if use_wandb:
        wandb.finish()


if __name__ == "__main__":
    main()
