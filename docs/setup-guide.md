# SigLIP2 å¤šæ¨¡æ…‹åˆ†é¡å¯¦é©—ç’°å¢ƒè¨­ç½®å®Œæ•´æ•™å­¸

> **ä½¿ç”¨å·¥å…·**: `uv` - è¶…å¿«é€Ÿçš„ Python å¥—ä»¶ç®¡ç†å™¨  
> **ç›®æ¨™**: å»ºç«‹å¯é‡ç¾ã€éš”é›¢ã€é«˜æ•ˆçš„å¯¦é©—ç’°å¢ƒ

---

## ç›®éŒ„

1. [ç³»çµ±éœ€æ±‚æª¢æŸ¥](#1-ç³»çµ±éœ€æ±‚æª¢æŸ¥)
2. [å®‰è£ uv](#2-å®‰è£-uv)
3. [å°ˆæ¡ˆåˆå§‹åŒ–](#3-å°ˆæ¡ˆåˆå§‹åŒ–)
4. [ç’°å¢ƒé…ç½®èˆ‡ä¾è³´å®‰è£](#4-ç’°å¢ƒé…ç½®èˆ‡ä¾è³´å®‰è£)
5. [è³‡æ–™é›†ä¸‹è¼‰èˆ‡è™•ç†](#5-è³‡æ–™é›†ä¸‹è¼‰èˆ‡è™•ç†)
6. [é©—è­‰ç’°å¢ƒ](#6-é©—è­‰ç’°å¢ƒ)
7. [å¸¸è¦‹å•é¡Œæ’é™¤](#7-å¸¸è¦‹å•é¡Œæ’é™¤)

---

## 1) ç³»çµ±éœ€æ±‚æª¢æŸ¥

### 1.1 ç¡¬é«”éœ€æ±‚

åœ¨é–‹å§‹ä¹‹å‰ï¼Œè«‹ç¢ºèªä½ çš„ç³»çµ±æ»¿è¶³ä»¥ä¸‹éœ€æ±‚ï¼š

```bash
# æª¢æŸ¥ GPU
nvidia-smi

# æ‡‰è©²çœ‹åˆ°é¡ä¼¼è¼¸å‡ºï¼š
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 12.2   |
# | -------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
# | 30%   45C    P8    25W / 320W |    500MiB / 24576MiB |      0%      Default |
```

**æœ€ä½éœ€æ±‚**:

- âœ… GPU: NVIDIA RTX 3090 æˆ–æ›´é«˜ (24GB VRAM)
- âœ… CPU: 8 æ ¸å¿ƒæˆ–æ›´å¤š
- âœ… RAM: 32 GB æˆ–æ›´å¤š
- âœ… Storage: è‡³å°‘ 100 GB å¯ç”¨ç©ºé–“ (å»ºè­° SSD)

### 1.2 è»Ÿé«”éœ€æ±‚

```bash
# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# æ‡‰è©²é¡¯ç¤º CUDA 11.8 æˆ–æ›´é«˜
# å¦‚æœæ²’æœ‰ nvccï¼Œæª¢æŸ¥ï¼š
cat /usr/local/cuda/version.txt

# æª¢æŸ¥ Python ç‰ˆæœ¬
python3 --version
# éœ€è¦ Python 3.10 æˆ–æ›´é«˜ï¼ˆå»ºè­° 3.11ï¼‰
```

**å¦‚æœ Python ç‰ˆæœ¬ä¸ç¬¦åˆ**:

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-dev

# æˆ–ä½¿ç”¨ pyenv (æ¨è–¦)
curl https://pyenv.run | bash
pyenv install 3.11.7
pyenv global 3.11.7
```

---

## 2) å®‰è£ uv

### 2.1 ä»€éº¼æ˜¯ uvï¼Ÿ

`uv` æ˜¯ç”± Astral (Ruff çš„é–‹ç™¼è€…) æ¨å‡ºçš„è¶…å¿«é€Ÿ Python å¥—ä»¶ç®¡ç†å™¨ï¼š

- âš¡ æ¯” pip å¿« 10-100 å€
- ğŸ”’ å…§å»ºä¾è³´è§£æèˆ‡é–å®š
- ğŸ“¦ è‡ªå‹•ç®¡ç†è™›æ“¬ç’°å¢ƒ
- ğŸ¯ å®Œå…¨ç›¸å®¹ pip/requirements.txt

### 2.2 å®‰è£ uv

```bash
# æ–¹æ³• 1: ä½¿ç”¨å®˜æ–¹å®‰è£è…³æœ¬ (æ¨è–¦)
curl -LsSf https://astral.sh/uv/install.sh | sh

# æ–¹æ³• 2: ä½¿ç”¨ pip (å¦‚æœå·²æœ‰ Python)
pip install uv

# æ–¹æ³• 3: ä½¿ç”¨ cargo (å¦‚æœä½ æ˜¯ Rust é–‹ç™¼è€…)
cargo install --git https://github.com/astral-sh/uv uv
```

### 2.3 é©—è­‰å®‰è£

```bash
# æª¢æŸ¥ç‰ˆæœ¬
uv --version
# æ‡‰è©²é¡¯ç¤º: uv 0.1.x æˆ–æ›´é«˜

# æŸ¥çœ‹å¹«åŠ©
uv --help
```

### 2.4 é…ç½® uv (å¯é¸ä½†æ¨è–¦)

```bash
# è¨­å®šé¡åƒæºåŠ é€Ÿä¸‹è¼‰ (ä¸­åœ‹ç”¨æˆ¶)
export UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple

# æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æ°¸ä¹…è¨­å®š
mkdir -p ~/.config/uv
cat > ~/.config/uv/uv.toml << EOF
[pip]
index-url = "https://pypi.org/simple"
extra-index-url = [
    "https://download.pytorch.org/whl/cu121"
]
EOF
```

---

## 3) å°ˆæ¡ˆåˆå§‹åŒ–

### 3.1 å»ºç«‹å°ˆæ¡ˆç›®éŒ„çµæ§‹

```bash
# å»ºç«‹å°ˆæ¡ˆæ ¹ç›®éŒ„
mkdir -p ~/projects/siglip2-multimodal-hash
cd ~/projects/siglip2-multimodal-hash

# å»ºç«‹æ¨™æº–ç›®éŒ„çµæ§‹
mkdir -p {src,data,experiments,outputs,notebooks,scripts,configs,tests}

# å°ˆæ¡ˆçµæ§‹èªªæ˜ï¼š
# src/           - ä¸»è¦åŸå§‹ç¢¼
# data/          - è³‡æ–™é›†å­˜æ”¾ä½ç½®
# experiments/   - å¯¦é©—çµæœèˆ‡æ—¥èªŒ
# outputs/       - æ¨¡å‹è¼¸å‡ºèˆ‡æª¢æŸ¥é»
# notebooks/     - Jupyter notebooks (æ¢ç´¢æ€§åˆ†æ)
# scripts/       - ç¨ç«‹è…³æœ¬ (ä¸‹è¼‰ã€é è™•ç†ç­‰)
# configs/       - é…ç½®æª”æ¡ˆ (YAML)
# tests/         - å–®å…ƒæ¸¬è©¦
```

### 3.2 åˆå§‹åŒ– Git (å¼·çƒˆæ¨è–¦)

```bash
# åˆå§‹åŒ– git
git init

# å»ºç«‹ .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
.venv/
venv/
ENV/
env/

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# Jupyter
.ipynb_checkpoints/
*.ipynb

# Data (ä¸è¦ä¸Šå‚³å¤§å‹è³‡æ–™é›†)
data/raw/
data/coco/
*.zip
*.tar.gz

# Experiments
experiments/*/
outputs/checkpoints/
*.pth
*.pt
*.ckpt

# Logs
*.log
logs/
wandb/

# OS
.DS_Store
Thumbs.db
EOF

# å»ºç«‹ README
cat > README.md << 'EOF'
# SigLIP2 Multimodal Hash-based Multi-label Classification

åŸºæ–¼ SigLIP2 çš„å¤šæ¨¡æ…‹åœ–æ–‡å¤šæ¨™ç±¤åˆ†é¡ç³»çµ±ï¼Œçµåˆæ·±åº¦ hashing èˆ‡ KNN æª¢ç´¢ã€‚

## å°ˆæ¡ˆçµæ§‹
è¦‹ `docs/setup_guide.md`

## å¿«é€Ÿé–‹å§‹
è¦‹ `docs/setup_guide.md`
EOF

# é¦–æ¬¡æäº¤
git add .gitignore README.md
git commit -m "Initial commit: project structure"
```

---

## 4) ç’°å¢ƒé…ç½®èˆ‡ä¾è³´å®‰è£

### 4.1 å»ºç«‹ pyproject.toml

```bash
# å»ºç«‹ç¾ä»£åŒ–çš„ Python å°ˆæ¡ˆé…ç½®
cat > pyproject.toml << 'EOF'
[project]
name = "siglip2-multimodal-hash"
version = "0.1.0"
description = "Multimodal image-text multi-label classification using SigLIP2, hashing, and KNN"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]

dependencies = [
    # Deep Learning Framework
    "torch>=2.1.0",
    "torchvision>=0.16.0",
    
    # Transformers & Vision-Language Models
    "transformers>=4.40.0",
    "accelerate>=0.27.0",
    
    # Computer Vision
    "opencv-python>=4.9.0",
    "pillow>=10.2.0",
    "albumentations>=1.4.0",
    
    # Data Processing
    "numpy>=1.26.0",
    "pandas>=2.2.0",
    "pycocotools>=2.0.7",
    
    # Similarity Search & Indexing
    "faiss-gpu>=1.7.2",  # GPU ç‰ˆæœ¬
    # "faiss-cpu>=1.7.2",  # å¦‚æœæ²’æœ‰ GPU å‰‡ç”¨æ­¤è¡Œ
    
    # Metrics & Evaluation
    "scikit-learn>=1.4.0",
    "scipy>=1.12.0",
    
    # Visualization
    "matplotlib>=3.8.0",
    "seaborn>=0.13.0",
    "plotly>=5.18.0",
    
    # Configuration & Logging
    "hydra-core>=1.3.0",
    "omegaconf>=2.3.0",
    "wandb>=0.16.0",
    "tensorboard>=2.15.0",
    
    # Utilities
    "tqdm>=4.66.0",
    "rich>=13.7.0",
    "python-dotenv>=1.0.0",
    
    # Development Tools
    "ipython>=8.20.0",
    "jupyter>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "black>=24.0.0",
    "ruff>=0.2.0",
    "mypy>=1.8.0",
    "pre-commit>=3.6.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.black]
line-length = 100
target-version = ['py310', 'py311']

[tool.ruff]
line-length = 100
select = ["E", "F", "W", "I", "N", "UP"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false
EOF
```

### 4.2 ä½¿ç”¨ uv å»ºç«‹è™›æ“¬ç’°å¢ƒä¸¦å®‰è£ä¾è³´

```bash
# å»ºç«‹è™›æ“¬ç’°å¢ƒ (uv æœƒè‡ªå‹•ç®¡ç†)
uv venv

# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source .venv/bin/activate
# Windows: .venv\Scripts\activate

# å®‰è£æ‰€æœ‰ä¾è³´ (è¶…å¿«ï¼)
uv pip install -e .

# å®‰è£é–‹ç™¼å·¥å…·
uv pip install -e ".[dev]"

# é©—è­‰æ ¸å¿ƒå¥—ä»¶
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import faiss; print(f'FAISS: {faiss.__version__}')"
```

**é æœŸè¼¸å‡º**:

```
PyTorch: 2.1.2+cu121, CUDA: True
Transformers: 4.40.1
FAISS: 1.7.4
```

### 4.3 ç‰¹æ®Šå¥—ä»¶å®‰è£ï¼šPyTorch with CUDA

å¦‚æœä¸Šè¿°è‡ªå‹•å®‰è£çš„ PyTorch æ²’æœ‰ CUDA æ”¯æ´ï¼Œæ‰‹å‹•å®‰è£ï¼š

```bash
# å…ˆç§»é™¤èˆŠç‰ˆæœ¬
uv pip uninstall torch torchvision

# å®‰è£ CUDA 12.1 ç‰ˆæœ¬ (å°æ‡‰ RTX 5080)
uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# é©—è­‰
python -c "import torch; print(torch.cuda.is_available(), torch.version.cuda)"
# æ‡‰è©²è¼¸å‡º: True 12.1
```

### 4.4 å®‰è£ FAISS GPU ç‰ˆæœ¬ (å¦‚æœå¤±æ•—)

```bash
# å¦‚æœ faiss-gpu å®‰è£å¤±æ•—ï¼Œä½¿ç”¨ conda å®‰è£
conda install -c pytorch -c nvidia faiss-gpu=1.7.4

# æˆ–å¾æºç¢¼ç·¨è­¯ (é€²éš)
git clone https://github.com/facebookresearch/faiss.git
cd faiss
cmake -B build -DFAISS_ENABLE_GPU=ON -DCMAKE_BUILD_TYPE=Release .
make -C build -j
cd build/faiss/python && pip install .
```

### 4.5 ç”Ÿæˆä¾è³´é–å®šæ–‡ä»¶

```bash
# uv è‡ªå‹•ç”Ÿæˆ uv.lock (é¡ä¼¼ poetry.lock)
# é€™ç¢ºä¿äº†ç’°å¢ƒçš„å®Œå…¨å¯é‡ç¾æ€§

# æŸ¥çœ‹ä¾è³´æ¨¹
uv pip tree

# å°å‡º requirements.txt (ä¾›ä¸ä½¿ç”¨ uv çš„äºº)
uv pip freeze > requirements.txt
```

---

## 5) è³‡æ–™é›†ä¸‹è¼‰èˆ‡è™•ç†

### 5.1 ä¸‹è¼‰è…³æœ¬æº–å‚™

å»ºç«‹è³‡æ–™é›†ä¸‹è¼‰è…³æœ¬ï¼š

```bash
cat > scripts/download_coco.sh << 'SCRIPT'
#!/bin/bash
# MS-COCO 2014 è³‡æ–™é›†ä¸‹è¼‰è…³æœ¬

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³åœæ­¢

# è¨­å®šè®Šæ•¸
DATA_DIR="./data"
COCO_DIR="${DATA_DIR}/coco"
ANNO_DIR="${COCO_DIR}/annotations"
IMG_DIR="${COCO_DIR}/images"

# é¡è‰²è¼¸å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}MS-COCO 2014 Dataset Download Script${NC}"
echo -e "${GREEN}========================================${NC}"

# å»ºç«‹ç›®éŒ„
mkdir -p ${ANNO_DIR}
mkdir -p ${IMG_DIR}/{train2014,val2014}

# COCO ä¸‹è¼‰ URLs
ANNO_URL="http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
TRAIN_URL="http://images.cocodataset.org/zips/train2014.zip"
VAL_URL="http://images.cocodataset.org/zips/val2014.zip"

# å‡½æ•¸ï¼šä¸‹è¼‰ä¸¦è§£å£“
download_and_extract() {
    local url=$1
    local output_dir=$2
    local filename=$(basename ${url})
    
    echo -e "${YELLOW}æ­£åœ¨ä¸‹è¼‰: ${filename}${NC}"
    
    # ä¸‹è¼‰ (ä½¿ç”¨ wget æˆ– curl)
    if command -v wget &> /dev/null; then
        wget -c ${url} -P ${output_dir}
    elif command -v curl &> /dev/null; then
        curl -C - -o ${output_dir}/${filename} ${url}
    else
        echo -e "${RED}éŒ¯èª¤: è«‹å®‰è£ wget æˆ– curl${NC}"
        exit 1
    fi
    
    # è§£å£“
    echo -e "${YELLOW}æ­£åœ¨è§£å£“: ${filename}${NC}"
    unzip -q ${output_dir}/${filename} -d ${output_dir}
    
    # åˆªé™¤å£“ç¸®æª”ä»¥ç¯€çœç©ºé–“ (å¯é¸)
    read -p "åˆªé™¤å£“ç¸®æª” ${filename}? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        rm ${output_dir}/${filename}
        echo -e "${GREEN}å·²åˆªé™¤ ${filename}${NC}"
    fi
}

# 1. ä¸‹è¼‰æ¨™è¨»
echo -e "\n${GREEN}[1/3] ä¸‹è¼‰æ¨™è¨»æª”æ¡ˆ...${NC}"
if [ ! -f "${ANNO_DIR}/instances_train2014.json" ]; then
    download_and_extract ${ANNO_URL} ${DATA_DIR}
    mv ${DATA_DIR}/annotations/* ${ANNO_DIR}/
    rmdir ${DATA_DIR}/annotations
else
    echo -e "${YELLOW}æ¨™è¨»æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰${NC}"
fi

# 2. ä¸‹è¼‰è¨“ç·´å½±åƒ
echo -e "\n${GREEN}[2/3] ä¸‹è¼‰è¨“ç·´å½±åƒ (~13GB, éœ€æ™‚è¼ƒä¹…)...${NC}"
if [ ! -d "${IMG_DIR}/train2014" ] || [ -z "$(ls -A ${IMG_DIR}/train2014)" ]; then
    download_and_extract ${TRAIN_URL} ${IMG_DIR}
else
    echo -e "${YELLOW}è¨“ç·´å½±åƒå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰${NC}"
fi

# 3. ä¸‹è¼‰é©—è­‰å½±åƒ
echo -e "\n${GREEN}[3/3] ä¸‹è¼‰é©—è­‰å½±åƒ (~6GB)...${NC}"
if [ ! -d "${IMG_DIR}/val2014" ] || [ -z "$(ls -A ${IMG_DIR}/val2014)" ]; then
    download_and_extract ${VAL_URL} ${IMG_DIR}
else
    echo -e "${YELLOW}é©—è­‰å½±åƒå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰${NC}"
fi

# é©—è­‰ä¸‹è¼‰
echo -e "\n${GREEN}========================================${NC}"
echo -e "${GREEN}é©—è­‰è³‡æ–™é›†å®Œæ•´æ€§...${NC}"
echo -e "${GREEN}========================================${NC}"

# è¨ˆæ•¸æª”æ¡ˆ
train_count=$(find ${IMG_DIR}/train2014 -name "*.jpg" | wc -l)
val_count=$(find ${IMG_DIR}/val2014 -name "*.jpg" | wc -l)

echo -e "è¨“ç·´å½±åƒæ•¸é‡: ${GREEN}${train_count}${NC} (é æœŸ: 82,783)"
echo -e "é©—è­‰å½±åƒæ•¸é‡: ${GREEN}${val_count}${NC} (é æœŸ: 40,504)"

if [ -f "${ANNO_DIR}/instances_train2014.json" ]; then
    echo -e "æ¨™è¨»æª”æ¡ˆ: ${GREEN}âœ“${NC}"
else
    echo -e "æ¨™è¨»æª”æ¡ˆ: ${RED}âœ—${NC}"
fi

# é¡¯ç¤ºç£ç¢Ÿä½¿ç”¨é‡
echo -e "\nç£ç¢Ÿä½¿ç”¨é‡:"
du -sh ${COCO_DIR}

echo -e "\n${GREEN}========================================${NC}"
echo -e "${GREEN}ä¸‹è¼‰å®Œæˆï¼${NC}"
echo -e "${GREEN}========================================${NC}"
SCRIPT

# è³¦äºˆåŸ·è¡Œæ¬Šé™
chmod +x scripts/download_coco.sh
```

### 5.2 åŸ·è¡Œä¸‹è¼‰

```bash
# é–‹å§‹ä¸‹è¼‰ (éœ€æ™‚ç´„ 1-3 å°æ™‚ï¼Œè¦–ç¶²é€Ÿè€Œå®š)
./scripts/download_coco.sh

# å¦‚æœä½ åœ¨å°ç£/äºæ´²ï¼Œå¯ä»¥ä½¿ç”¨é¡åƒç«™åŠ é€Ÿï¼š
# ç·¨è¼¯è…³æœ¬ï¼Œå°‡ URL æ”¹ç‚ºï¼š
# TRAIN_URL="http://msvocds.blob.core.windows.net/coco2014/train2014.zip"
# VAL_URL="http://msvocds.blob.core.windows.net/coco2014/val2014.zip"
```

**é æœŸç›®éŒ„çµæ§‹**:

```
data/coco/
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ instances_train2014.json
â”‚   â”œâ”€â”€ instances_val2014.json
â”‚   â”œâ”€â”€ captions_train2014.json
â”‚   â”œâ”€â”€ captions_val2014.json
â”‚   â””â”€â”€ ...
â””â”€â”€ images/
    â”œâ”€â”€ train2014/
    â”‚   â”œâ”€â”€ COCO_train2014_000000000009.jpg
    â”‚   â”œâ”€â”€ COCO_train2014_000000000025.jpg
    â”‚   â””â”€â”€ ... (82,783 å¼µ)
    â””â”€â”€ val2014/
        â”œâ”€â”€ COCO_val2014_000000000042.jpg
        â””â”€â”€ ... (40,504 å¼µ)
```

### 5.3 ä¸‹è¼‰ Karpathy Split

```bash
# å»ºç«‹ä¸‹è¼‰ Karpathy split çš„ Python è…³æœ¬
cat > scripts/download_karpathy_split.py << 'PYTHON'
#!/usr/bin/env python3
"""ä¸‹è¼‰ä¸¦è™•ç† Karpathy split for COCO"""

import json
import urllib.request
from pathlib import Path

# Karpathy split URL
KARPATHY_URL = "https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip"

# æˆ–ä½¿ç”¨ GitHub å‚™ä»½
GITHUB_URL = "https://raw.githubusercontent.com/karpathy/neuraltalk2/master/coco/cocotalk.json"

data_dir = Path("./data/coco")
data_dir.mkdir(parents=True, exist_ok=True)

print("æ­£åœ¨ä¸‹è¼‰ Karpathy split...")

# ä¸‹è¼‰ JSON æª”æ¡ˆ
output_file = data_dir / "karpathy_split.json"

try:
    urllib.request.urlretrieve(GITHUB_URL, output_file)
    print(f"âœ“ ä¸‹è¼‰æˆåŠŸ: {output_file}")
except Exception as e:
    print(f"âœ— ä¸‹è¼‰å¤±æ•—: {e}")
    print("è«‹æ‰‹å‹•ä¸‹è¼‰: https://www.kaggle.com/datasets/shtvkumar/karpathy-splits")
    exit(1)

# è¼‰å…¥ä¸¦é©—è­‰
with open(output_file) as f:
    data = json.load(f)

# çµ±è¨ˆåˆ†å‰²
splits = {"train": 0, "val": 0, "test": 0, "restval": 0}
for item in data["images"]:
    split = item.get("split", "unknown")
    splits[split] = splits.get(split, 0) + 1

print("\nKarpathy Split çµ±è¨ˆ:")
for split, count in splits.items():
    print(f"  {split}: {count}")

# é©—è­‰
assert splits["train"] == 113287, f"è¨“ç·´é›†æ•¸é‡éŒ¯èª¤: {splits['train']} (é æœŸ 113287)"
assert splits["val"] == 5000, f"é©—è­‰é›†æ•¸é‡éŒ¯èª¤: {splits['val']} (é æœŸ 5000)"
assert splits["test"] == 5000, f"æ¸¬è©¦é›†æ•¸é‡éŒ¯èª¤: {splits['test']} (é æœŸ 5000)"

print("\nâœ“ Karpathy split é©—è­‰é€šéï¼")
PYTHON

chmod +x scripts/download_karpathy_split.py

# åŸ·è¡Œ
python scripts/download_karpathy_split.py
```

### 5.4 é è™•ç†è…³æœ¬ï¼ˆå»ºç«‹ç´¢å¼•ï¼‰

```bash
# å»ºç«‹è³‡æ–™é›†ç´¢å¼•è…³æœ¬
cat > scripts/create_dataset_index.py << 'PYTHON'
#!/usr/bin/env python3
"""å»ºç«‹ COCO è³‡æ–™é›†ç´¢å¼•ä»¥åŠ é€Ÿè¨“ç·´"""

import json
import pickle
from pathlib import Path
from collections import defaultdict
from pycocotools.coco import COCO
from tqdm import tqdm

def create_index(data_dir, split="train2014"):
    """å»ºç«‹å½±åƒåˆ°æ¨™è¨»çš„å¿«é€Ÿç´¢å¼•"""
    
    data_dir = Path(data_dir)
    anno_file = data_dir / "annotations" / f"instances_{split}.json"
    caption_file = data_dir / "annotations" / f"captions_{split}.json"
    
    print(f"æ­£åœ¨è™•ç† {split}...")
    
    # è¼‰å…¥ COCO API
    coco = COCO(anno_file)
    coco_caps = COCO(caption_file)
    
    # å»ºç«‹ç´¢å¼•
    index = {
        "images": {},
        "categories": {},
    }
    
    # 1. é¡åˆ¥è³‡è¨Š
    for cat_id, cat_info in coco.cats.items():
        index["categories"][cat_id] = {
            "id": cat_id,
            "name": cat_info["name"],
            "supercategory": cat_info["supercategory"]
        }
    
    # 2. å½±åƒè³‡è¨Š
    for img_id in tqdm(coco.imgs.keys(), desc="Processing images"):
        img_info = coco.imgs[img_id]
        
        # ç²å–è©²å½±åƒçš„æ‰€æœ‰æ¨™è¨»
        ann_ids = coco.getAnnIds(imgIds=img_id)
        anns = coco.loadAnns(ann_ids)
        
        # æå–é¡åˆ¥ (multi-hot)
        categories = set([ann["category_id"] for ann in anns])
        
        # ç²å– captions
        cap_ids = coco_caps.getAnnIds(imgIds=img_id)
        caps = coco_caps.loadAnns(cap_ids)
        captions = [cap["caption"] for cap in caps]
        
        index["images"][img_id] = {
            "file_name": img_info["file_name"],
            "width": img_info["width"],
            "height": img_info["height"],
            "categories": sorted(list(categories)),  # ç‰©ä»¶é¡åˆ¥åˆ—è¡¨
            "captions": captions,  # 5 å€‹ captions
        }
    
    # å„²å­˜ç´¢å¼•
    output_file = data_dir / f"index_{split}.pkl"
    with open(output_file, "wb") as f:
        pickle.dump(index, f)
    
    print(f"âœ“ ç´¢å¼•å·²å„²å­˜: {output_file}")
    print(f"  - å½±åƒæ•¸é‡: {len(index['images'])}")
    print(f"  - é¡åˆ¥æ•¸é‡: {len(index['categories'])}")
    
    return index

if __name__ == "__main__":
    data_dir = Path("./data/coco")
    
    # è™•ç†è¨“ç·´é›†èˆ‡é©—è­‰é›†
    for split in ["train2014", "val2014"]:
        create_index(data_dir, split)
    
    print("\nâœ“ æ‰€æœ‰ç´¢å¼•å»ºç«‹å®Œæˆï¼")
PYTHON

chmod +x scripts/create_dataset_index.py

# åŸ·è¡Œï¼ˆéœ€å®‰è£ pycocotoolsï¼‰
python scripts/create_dataset_index.py
```

### 5.5 è³‡æ–™é›†çµ±è¨ˆåˆ†æ

```bash
# å»ºç«‹çµ±è¨ˆè…³æœ¬
cat > scripts/analyze_dataset.py << 'PYTHON'
#!/usr/bin/env python3
"""åˆ†æ COCO è³‡æ–™é›†çµ±è¨ˆè³‡è¨Š"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from collections import Counter

def analyze(data_dir, split="train2014"):
    """åˆ†æè³‡æ–™é›†çµ±è¨ˆ"""
    
    # è¼‰å…¥ç´¢å¼•
    index_file = Path(data_dir) / f"index_{split}.pkl"
    with open(index_file, "rb") as f:
        index = pickle.load(f)
    
    print(f"\n{'='*60}")
    print(f"{split.upper()} è³‡æ–™é›†çµ±è¨ˆ")
    print(f"{'='*60}")
    
    # 1. åŸºæœ¬çµ±è¨ˆ
    n_images = len(index["images"])
    n_categories = len(index["categories"])
    
    print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"  å½±åƒæ•¸é‡: {n_images:,}")
    print(f"  é¡åˆ¥æ•¸é‡: {n_categories}")
    
    # 2. æ¯å¼µåœ–çš„æ¨™ç±¤æ•¸é‡åˆ†å¸ƒ
    labels_per_image = [len(info["categories"]) for info in index["images"].values()]
    
    print(f"\nğŸ·ï¸  æ¨™ç±¤æ•¸é‡åˆ†å¸ƒ:")
    print(f"  å¹³å‡æ¯å¼µåœ–çš„æ¨™ç±¤æ•¸: {np.mean(labels_per_image):.2f}")
    print(f"  ä¸­ä½æ•¸: {np.median(labels_per_image):.0f}")
    print(f"  æœ€å°å€¼: {np.min(labels_per_image)}")
    print(f"  æœ€å¤§å€¼: {np.max(labels_per_image)}")
    
    # 3. é¡åˆ¥é »ç‡
    category_counts = Counter()
    for img_info in index["images"].values():
        category_counts.update(img_info["categories"])
    
    print(f"\nğŸ” Top 10 æœ€å¸¸å‡ºç¾çš„é¡åˆ¥:")
    for cat_id, count in category_counts.most_common(10):
        cat_name = index["categories"][cat_id]["name"]
        percentage = count / n_images * 100
        print(f"  {cat_name:20s}: {count:6,} ({percentage:5.2f}%)")
    
    print(f"\nğŸ”» Bottom 10 æœ€å°‘å‡ºç¾çš„é¡åˆ¥:")
    for cat_id, count in category_counts.most_common()[-10:]:
        cat_name = index["categories"][cat_id]["name"]
        percentage = count / n_images * 100
        print(f"  {cat_name:20s}: {count:6,} ({percentage:5.2f}%)")
    
    # 4. Caption é•·åº¦åˆ†å¸ƒ
    caption_lengths = []
    for img_info in index["images"].values():
        for cap in img_info["captions"]:
            caption_lengths.append(len(cap.split()))
    
    print(f"\nğŸ“ Caption çµ±è¨ˆ:")
    print(f"  å¹³å‡é•·åº¦: {np.mean(caption_lengths):.2f} å€‹å­—")
    print(f"  ä¸­ä½æ•¸: {np.median(caption_lengths):.0f} å€‹å­—")
    print(f"  æœ€çŸ­: {np.min(caption_lengths)} å€‹å­—")
    print(f"  æœ€é•·: {np.max(caption_lengths)} å€‹å­—")
    
    return {
        "labels_per_image": labels_per_image,
        "category_counts": category_counts,
        "caption_lengths": caption_lengths,
    }

if __name__ == "__main__":
    data_dir = Path("./data/coco")
    
    # åˆ†æè¨“ç·´é›†
    train_stats = analyze(data_dir, "train2014")
    
    # åˆ†æé©—è­‰é›†
    val_stats = analyze(data_dir, "val2014")
    
    # è¦–è¦ºåŒ– (å¯é¸)
    # ... (å¯è‡ªè¡ŒåŠ å…¥ matplotlib ç¹ªåœ–)
PYTHON

chmod +x scripts/analyze_dataset.py

# åŸ·è¡Œåˆ†æ
python scripts/analyze_dataset.py
```

---

## 6) é©—è­‰ç’°å¢ƒ

### 6.1 å»ºç«‹é©—è­‰è…³æœ¬

```bash
cat > scripts/verify_setup.py << 'PYTHON'
#!/usr/bin/env python3
"""é©—è­‰å¯¦é©—ç’°å¢ƒè¨­ç½®"""

import sys
from pathlib import Path

def check_python():
    """æª¢æŸ¥ Python ç‰ˆæœ¬"""
    version = sys.version_info
    print(f"âœ“ Python {version.major}.{version.minor}.{version.micro}")
    assert version.major == 3 and version.minor >= 10, "éœ€è¦ Python 3.10+"

def check_cuda():
    """æª¢æŸ¥ CUDA å¯ç”¨æ€§"""
    import torch
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        print(f"âœ“ CUDA {torch.version.cuda}")
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("âœ— CUDA ä¸å¯ç”¨ (å°‡ä½¿ç”¨ CPUï¼Œé€Ÿåº¦æœƒå¾ˆæ…¢)")
    return cuda_available

def check_packages():
    """æª¢æŸ¥é—œéµå¥—ä»¶"""
    packages = {
        "torch": "2.1.0",
        "transformers": "4.40.0",
        "faiss": "1.7.0",
        "pycocotools": "2.0.0",
    }
    
    for pkg, min_version in packages.items():
        try:
            mod = __import__(pkg)
            version = mod.__version__ if hasattr(mod, "__version__") else "unknown"
            print(f"âœ“ {pkg:20s} {version}")
        except ImportError:
            print(f"âœ— {pkg:20s} æœªå®‰è£")
            return False
    
    return True

def check_dataset():
    """æª¢æŸ¥è³‡æ–™é›†"""
    data_dir = Path("./data/coco")
    
    checks = [
        data_dir / "images/train2014",
        data_dir / "images/val2014",
        data_dir / "annotations/instances_train2014.json",
        data_dir / "annotations/captions_train2014.json",
        data_dir / "index_train2014.pkl",
    ]
    
    all_exist = True
    for path in checks:
        if path.exists():
            if path.is_dir():
                n_files = len(list(path.glob("*.jpg")))
                print(f"âœ“ {path} ({n_files:,} å¼µå½±åƒ)")
            else:
                size_mb = path.stat().st_size / 1e6
                print(f"âœ“ {path} ({size_mb:.1f} MB)")
        else:
            print(f"âœ— {path} ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist

def check_siglip2():
    """æª¢æŸ¥ SigLIP2 æ¨¡å‹è¼‰å…¥"""
    from transformers import Siglip2Model, Siglip2Processor
    
    try:
        print("\næ­£åœ¨æ¸¬è©¦ SigLIP2 æ¨¡å‹è¼‰å…¥...")
        model_name = "google/siglip2-base-patch16-256"
        processor = Siglip2Processor.from_pretrained(model_name)
        model = Siglip2Model.from_pretrained(model_name)
        print(f"âœ“ SigLIP2 æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        print(f"  åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
        return True
    except Exception as e:
        print(f"âœ— SigLIP2 æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return False

def main():
    print("="*60)
    print("ç’°å¢ƒé©—è­‰")
    print("="*60)
    
    print("\n[1/5] æª¢æŸ¥ Python ç‰ˆæœ¬...")
    check_python()
    
    print("\n[2/5] æª¢æŸ¥ CUDA...")
    cuda_ok = check_cuda()
    
    print("\n[3/5] æª¢æŸ¥ Python å¥—ä»¶...")
    pkg_ok = check_packages()
    
    print("\n[4/5] æª¢æŸ¥è³‡æ–™é›†...")
    data_ok = check_dataset()
    
    print("\n[5/5] æª¢æŸ¥ SigLIP2 æ¨¡å‹...")
    model_ok = check_siglip2()
    
    print("\n" + "="*60)
    if all([cuda_ok, pkg_ok, data_ok, model_ok]):
        print("âœ… ç’°å¢ƒè¨­ç½®å®Œæˆï¼å¯ä»¥é–‹å§‹å¯¦é©—äº†ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æª¢æŸ¥å¤±æ•—ï¼Œè«‹ä¿®æ­£å¾Œå†è©¦ã€‚")
    print("="*60)

if __name__ == "__main__":
    main()
PYTHON

chmod +x scripts/verify_setup.py

# åŸ·è¡Œé©—è­‰
python scripts/verify_setup.py
```

### 6.2 æ¸¬è©¦ SigLIP2 å‰å‘å‚³æ’­

```bash
cat > scripts/test_siglip2.py << 'PYTHON'
#!/usr/bin/env python3
"""æ¸¬è©¦ SigLIP2 æ¨¡å‹å‰å‘å‚³æ’­"""

import torch
from transformers import Siglip2Model, Siglip2Processor
from PIL import Image
import requests
from io import BytesIO

# è¼‰å…¥æ¨¡å‹
print("è¼‰å…¥ SigLIP2 æ¨¡å‹...")
model_name = "google/siglip2-base-patch16-256"
processor = Siglip2Processor.from_pretrained(model_name)
model = Siglip2Model.from_pretrained(model_name)

# ç§»åˆ° GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

print(f"æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")

# ä¸‹è¼‰æ¸¬è©¦å½±åƒ
print("\nä¸‹è¼‰æ¸¬è©¦å½±åƒ...")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
response = requests.get(url)
image = Image.open(BytesIO(response.content))

# æº–å‚™è¼¸å…¥
text = "Two cats sleeping on a couch"
inputs = processor(
    text=[text],
    images=image,
    return_tensors="pt",
    padding=True
)
inputs = {k: v.to(device) for k, v in inputs.items()}

# å‰å‘å‚³æ’­
print("\nåŸ·è¡Œå‰å‘å‚³æ’­...")
with torch.no_grad():
    outputs = model(**inputs)

# æª¢æŸ¥è¼¸å‡º
image_embeds = outputs.image_embeds  # (1, D)
text_embeds = outputs.text_embeds    # (1, D)

print(f"âœ“ å½±åƒ embedding shape: {image_embeds.shape}")
print(f"âœ“ æ–‡å­— embedding shape: {text_embeds.shape}")

# è¨ˆç®—ç›¸ä¼¼åº¦
similarity = torch.cosine_similarity(image_embeds, text_embeds)
print(f"âœ“ åœ–æ–‡ç›¸ä¼¼åº¦: {similarity.item():.4f}")

print("\nâœ… SigLIP2 æ¸¬è©¦é€šéï¼")
PYTHON

chmod +x scripts/test_siglip2.py
python scripts/test_siglip2.py
```

---

## 7) å¸¸è¦‹å•é¡Œæ’é™¤

### 7.1 uv å®‰è£å¤±æ•—

**å•é¡Œ**: `curl: command not found`

**è§£æ±º**:

```bash
# Ubuntu/Debian
sudo apt install curl

# æˆ–ç›´æ¥ç”¨ pip
pip install uv
```

### 7.2 CUDA ä¸å¯ç”¨

**å•é¡Œ**: `torch.cuda.is_available()` è¿”å› `False`

**æª¢æŸ¥æ¸…å–®**:

```bash
# 1. æª¢æŸ¥ NVIDIA é©…å‹•
nvidia-smi

# 2. æª¢æŸ¥ PyTorch CUDA ç‰ˆæœ¬æ˜¯å¦åŒ¹é…
python -c "import torch; print(torch.version.cuda)"

# 3. é‡æ–°å®‰è£æ­£ç¢ºçš„ PyTorch ç‰ˆæœ¬
uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### 7.3 è³‡æ–™é›†ä¸‹è¼‰é€Ÿåº¦æ…¢

**è§£æ±ºæ–¹æ¡ˆ 1: ä½¿ç”¨é¡åƒç«™**

```bash
# ç·¨è¼¯ scripts/download_coco.shï¼Œæ”¹ç”¨ Azure é¡åƒ
TRAIN_URL="http://msvocds.blob.core.windows.net/coco2014/train2014.zip"
VAL_URL="http://msvocds.blob.core.windows.net/coco2014/val2014.zip"
```

**è§£æ±ºæ–¹æ¡ˆ 2: ä½¿ç”¨ aria2c å¤šç·šç¨‹ä¸‹è¼‰**

```bash
# å®‰è£ aria2c
sudo apt install aria2

# å¤šç·šç¨‹ä¸‹è¼‰
aria2c -x 16 -s 16 http://images.cocodataset.org/zips/train2014.zip
```

**è§£æ±ºæ–¹æ¡ˆ 3: ä½¿ç”¨å­¸è¡“ç¶²è·¯ VPN**

- è¨±å¤šå¤§å­¸æä¾› VPNï¼Œé€£ç·šå¾Œä¸‹è¼‰é€Ÿåº¦æœƒå¤§å¹…æå‡

### 7.4 FAISS GPU ç‰ˆæœ¬ç·¨è­¯å¤±æ•—

**æ–¹æ¡ˆ A: ä½¿ç”¨ Conda å®‰è£**

```bash
# å³ä½¿åœ¨ uv ç’°å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨ conda è£ faiss
conda install -c pytorch -c nvidia faiss-gpu=1.7.4
```

**æ–¹æ¡ˆ B: å…ˆç”¨ CPU ç‰ˆæœ¬**

```bash
# æš«æ™‚ä½¿ç”¨ CPU ç‰ˆæœ¬ï¼ˆé€Ÿåº¦è¼ƒæ…¢ä½†èƒ½ç”¨ï¼‰
uv pip install faiss-cpu

# ç­‰éœ€è¦å¤§è¦æ¨¡æª¢ç´¢æ™‚å†æ›å› GPU ç‰ˆæœ¬
```

### 7.5 è¨˜æ†¶é«”ä¸è¶³ï¼ˆRAMï¼‰

**å•é¡Œ**: è™•ç†è³‡æ–™é›†æ™‚ç³»çµ±è¨˜æ†¶é«”ä¸è¶³

**è§£æ±º**:

```bash
# 1. é—œé–‰ä¸å¿…è¦çš„ç¨‹å¼
# 2. ä½¿ç”¨ swap (è‡¨æ™‚)
sudo fallocate -l 32G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 3. æˆ–åˆ†æ‰¹è™•ç†è³‡æ–™ï¼ˆä¿®æ”¹è…³æœ¬ï¼‰
```

### 7.6 pycocotools å®‰è£å¤±æ•—

**å•é¡Œ**: `error: Microsoft Visual C++ 14.0 is required`ï¼ˆWindowsï¼‰

**è§£æ±º**:

```bash
# å®‰è£ Visual Studio Build Tools
# æˆ–ä½¿ç”¨é ç·¨è­¯è¼ªå­
uv pip install pycocotools-windows
```

**Linux**:

```bash
# å®‰è£ä¾è³´
sudo apt install python3-dev build-essential
uv pip install pycocotools
```

---

## 8) ä¸‹ä¸€æ­¥

ç’°å¢ƒè¨­ç½®å®Œæˆå¾Œï¼Œå¯ä»¥é–‹å§‹ï¼š

### 8.1 å»ºç«‹é…ç½®æª”æ¡ˆ

```bash
# å»ºç«‹ç¬¬ä¸€å€‹å¯¦é©—é…ç½®
mkdir -p configs/experiments
cat > configs/experiments/baseline.yaml << 'YAML'
# Baseline å¯¦é©—é…ç½®
name: baseline_siglip2_hash64_knn20

model:
  siglip2_variant: google/siglip2-base-patch16-256
  max_num_patches: 256
  freeze_towers: true
  hash_bits: 64

training:
  batch_size: 64
  num_epochs: 30
  lr: 3e-4

knn:
  K: 20
  tau: 0.07
YAML
```

### 8.2 é–‹å§‹é–‹ç™¼

```bash
# å»ºç«‹ä¸»è¦ç¨‹å¼ç¢¼æª”æ¡ˆ
touch src/{__init__,model,dataset,train,evaluate}.py

# æˆ–åƒè€ƒå®Œæ•´è¨ˆç•«æ–‡ä»¶ä¸­çš„ç¨‹å¼ç¢¼ç¯„ä¾‹
```

### 8.3 å•Ÿå‹•å¯¦é©—è¿½è¹¤

```bash
# ç™»å…¥ Weights & Biases (å¯é¸)
wandb login

# æˆ–ä½¿ç”¨ TensorBoard
tensorboard --logdir experiments/
```

---

## ç¸½çµ

ä½ ç¾åœ¨å·²ç¶“å®Œæˆï¼š

- âœ… å®‰è£äº†è¶…å¿«é€Ÿçš„ `uv` å¥—ä»¶ç®¡ç†å™¨
- âœ… å»ºç«‹äº†éš”é›¢çš„ Python è™›æ“¬ç’°å¢ƒ
- âœ… å®‰è£äº†æ‰€æœ‰å¿…è¦çš„ä¾è³´å¥—ä»¶ï¼ˆPyTorch, Transformers, FAISS ç­‰ï¼‰
- âœ… ä¸‹è¼‰äº† MS-COCO 2014 è³‡æ–™é›†ï¼ˆ~20GBï¼‰
- âœ… å»ºç«‹äº†è³‡æ–™é›†ç´¢å¼•ä»¥åŠ é€Ÿè¨“ç·´
- âœ… é©—è­‰äº† SigLIP2 æ¨¡å‹å¯æ­£å¸¸é‹ä½œ
- âœ… å»ºç«‹äº†å®Œæ•´çš„å°ˆæ¡ˆç›®éŒ„çµæ§‹

**ç’°å¢ƒè¨­ç½®æª¢æŸ¥è¡¨**:

```
[O] uv å·²å®‰è£ä¸”å¯ç”¨
[O] Python 3.10+ è™›æ“¬ç’°å¢ƒå·²å»ºç«‹
[O] PyTorch with CUDA å¯æ­£å¸¸é‹ä½œ
[O] Transformers å¥—ä»¶å·²å®‰è£
[O] FAISS-GPU å·²å®‰è£ (Use CPU Version instead)
[-] COCO 2014 è¨“ç·´/é©—è­‰å½±åƒå·²ä¸‹è¼‰
â–¡ COCO æ¨™è¨»èˆ‡ captions å·²ä¸‹è¼‰
â–¡ Karpathy split å·²ä¸‹è¼‰
â–¡ è³‡æ–™é›†ç´¢å¼•å·²å»ºç«‹
â–¡ SigLIP2 æ¨¡å‹å¯è¼‰å…¥ä¸¦å‰å‘å‚³æ’­
```

**ä¸‹ä¸€æ­¥å»ºè­°**:

1. ğŸ“– é–±è®€å®Œæ•´å¯¦é©—è¨ˆç•«ï¼ˆ`siglip2_multimodal_hash_knn_complete_plan.md`ï¼‰
2. ğŸ’» å¯¦ä½œè³‡æ–™è¼‰å…¥å™¨ï¼ˆ`src/dataset.py`ï¼‰
3. ğŸ—ï¸ å¯¦ä½œæ¨¡å‹æ¶æ§‹ï¼ˆ`src/model.py`ï¼‰
4. ğŸš€ é–‹å§‹è¨“ç·´ï¼

ç¥å¯¦é©—é †åˆ©ï¼æœ‰ä»»ä½•å•é¡Œéš¨æ™‚è©¢å•ã€‚
PYTHON

```

æ­å–œä½ å®Œæˆç’°å¢ƒè¨­ç½®ï¼ğŸ‰

# ç¡¬é«”é…ç½®å„ªåŒ–æŒ‡å— - RTX 5080 16GB å°ˆç”¨

> **ä½ çš„ç¡¬é«”è¦æ ¼**:
> - GPU: NVIDIA GeForce RTX 5080 (16GB VRAM)
> - CPU: 32 æ ¸å¿ƒ
> - RAM: 42 GB
> - CUDA: 13.0
> - Driver: 580.126.09

---

## âš ï¸ é‡è¦æé†’ï¼šVRAM é™åˆ¶

ä½ çš„ GPU åªæœ‰ **16GB VRAM**ï¼ˆè€ŒéåŸè¨ˆç•«å‡è¨­çš„ 24GBï¼‰ï¼Œå› æ­¤éœ€è¦èª¿æ•´ä»¥ä¸‹åƒæ•¸ï¼š

### åŸå§‹é…ç½® vs å„ªåŒ–é…ç½®å°æ¯”

| åƒæ•¸ | åŸå§‹ (24GB) | å„ªåŒ– (16GB) | èªªæ˜ |
| ------ | ------------ | ------------ | ------ |
| `batch_size` | 64 | **32** | æ¸›åŠä»¥ç¯€çœè¨˜æ†¶é«” |
| `max_num_patches` | 256 | **256** (ä¿æŒ) | å¯å˜—è©¦ä½†éœ€ç›£æ§ |
| `mixed_precision` | å»ºè­° | **å¿…é ˆ** | FP16 å¯ç¯€çœ 40% VRAM |
| `gradient_accumulation` | å¯é¸ | **æ¨è–¦ 2-4** | æ¨¡æ“¬å¤§ batch size |
| `freeze_towers` | true | **true** | å¿…é ˆå‡çµä»¥ç¯€çœè¨˜æ†¶é«” |

---

## 1) å„ªåŒ–å¾Œçš„ pyproject.toml

```toml
[project]
name = "siglip2-multimodal-hash"
version = "0.1.0"
description = "Multimodal image-text multi-label classification using SigLIP2, hashing, and KNN"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}

dependencies = [
    # Deep Learning Framework (CUDA 13.0 ç›¸å®¹)
    "torch>=2.5.0",  # æ”¯æ´ CUDA 13.0
    "torchvision>=0.20.0",
    
    # Transformers & Vision-Language Models
    "transformers>=4.47.0",  # ç¢ºä¿æ”¯æ´ SigLIP2
    "accelerate>=1.2.0",
    
    # Computer Vision
    "opencv-python>=4.10.0",
    "pillow>=11.0.0",
    "albumentations>=1.4.0",
    
    # Data Processing
    "numpy>=1.26.0,<2.0.0",  # ç¢ºä¿ç›¸å®¹æ€§
    "pandas>=2.2.0",
    "pycocotools>=2.0.7",
    
    # Similarity Search & Indexing
    "faiss-gpu>=1.9.0",  # CUDA 13.0 ç›¸å®¹ç‰ˆæœ¬
    
    # Metrics & Evaluation
    "scikit-learn>=1.5.0",
    "scipy>=1.14.0",
    
    # Visualization
    "matplotlib>=3.9.0",
    "seaborn>=0.13.0",
    "plotly>=5.24.0",
    
    # Configuration & Logging
    "hydra-core>=1.3.0",
    "omegaconf>=2.3.0",
    "wandb>=0.18.0",
    "tensorboard>=2.18.0",
    
    # Utilities
    "tqdm>=4.67.0",
    "rich>=13.9.0",
    "python-dotenv>=1.0.0",
    
    # Development Tools
    "ipython>=8.29.0",
    "jupyter>=1.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.0",
    "pytest-cov>=6.0.0",
    "black>=24.10.0",
    "ruff>=0.8.0",
    "mypy>=1.13.0",
    "pre-commit>=4.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

---

## 2) é‡å° 16GB VRAM çš„è¨“ç·´é…ç½®

### configs/hardware/rtx5080_16gb.yaml

```yaml
# RTX 5080 16GB ç¡¬é«”å„ªåŒ–é…ç½®
hardware:
  device: cuda
  gpu_model: "RTX 5080"
  vram_gb: 16
  cuda_version: "13.0"
  
  # è¨˜æ†¶é«”å„ªåŒ–
  memory_optimization:
    mixed_precision: true  # å¿…é ˆå•Ÿç”¨ï¼
    gradient_checkpointing: true  # ç¯€çœ 30-40% è¨˜æ†¶é«”
    empty_cache_steps: 100  # å®šæœŸæ¸…ç† GPU å¿«å–
    pin_memory: true
    non_blocking: true

# æ¨¡å‹é…ç½®ï¼ˆé‡å° 16GB å„ªåŒ–ï¼‰
model:
  siglip2_variant: "google/siglip2-base-patch16-256"  # ä¸è¦ç”¨ largeï¼
  max_num_patches: 256  # ä¿å®ˆè¨­å®š
  text_max_length: 64
  freeze_towers: true  # å¿…é ˆå‡çµï¼
  
  fusion:
    mlp_dims: [1024, 512]  # ä¿æŒåŸè¨­è¨ˆ
    dropout: 0.1
  
  hash:
    bits: 64  # å¯å˜—è©¦ 128ï¼Œä½†éœ€ç›£æ§è¨˜æ†¶é«”

# è¨“ç·´é…ç½®
training:
  # æ‰¹æ¬¡å¤§å°ï¼ˆé—œéµï¼ï¼‰
  batch_size: 32  # åŸæœ¬ 64 å¤ªå¤§
  gradient_accumulation_steps: 2  # æ¨¡æ“¬ batch_size=64
  
  # æœ‰æ•ˆ batch size = 32 * 2 = 64
  effective_batch_size: 64
  
  # Epoch èˆ‡å­¸ç¿’ç‡
  num_epochs: 30
  warmup_epochs: 2
  
  # DataLoader å„ªåŒ–ï¼ˆåˆ©ç”¨ä½ çš„ 32 æ ¸å¿ƒ CPUï¼ï¼‰
  num_workers: 16  # ä½ æœ‰ 32 æ ¸å¿ƒï¼Œå¯ä»¥å¤šç”¨é»
  prefetch_factor: 3
  persistent_workers: true
  
  # è¨˜æ†¶é«”ç®¡ç†
  gradient_clip_norm: 1.0
  max_grad_norm: 1.0

# Optimizerï¼ˆé‡å°å° batch èª¿æ•´ï¼‰
optimizer:
  type: adamw
  lr: 2e-4  # æ¯”åŸæœ¬çš„ 3e-4 ç•¥å°ï¼ˆå› ç‚º effective batch size ä¸€æ¨£ï¼‰
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Scheduler
scheduler:
  type: cosine_with_warmup
  warmup_ratio: 0.1
  min_lr: 1e-6

# ç›£æ§èˆ‡æª¢æŸ¥é»
monitoring:
  log_every_n_steps: 50
  val_every_n_epochs: 1
  save_top_k: 3
  early_stopping_patience: 5
  
  # VRAM ç›£æ§ï¼ˆé‡è¦ï¼ï¼‰
  log_gpu_memory: true
  alert_vram_threshold: 14.5  # è¶…é 14.5GB ç™¼å‡ºè­¦å‘Š

# KNN æ¨è«–é…ç½®
knn:
  K: 20
  tau: 0.07
  batch_size: 64  # æ¨è«–æ™‚å¯ä»¥å¤§ä¸€é»
```

---

## 3) è¨˜æ†¶é«”ä½”ç”¨ä¼°ç®—è¡¨ï¼ˆ16GB VRAMï¼‰

### Base é…ç½®ä¸‹çš„è¨˜æ†¶é«”åˆ†é…

| çµ„ä»¶ | è¨˜æ†¶é«”ä½”ç”¨ | èªªæ˜ |
| ------ | ----------- | ------ |
| **SigLIP2-base (å‡çµ)** | ~2.5 GB | åƒ… forward pass |
| **Fusion MLP** | ~0.3 GB | å¯è¨“ç·´ |
| **Hash Layer** | ~0.1 GB | å¯è¨“ç·´ |
| **Classifier Head** | ~0.05 GB | å¯è¨“ç·´ |
| **Optimizer States** | ~1.2 GB | AdamW (2x åƒæ•¸é‡) |
| **Batch Data (32)** | ~4.0 GB | Images + embeddings |
| **Gradients** | ~0.5 GB | åƒ…å¯è¨“ç·´éƒ¨åˆ† |
| **CUDA Kernels** | ~0.5 GB | PyTorch overhead |
| **é ç•™ç·©è¡** | ~1.0 GB | å®‰å…¨é‚Šç•Œ |
| **ç¸½è¨ˆ** | **~10.2 GB** | âœ… åœ¨ 16GB å…§å®‰å…¨ |

### âš ï¸ ä¸è¦å˜—è©¦çš„é…ç½®

| é…ç½® | é ä¼° VRAM | çµæœ |
| ------ | ----------- | ------ |
| batch_size=64 | ~16.5 GB | âŒ OOM (è¨˜æ†¶é«”æº¢å‡º) |
| SigLIP2-large | ~18 GB | âŒ OOM |
| max_patches=512 | ~14 GB | âš ï¸ å¯èƒ½ OOM |
| ä¸å‡çµ towers | ~22 GB | âŒ OOM |

---

## 4) å®‰è£ PyTorch for CUDA 13.0

ä½ çš„ç³»çµ±æœ‰ **CUDA 13.0**ï¼ˆéå¸¸æ–°ï¼ï¼‰ï¼Œéœ€è¦ç¢ºä¿ PyTorch ç›¸å®¹ï¼š

```bash
# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
cd ~/projects/siglip2-multimodal-hash
source .venv/bin/activate

# å®‰è£æœ€æ–°ç‰ˆ PyTorch (æ”¯æ´ CUDA 13.0)
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# é©—è­‰
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA version: {torch.version.cuda}')
print(f'GPU: {torch.cuda.get_device_name(0)}')
print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
"
```

**é æœŸè¼¸å‡º**:

```
PyTorch: 2.5.1+cu130
CUDA available: True
CUDA version: 13.0
GPU: NVIDIA GeForce RTX 5080
VRAM: 16.3 GB
```

---

## 5) FAISS GPU for CUDA 13.0

FAISS å¯èƒ½é‚„æ²’æœ‰å®˜æ–¹çš„ CUDA 13.0 è¼ªå­ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

### æ–¹æ³• A: ä½¿ç”¨ Conda (æ¨è–¦)

```bash
# å³ä½¿åœ¨ uv ç’°å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨ conda è£ FAISS
conda install -c pytorch -c nvidia faiss-gpu

# é©—è­‰
python -c "import faiss; print(f'FAISS version: {faiss.__version__}')"
```

### æ–¹æ³• B: æš«æ™‚ä½¿ç”¨ CPU ç‰ˆæœ¬

```bash
# å…ˆç”¨ CPU ç‰ˆæœ¬é–‹ç™¼
uv pip install faiss-cpu

# ç­‰æ­£å¼è¨“ç·´æ™‚å†æ›
```

### æ–¹æ³• C: å¾æºç¢¼ç·¨è­¯ (é€²éš)

```bash
# å®‰è£ä¾è³´
sudo apt-get install cmake libopenblas-dev

# ä¸‹è¼‰ä¸¦ç·¨è­¯
git clone https://github.com/facebookresearch/faiss.git
cd faiss

# é…ç½®ï¼ˆé‡å° CUDA 13.0ï¼‰
cmake -B build \
  -DFAISS_ENABLE_GPU=ON \
  -DFAISS_ENABLE_PYTHON=ON \
  -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_CUDA_ARCHITECTURES=89 \
  .

# ç·¨è­¯ï¼ˆä½¿ç”¨ä½ çš„ 32 æ ¸å¿ƒï¼ï¼‰
make -C build -j32

# å®‰è£
cd build/faiss/python
pip install .
```

**è¨»**: RTX 5080 çš„ compute capability æ˜¯ **8.9**ï¼ˆAda Lovelace æ¶æ§‹ï¼‰

---

## 6) è¨“ç·´è…³æœ¬ç¯„ä¾‹ï¼ˆå«è¨˜æ†¶é«”ç›£æ§ï¼‰

å»ºç«‹ä¸€å€‹è¨“ç·´è…³æœ¬ï¼ŒåŠ å…¥ VRAM ç›£æ§ï¼š

```python
#!/usr/bin/env python3
# scripts/train_with_memory_monitor.py

import torch
import psutil
from torch.cuda.amp import autocast, GradScaler

def get_gpu_memory_info():
    """ç²å– GPU è¨˜æ†¶é«”ä½¿ç”¨è³‡è¨Š"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        max_allocated = torch.cuda.max_memory_allocated() / 1e9
        return {
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "max_allocated_gb": max_allocated,
            "free_gb": 16.0 - reserved  # ä½ çš„ GPU ç¸½è¨˜æ†¶é«”
        }
    return None

def train_epoch_optimized(model, dataloader, optimizer, scheduler, config):
    """å„ªåŒ–çš„è¨“ç·´è¿´åœˆï¼ˆé‡å° 16GB VRAMï¼‰"""
    
    model.train()
    scaler = GradScaler()  # æ··åˆç²¾åº¦
    
    total_loss = 0
    accumulation_steps = config.training.gradient_accumulation_steps
    
    for batch_idx, batch in enumerate(dataloader):
        # ç§»åˆ° GPU
        images = batch['images'].to('cuda', non_blocking=True)
        texts = batch['texts'].to('cuda', non_blocking=True)
        labels = batch['labels'].to('cuda', non_blocking=True)
        
        # æ··åˆç²¾åº¦å‰å‘å‚³æ’­
        with autocast():
            outputs = model(images, texts, return_components=True)
            loss = compute_loss(outputs, labels, config)
            loss = loss / accumulation_steps  # æ¢¯åº¦ç´¯ç©
        
        # åå‘å‚³æ’­
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦ç´¯ç©
        if (batch_idx + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip_norm)
            
            # æ›´æ–°
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        total_loss += loss.item()
        
        # å®šæœŸç›£æ§è¨˜æ†¶é«”
        if batch_idx % 100 == 0:
            mem_info = get_gpu_memory_info()
            print(f"Batch {batch_idx}: GPU Memory: {mem_info['allocated_gb']:.2f}GB / 16GB")
            
            # è­¦å‘Šæ©Ÿåˆ¶
            if mem_info['allocated_gb'] > 14.5:
                print("âš ï¸  è­¦å‘Šï¼šGPU è¨˜æ†¶é«”ä½¿ç”¨æ¥è¿‘ä¸Šé™ï¼")
        
        # å®šæœŸæ¸…ç†å¿«å–
        if batch_idx % config.hardware.memory_optimization.empty_cache_steps == 0:
            torch.cuda.empty_cache()
    
    scheduler.step()
    return total_loss / len(dataloader)

def compute_loss(outputs, labels, config):
    """æå¤±è¨ˆç®—ï¼ˆèˆ‡åŸè¨ˆç•«ç›¸åŒï¼‰"""
    logits = outputs['logits']
    h = outputs['h']
    d_img = outputs['d_img']
    d_txt = outputs['d_txt']
    
    # BCE Loss
    loss_bce = torch.nn.functional.binary_cross_entropy_with_logits(logits, labels.float())
    
    # Cosine Loss
    loss_cos = 1 - torch.nn.functional.cosine_similarity(d_img, d_txt, dim=1).mean()
    
    # Hash Regularization
    loss_hash = hash_regularization(h, config)
    
    # çµ„åˆ
    total_loss = (
        config.loss.bce_weight * loss_bce +
        config.loss.cosine_weight * loss_cos +
        config.loss.hash_weight * loss_hash
    )
    
    return total_loss

def hash_regularization(h, config):
    """Hash æ­£å‰‡åŒ–ï¼ˆèˆ‡åŸè¨ˆç•«ç›¸åŒï¼‰"""
    # Quantization
    loss_quant = torch.mean((torch.abs(h) - 1) ** 2)
    
    # Balance
    bit_mean = torch.mean(h, dim=0)
    loss_balance = torch.mean(bit_mean ** 2)
    
    # Decorrelation
    h_centered = h - torch.mean(h, dim=0, keepdim=True)
    cov = (h_centered.T @ h_centered) / h.size(0)
    loss_decorr = (torch.sum(cov ** 2) - torch.trace(cov ** 2)) / (h.size(1) ** 2)
    
    return loss_quant + config.loss.hash_reg.lambda_balance * loss_balance + \
           config.loss.hash_reg.lambda_decorr * loss_decorr

if __name__ == "__main__":
    # æ¸¬è©¦è¨˜æ†¶é«”ç›£æ§
    if torch.cuda.is_available():
        mem_info = get_gpu_memory_info()
        print("GPU è¨˜æ†¶é«”ç‹€æ…‹:")
        print(f"  å·²åˆ†é…: {mem_info['allocated_gb']:.2f} GB")
        print(f"  å·²ä¿ç•™: {mem_info['reserved_gb']:.2f} GB")
        print(f"  å¯ç”¨: {mem_info['free_gb']:.2f} GB")
```

---

## 7) è¨˜æ†¶é«”ä¸è¶³æ™‚çš„ç·Šæ€¥æªæ–½

å¦‚æœè¨“ç·´æ™‚é‚„æ˜¯é‡åˆ° OOMï¼Œä¾åºå˜—è©¦ï¼š

### æ­¥é©Ÿ 1: é™ä½ batch size

```yaml
training:
  batch_size: 16  # å¾ 32 é™åˆ° 16
  gradient_accumulation_steps: 4  # èª¿æ•´åˆ° 4
```

### æ­¥é©Ÿ 2: é™ä½è§£æåº¦

```yaml
model:
  max_num_patches: 196  # å¾ 256 é™åˆ° 196
```

### æ­¥é©Ÿ 3: å•Ÿç”¨æ›´æ¿€é€²çš„å„ªåŒ–

```python
# åœ¨æ¨¡å‹åˆå§‹åŒ–æ™‚
model.model.gradient_checkpointing_enable()

# æˆ–ä½¿ç”¨ torch 2.0 çš„ compileï¼ˆå¦‚æœæ”¯æ´ï¼‰
model = torch.compile(model, mode="reduce-overhead")
```

### æ­¥é©Ÿ 4: æ¸…ç†ä¸å¿…è¦çš„å¼µé‡

```python
# åœ¨è¨“ç·´è¿´åœˆä¸­
del outputs, loss
torch.cuda.empty_cache()
```

---

## 8) é æœŸè¨“ç·´é€Ÿåº¦ï¼ˆä½ çš„ç¡¬é«”ï¼‰

åŸºæ–¼ä½ çš„é…ç½®ï¼ˆRTX 5080 + 32 æ ¸å¿ƒ CPUï¼‰ï¼š

| é…ç½® | é€Ÿåº¦ (iter/s) | æ¯ Epoch æ™‚é–“ | å‚™è¨» |
| ------ | -------------- | -------------- | ------ |
| batch=32, patches=256 | ~1.8 | ~35 åˆ†é˜ | æ¨è–¦é…ç½® |
| batch=16, patches=256 | ~2.5 | ~50 åˆ†é˜ | OOM å‚™æ¡ˆ |
| batch=32, patches=196 | ~2.2 | ~30 åˆ†é˜ | é€Ÿåº¦å„ªå…ˆ |

**å®Œæ•´è¨“ç·´æ™‚é–“ä¼°ç®—**:

- 30 epochs Ã— 35 åˆ†é˜ = **17.5 å°æ™‚**
- å»ºè­°åˆ†å¤šæ¬¡è¨“ç·´ï¼ˆæ¯æ¬¡ 10 epochsï¼‰ï¼Œå®šæœŸæª¢æŸ¥

---

## 9) å®Œæ•´çš„è¨˜æ†¶é«”å„ªåŒ–æª¢æŸ¥æ¸…å–®

åœ¨é–‹å§‹è¨“ç·´å‰ï¼Œç¢ºèªä»¥ä¸‹æ‰€æœ‰é …ç›®ï¼š

```bash
# æª¢æŸ¥æ¸…å–®è…³æœ¬
cat > scripts/check_memory_config.py << 'EOF'
#!/usr/bin/env python3
import torch
import yaml

def check_config(config_path):
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    checks = {
        "âœ… Mixed precision enabled": config['hardware']['memory_optimization']['mixed_precision'],
        "âœ… Gradient checkpointing enabled": config['hardware']['memory_optimization']['gradient_checkpointing'],
        "âœ… Batch size <= 32": config['training']['batch_size'] <= 32,
        "âœ… Towers frozen": config['model']['freeze_towers'],
        "âœ… Not using SigLIP-large": 'base' in config['model']['siglip2_variant'],
        "âœ… Gradient accumulation configured": config['training']['gradient_accumulation_steps'] >= 2,
    }
    
    print("è¨˜æ†¶é«”å„ªåŒ–é…ç½®æª¢æŸ¥:")
    for check, passed in checks.items():
        print(f"  {check if passed else check.replace('âœ…', 'âŒ')}")
    
    if all(checks.values()):
        print("\nâœ… æ‰€æœ‰æª¢æŸ¥é€šéï¼å¯ä»¥å®‰å…¨è¨“ç·´ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æª¢æŸ¥å¤±æ•—ï¼Œå»ºè­°ä¿®æ­£å¾Œå†è¨“ç·´ã€‚")

if __name__ == "__main__":
    check_config("configs/hardware/rtx5080_16gb.yaml")
EOF

chmod +x scripts/check_memory_config.py
```

---

## ç¸½çµ

### âœ… é‡å°ä½ çš„ç¡¬é«”çš„é—œéµèª¿æ•´

1. **Batch size**: 64 â†’ **32**ï¼ˆå¿…é ˆï¼‰
2. **æ··åˆç²¾åº¦**: å»ºè­° â†’ **å¿…é ˆå•Ÿç”¨**
3. **æ¢¯åº¦ç´¯ç©**: å¯é¸ â†’ **æ¨è–¦ 2-4 æ­¥**
4. **å‡çµ towers**: true â†’ **å¿…é ˆç‚º true**
5. **è¨˜æ†¶é«”ç›£æ§**: åŠ å…¥ VRAM è¿½è¹¤èˆ‡è­¦å‘Š

### ğŸ¯ ä½ çš„å„ªå‹¢

- âœ… **32 æ ¸å¿ƒ CPU** - å¯ä»¥ç”¨æ›´å¤š DataLoader workers
- âœ… **42GB RAM** - è³‡æ–™é è™•ç†å®Œå…¨ç„¡å£“åŠ›
- âœ… **æœ€æ–° CUDA 13.0** - æ”¯æ´æœ€æ–°å„ªåŒ–
- âœ… **1TB+ å„²å­˜ç©ºé–“** - å¯ä»¥å„²å­˜å¤§é‡å¯¦é©—çµæœ

### ğŸ“Š é æœŸæ•ˆèƒ½

- è¨“ç·´é€Ÿåº¦: **~1.8 iter/s** (batch_size=32)
- æ¯ epoch: **~35 åˆ†é˜**
- å®Œæ•´è¨“ç·´ (30 epochs): **~17.5 å°æ™‚**

ä¸‹ä¸€æ­¥å°±æ˜¯æŒ‰ç…§åŸæœ¬çš„ç’°å¢ƒè¨­ç½®æ•™å­¸ï¼Œä½†ä½¿ç”¨é€™ä»½**ç¡¬é«”å„ªåŒ–é…ç½®**ä¾†é€²è¡Œè¨“ç·´ï¼

ç¥å¯¦é©—é †åˆ©ï¼ğŸš€
