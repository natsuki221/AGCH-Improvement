#!/usr/bin/env python3
# scripts/evaluate.py
"""
æ¨¡å‹è©•ä¼°è…³æœ¬

å°æ‡‰æ‰‹å†Šç« ç¯€:
- Â§9.3 é©—è­‰è¿´åœˆ
- Â§12 è©•ä¼°æŒ‡æ¨™

åŠŸèƒ½:
1. è¼‰å…¥æ¨¡å‹èˆ‡ KNN ç´¢å¼•
2. åœ¨é©—è­‰/æ¸¬è©¦é›†ä¸Šé€²è¡Œæ¨è«–
3. è¨ˆç®— mAPã€Precisionã€Recallã€F1 ç­‰æŒ‡æ¨™
4. è¼¸å‡ºè©•ä¼°å ±å‘Š
"""

import argparse
import sys
from pathlib import Path

import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast
from tqdm import tqdm
import numpy as np
from sklearn.metrics import (
    average_precision_score,
    f1_score,
    precision_score,
    recall_score,
    classification_report,
)
import json

# åŠ å…¥ src åˆ° Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from omegaconf import OmegaConf
from siglip2_multimodal_hash.model import MultimodalHashKNN
from siglip2_multimodal_hash.dataset import COCOMultiLabelDataset
from siglip2_multimodal_hash.knn import HashIndex, predict_tags
from siglip2_multimodal_hash.losses import compute_total_loss
from transformers import Siglip2Processor


def parse_args():
    parser = argparse.ArgumentParser(description="æ¨¡å‹è©•ä¼°")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹ checkpoint è·¯å¾‘")
    parser.add_argument(
        "--index",
        type=str,
        default=None,
        help="KNN ç´¢å¼•è·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰ã€‚å¦‚æœæä¾›ï¼Œä½¿ç”¨ KNN æ¨è«–",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="val",
        choices=["train", "val", "test"],
        help="è©•ä¼°çš„è³‡æ–™é›†åˆ‡åˆ†",
    )
    parser.add_argument("--batch_size", type=int, default=64, help="æ¨è«– batch size")
    parser.add_argument("--num_workers", type=int, default=8, help="DataLoader workers æ•¸é‡")
    parser.add_argument("--threshold", type=float, default=0.5, help="åˆ†é¡é–¾å€¼")
    parser.add_argument("--k", type=int, default=20, help="KNN é„°å±…æ•¸é‡")
    parser.add_argument("--tau", type=float, default=0.07, help="KNN softmax æº«åº¦åƒæ•¸")
    parser.add_argument("--fold_idx", type=int, default=None, help="K-Fold æ¨¡å¼ä¸‹çš„ fold ç´¢å¼•")
    parser.add_argument("--output", type=str, default=None, help="è©•ä¼°çµæœè¼¸å‡ºè·¯å¾‘ï¼ˆJSONï¼‰")
    parser.add_argument("--use_knn", action="store_true", help="ä½¿ç”¨ KNN æ¨è«–ï¼ˆéœ€è¦ --indexï¼‰")
    return parser.parse_args()


def evaluate_with_classifier(
    model: MultimodalHashKNN, dataloader: DataLoader, loss_config, threshold: float = 0.5
) -> dict:
    """
    ä½¿ç”¨åˆ†é¡å™¨é ­é€²è¡Œè©•ä¼°

    Returns:
        è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    model.eval()

    all_logits = []
    all_labels = []
    total_loss = 0

    print("\nä½¿ç”¨åˆ†é¡å™¨é ­é€²è¡Œè©•ä¼°...")

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è©•ä¼°ä¸­"):
            pixel_values = batch["pixel_values"].cuda()
            input_ids = batch["input_ids"].cuda()
            attention_mask = batch["attention_mask"].cuda()
            labels = batch["labels"].cuda()

            with autocast(dtype=torch.float16):
                outputs = model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    return_components=True,
                )

                loss_dict = compute_total_loss(outputs, labels, loss_config)

            total_loss += loss_dict["total"].item()
            all_logits.append(outputs["logits"].cpu())
            all_labels.append(labels.cpu())

    # åˆä½µ
    all_logits = torch.cat(all_logits, dim=0)
    all_labels = torch.cat(all_labels, dim=0)

    # è¨ˆç®—æŒ‡æ¨™
    y_true = all_labels.numpy()
    y_scores = torch.sigmoid(all_logits).numpy()
    y_pred = (y_scores > threshold).astype(int)

    metrics = {
        "loss": total_loss / len(dataloader),
        "mAP_macro": average_precision_score(y_true, y_scores, average="macro"),
        "mAP_micro": average_precision_score(y_true, y_scores, average="micro"),
        "mAP_weighted": average_precision_score(y_true, y_scores, average="weighted"),
        "f1_micro": f1_score(y_true, y_pred, average="micro", zero_division=0),
        "f1_macro": f1_score(y_true, y_pred, average="macro", zero_division=0),
        "f1_weighted": f1_score(y_true, y_pred, average="weighted", zero_division=0),
        "precision_micro": precision_score(y_true, y_pred, average="micro", zero_division=0),
        "precision_macro": precision_score(y_true, y_pred, average="macro", zero_division=0),
        "recall_micro": recall_score(y_true, y_pred, average="micro", zero_division=0),
        "recall_macro": recall_score(y_true, y_pred, average="macro", zero_division=0),
    }

    return metrics


def evaluate_with_knn(
    model: MultimodalHashKNN,
    dataloader: DataLoader,
    hash_index: HashIndex,
    k: int = 20,
    tau: float = 0.07,
    threshold: float = 0.5,
) -> dict:
    """
    ä½¿ç”¨ KNN é€²è¡Œè©•ä¼°

    Returns:
        è©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    model.eval()

    all_hashes = []
    all_labels = []

    print("\nä½¿ç”¨ KNN é€²è¡Œè©•ä¼°...")

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="æå– Hash"):
            pixel_values = batch["pixel_values"].cuda()
            input_ids = batch["input_ids"].cuda()
            attention_mask = batch["attention_mask"].cuda()
            labels = batch["labels"].numpy()

            h = model.get_hash(
                pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask
            )

            all_hashes.append(h.cpu().numpy())
            all_labels.append(labels)

    # åˆä½µ
    all_hashes = np.vstack(all_hashes)
    all_labels = np.vstack(all_labels)

    print(f"\né€²è¡Œ KNN æ¨è«– (K={k}, tau={tau})...")

    # KNN æ¨è«–
    predictions = predict_tags(
        query_hash=all_hashes,
        index=hash_index,
        k=k,
        tau=tau,
        voting_strategy="softmax",
        top_n=10,  # è¿”å› Top-10
    )

    # è¨ˆç®—æŒ‡æ¨™
    y_true = all_labels
    y_scores = predictions["tag_scores"]
    y_pred = (y_scores > threshold).astype(int)

    metrics = {
        "mAP_macro": average_precision_score(y_true, y_scores, average="macro"),
        "mAP_micro": average_precision_score(y_true, y_scores, average="micro"),
        "mAP_weighted": average_precision_score(y_true, y_scores, average="weighted"),
        "f1_micro": f1_score(y_true, y_pred, average="micro", zero_division=0),
        "f1_macro": f1_score(y_true, y_pred, average="macro", zero_division=0),
        "f1_weighted": f1_score(y_true, y_pred, average="weighted", zero_division=0),
        "precision_micro": precision_score(y_true, y_pred, average="micro", zero_division=0),
        "precision_macro": precision_score(y_true, y_pred, average="macro", zero_division=0),
        "recall_micro": recall_score(y_true, y_pred, average="micro", zero_division=0),
        "recall_macro": recall_score(y_true, y_pred, average="macro", zero_division=0),
        "knn_k": k,
        "knn_tau": tau,
    }

    return metrics


def print_metrics(metrics: dict, title: str = "è©•ä¼°çµæœ"):
    """æ ¼å¼åŒ–è¼¸å‡ºè©•ä¼°æŒ‡æ¨™"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Š {title}")
    print("=" * 60)

    if "loss" in metrics:
        print(f"\næå¤±: {metrics['loss']:.4f}")

    print("\nâ”€â”€ mAP â”€â”€")
    print(f"  Macro:    {metrics['mAP_macro']:.4f}")
    print(f"  Micro:    {metrics['mAP_micro']:.4f}")
    print(f"  Weighted: {metrics['mAP_weighted']:.4f}")

    print("\nâ”€â”€ F1 Score â”€â”€")
    print(f"  Macro:    {metrics['f1_macro']:.4f}")
    print(f"  Micro:    {metrics['f1_micro']:.4f}")
    print(f"  Weighted: {metrics['f1_weighted']:.4f}")

    print("\nâ”€â”€ Precision â”€â”€")
    print(f"  Macro:    {metrics['precision_macro']:.4f}")
    print(f"  Micro:    {metrics['precision_micro']:.4f}")

    print("\nâ”€â”€ Recall â”€â”€")
    print(f"  Macro:    {metrics['recall_macro']:.4f}")
    print(f"  Micro:    {metrics['recall_micro']:.4f}")

    if "knn_k" in metrics:
        print(f"\nâ”€â”€ KNN åƒæ•¸ â”€â”€")
        print(f"  K:   {metrics['knn_k']}")
        print(f"  Ï„:   {metrics['knn_tau']}")

    print("=" * 60)


def main():
    args = parse_args()

    print("=" * 60)
    print("æ¨¡å‹è©•ä¼°")
    print("=" * 60)

    # è¼‰å…¥ checkpoint
    print(f"\nè¼‰å…¥ checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location="cpu")
    config = OmegaConf.create(checkpoint["config"])

    # å»ºç«‹æ¨¡å‹
    print("\nå»ºç«‹æ¨¡å‹...")
    model = MultimodalHashKNN(config.model).cuda()
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()

    print(
        f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆEpoch {checkpoint.get('epoch', 'N/A')}, "
        f"Val mAP: {checkpoint.get('val_mAP', 'N/A'):.4f}ï¼‰"
    )

    # å»ºç«‹ DataLoader
    print(f"\nå»ºç«‹ DataLoader (split={args.split})...")

    # åˆ†é–‹è¼‰å…¥ processor çµ„ä»¶ï¼ˆé¿é–‹ Siglip2Processor tokenizer bugï¼‰
    from transformers import AutoImageProcessor, GemmaTokenizerFast

    model_name = config.model.siglip2_variant
    image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=False)
    tokenizer = GemmaTokenizerFast.from_pretrained(model_name)

    class ProcessorWrapper:
        def __init__(self, image_processor, tokenizer):
            self.image_processor = image_processor
            self.tokenizer = tokenizer

        def __call__(self, text=None, images=None, **kwargs):
            result = {}
            return_tensors = kwargs.pop("return_tensors", "pt")
            if images is not None:
                result.update(self.image_processor(images=images, return_tensors=return_tensors))
            if text is not None:
                text_kwargs = {
                    k: v
                    for k, v in kwargs.items()
                    if k in ["padding", "max_length", "truncation", "add_special_tokens"]
                }
                result.update(self.tokenizer(text, return_tensors=return_tensors, **text_kwargs))
            return result

    processor = ProcessorWrapper(image_processor, tokenizer)

    use_k_fold = config.get("k_fold", {}).get("enabled", False) or args.fold_idx is not None

    dataset = COCOMultiLabelDataset(
        data_root=config.paths.data_root,
        processor=processor,
        max_num_patches=config.model.max_num_patches,
        text_max_length=config.model.text_max_length,
        use_k_fold=use_k_fold,
        fold_idx=args.fold_idx if use_k_fold else None,
        fold_split=args.split,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
    )

    print(f"âœ“ è³‡æ–™é›†: {len(dataset):,} å¼µå½±åƒ")

    # é¸æ“‡è©•ä¼°æ–¹å¼
    if args.use_knn and args.index:
        # KNN è©•ä¼°
        print(f"\nè¼‰å…¥ KNN ç´¢å¼•: {args.index}")
        hash_index = HashIndex.load(args.index)

        metrics = evaluate_with_knn(
            model=model,
            dataloader=dataloader,
            hash_index=hash_index,
            k=args.k,
            tau=args.tau,
            threshold=args.threshold,
        )
        title = f"KNN è©•ä¼°çµæœ (K={args.k}, Ï„={args.tau})"
    else:
        # åˆ†é¡å™¨è©•ä¼°
        metrics = evaluate_with_classifier(
            model=model, dataloader=dataloader, loss_config=config.loss, threshold=args.threshold
        )
        title = "åˆ†é¡å™¨è©•ä¼°çµæœ"

    # è¼¸å‡ºçµæœ
    print_metrics(metrics, title)

    # å„²å­˜çµæœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w") as f:
            json.dump(metrics, f, indent=2)

        print(f"\nâœ“ çµæœå·²å„²å­˜: {output_path}")

    print("\nè©•ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
