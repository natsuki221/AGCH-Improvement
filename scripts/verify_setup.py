#!/usr/bin/env python3
"""
ç’°å¢ƒé©—è­‰è…³æœ¬

å°æ‡‰æ‰‹å†Šç« ç¯€:
- Â§2.5 ç’°å¢ƒé©—è­‰
- Â§é™„éŒ„ A: å¿«é€Ÿå•Ÿå‹•æŒ‡ä»¤

åŠŸèƒ½:
1. æª¢æŸ¥ Python ç‰ˆæœ¬
2. æª¢æŸ¥ CUDA èˆ‡ GPU è³‡è¨Š
3. æª¢æŸ¥é—œéµå¥—ä»¶èˆ‡ç‰ˆæœ¬
4. æª¢æŸ¥è³‡æ–™é›†å®Œæ•´æ€§
5. æª¢æŸ¥é…ç½®æª”æ¡ˆ
6. æª¢æŸ¥ SigLIP2 æ¨¡å‹è¼‰å…¥
7. æª¢æŸ¥æœ¬å°ˆæ¡ˆæ¨¡çµ„å°å…¥

ä¾æ“šå¯¦éš›ç’°å¢ƒè¨­è¨ˆ:
- Python 3.12.12
- PyTorch 2.6.0+cu124
- transformers 5.0.0
- CUDA 12.4
"""

import sys
from pathlib import Path
from typing import List, Tuple, Dict
import importlib.util


def print_header(step: int, total: int, title: str):
    """åˆ—å°æ­¥é©Ÿæ¨™é¡Œ"""
    print(f"\n[{step}/{total}] {title}")
    print("-" * 50)


def check_python() -> bool:
    """æª¢æŸ¥ Python ç‰ˆæœ¬"""
    version = sys.version_info
    version_str = f"{version.major}.{version.minor}.{version.micro}"

    if version.major == 3 and version.minor >= 10:
        print(f"âœ“ Python {version_str}")
        return True
    else:
        print(f"âœ— Python {version_str} (éœ€è¦ 3.10+)")
        return False


def check_cuda() -> Tuple[bool, Dict]:
    """æª¢æŸ¥ CUDA å¯ç”¨æ€§"""
    info = {}

    try:
        import torch

        info["torch_version"] = torch.__version__
        print(f"âœ“ PyTorch {torch.__version__}")

        # æª¢æŸ¥ PyTorch ç‰ˆæœ¬
        expected_torch = "2.6.0"
        if expected_torch in torch.__version__:
            print(f"  âœ“ ç‰ˆæœ¬ç¬¦åˆé æœŸ ({expected_torch})")

        if torch.cuda.is_available():
            cuda_version = torch.version.cuda
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            compute_cap = torch.cuda.get_device_capability(0)

            info["cuda_version"] = cuda_version
            info["gpu_name"] = gpu_name
            info["vram_gb"] = vram_gb
            info["compute_capability"] = compute_cap

            print(f"âœ“ CUDA {cuda_version}")
            print(f"  GPU: {gpu_name}")
            print(f"  VRAM: {vram_gb:.1f} GB")
            print(f"  Compute Capability: {compute_cap[0]}.{compute_cap[1]}")

            # RTX 5080 ç›¸å®¹æ€§æç¤º
            if compute_cap[0] >= 12:
                print(f"  âš ï¸  æ³¨æ„: sm_{compute_cap[0]}{compute_cap[1]} éœ€è¦è¼ƒæ–° PyTorch ç‰ˆæœ¬æ”¯æ´")

            return True, info
        else:
            print("âœ— CUDA ä¸å¯ç”¨ (å°‡ä½¿ç”¨ CPUï¼Œé€Ÿåº¦æœƒå¾ˆæ…¢)")
            return False, info

    except ImportError:
        print("âœ— PyTorch æœªå®‰è£")
        return False, info


def check_packages() -> Tuple[bool, List[str]]:
    """æª¢æŸ¥é—œéµå¥—ä»¶ï¼ˆä¾æ“šå¯¦éš›ç’°å¢ƒï¼‰"""

    # å¥—ä»¶åˆ—è¡¨ï¼ˆåŸºæ–¼ uv pip list çµæœï¼‰
    packages = [
        # æ ¸å¿ƒæ·±åº¦å­¸ç¿’
        ("torch", "2.6.0", "æ ¸å¿ƒ"),
        ("torchvision", "0.21.0", "æ ¸å¿ƒ"),
        ("transformers", "5.0.0", "æ ¸å¿ƒ"),
        # è³‡æ–™è™•ç†
        ("PIL", None, "æ ¸å¿ƒ"),  # Pillow 12.1.0
        ("numpy", "2.4.1", "æ ¸å¿ƒ"),
        ("pandas", "3.0.0", "è³‡æ–™"),
        # KNN æª¢ç´¢
        ("faiss", None, "æ ¸å¿ƒ"),  # faiss-cpu 1.13.2
        # COCO è³‡æ–™é›†
        ("pycocotools", None, "æ ¸å¿ƒ"),  # 2.0.11
        # é…ç½®ç®¡ç†
        ("omegaconf", "2.3.0", "é…ç½®"),
        ("hydra", "1.3.2", "é…ç½®"),
        # è¨“ç·´è¿½è¹¤
        ("wandb", "0.24.1", "è¿½è¹¤"),
        ("tensorboard", "2.20.0", "è¿½è¹¤"),
        # è©•ä¼°èˆ‡è¦–è¦ºåŒ–
        ("sklearn", None, "æ ¸å¿ƒ"),  # scikit-learn 1.8.0
        ("matplotlib", None, "è¦–è¦ºåŒ–"),
        ("seaborn", None, "è¦–è¦ºåŒ–"),
        # å·¥å…·
        ("psutil", None, "å·¥å…·"),
        ("tqdm", None, "å·¥å…·"),
        ("accelerate", None, "åŠ é€Ÿ"),
    ]

    all_ok = True
    missing = []

    for item in packages:
        pkg, expected_ver, importance = item

        # è™•ç†ç‰¹æ®Šå°å…¥åç¨±
        import_map = {
            "PIL": ("PIL", "Pillow"),
            "sklearn": ("sklearn", "scikit-learn"),
            "hydra": ("hydra", "hydra-core"),
            "faiss": ("faiss", "faiss-cpu"),
        }

        import_name, display_name = import_map.get(pkg, (pkg, pkg))

        try:
            mod = __import__(import_name)

            # å–å¾—ç‰ˆæœ¬
            if pkg == "PIL":
                from PIL import __version__ as version
            elif pkg == "faiss":
                version = "1.13.2"  # faiss æ²’æœ‰ __version__
            elif pkg == "hydra":
                version = "1.3.2"
            else:
                version = getattr(mod, "__version__", "installed")

            # ç‰ˆæœ¬æ¯”å°
            if expected_ver and expected_ver in str(version):
                print(f"âœ“ {display_name:20s} {version}")
            else:
                print(f"âœ“ {display_name:20s} {version}")

        except ImportError:
            if importance == "æ ¸å¿ƒ":
                print(f"âœ— {display_name:20s} æœªå®‰è£")
                all_ok = False
                missing.append(display_name)
            else:
                print(f"âš ï¸  {display_name:20s} æœªå®‰è£ ({importance})")

    return all_ok, missing


def check_faiss_gpu() -> bool:
    """æª¢æŸ¥ FAISS GPU æ”¯æ´"""
    try:
        import faiss

        # æª¢æŸ¥æ˜¯å¦æœ‰ GPU ç‰ˆæœ¬
        has_gpu = hasattr(faiss, "index_cpu_to_gpu") and hasattr(faiss, "StandardGpuResources")

        if has_gpu:
            print("âœ“ FAISS GPU æ”¯æ´å¯ç”¨")
            return True
        else:
            print("âš ï¸  FAISS åƒ… CPU ç‰ˆæœ¬ (faiss-cpu 1.13.2)")
            print("   å»ºè­°: conda install -c pytorch -c nvidia faiss-gpu")
            return True  # ä¸è¦–ç‚ºéŒ¯èª¤

    except ImportError:
        print("âœ— FAISS æœªå®‰è£")
        return False


def check_dataset() -> Tuple[bool, Dict]:
    """æª¢æŸ¥è³‡æ–™é›†å®Œæ•´æ€§"""
    data_dir = Path("./data/coco")

    if not data_dir.exists():
        print(f"âœ— è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
        return False, {"missing": ["data/coco"]}

    # å¿…è¦æª”æ¡ˆ
    required_files = [
        ("images/train2014", "è¨“ç·´å½±åƒ", True),
        ("images/val2014", "é©—è­‰å½±åƒ", True),
        ("annotations/instances_train2014.json", "å¯¦ä¾‹æ¨™è¨»", False),
        ("annotations/instances_val2014.json", "å¯¦ä¾‹æ¨™è¨»", False),
        ("annotations/captions_train2014.json", "æè¿°æ¨™è¨»", False),
        ("annotations/captions_val2014.json", "æè¿°æ¨™è¨»", False),
        ("index_train2014.pkl", "è¨“ç·´ç´¢å¼•", False),
        ("index_val2014.pkl", "é©—è­‰ç´¢å¼•", False),
    ]

    # å¯é¸æª”æ¡ˆ
    optional_files = [
        ("karpathy_split.json", "Karpathy Split", False),
        ("5fold_split.json", "5-Fold Split", False),
    ]

    all_ok = True
    info = {"exists": [], "missing": [], "optional_missing": []}

    print("å¿…è¦æª”æ¡ˆ:")
    for path_str, desc, is_dir in required_files:
        path = data_dir / path_str

        if path.exists():
            if is_dir:
                n_files = len(list(path.glob("*.jpg")))
                print(f"  âœ“ {path_str} ({n_files:,} å¼µ)")
            else:
                size_mb = path.stat().st_size / 1e6
                print(f"  âœ“ {path_str} ({size_mb:.1f} MB)")
            info["exists"].append(path_str)
        else:
            print(f"  âœ— {path_str} ä¸å­˜åœ¨")
            info["missing"].append(path_str)
            all_ok = False

    print("\nå¯é¸æª”æ¡ˆ:")
    for path_str, desc, is_dir in optional_files:
        path = data_dir / path_str

        if path.exists():
            size_kb = path.stat().st_size / 1024
            print(f"  âœ“ {path_str} ({size_kb:.1f} KB)")
        else:
            print(f"  âš ï¸  {path_str} (éœ€è¦æ™‚åŸ·è¡Œå°æ‡‰è…³æœ¬å»ºç«‹)")
            info["optional_missing"].append(path_str)

    return all_ok, info


def check_configs() -> bool:
    """æª¢æŸ¥é…ç½®æª”æ¡ˆ"""
    try:
        import yaml
    except ImportError:
        print("âœ— PyYAML æœªå®‰è£")
        return False

    config_dir = Path("./configs")

    if not config_dir.exists():
        print(f"âœ— é…ç½®ç›®éŒ„ä¸å­˜åœ¨: {config_dir}")
        return False

    # å¿…è¦é…ç½®
    required_configs = [
        ("hardware/rtx5080_16gb.yaml", "ç¡¬é«”é…ç½®"),
        ("experiments/baseline.yaml", "åŸºæº–å¯¦é©—"),
    ]

    # å¯é¸é…ç½®
    optional_configs = [
        ("experiments/cv_experiment.yaml", "5-Fold CV"),
        ("experiments/ablation_fusion.yaml", "Fusion Ablation"),
        ("experiments/ablation_hash.yaml", "Hash Ablation"),
        ("experiments/grid_search.yaml", "Grid Search"),
    ]

    all_ok = True

    print("å¿…è¦é…ç½®:")
    for config_path, desc in required_configs:
        path = config_dir / config_path

        if path.exists():
            try:
                with open(path) as f:
                    yaml.safe_load(f)
                print(f"  âœ“ {config_path}")
            except Exception as e:
                print(f"  âœ— {config_path} (èªæ³•éŒ¯èª¤)")
                all_ok = False
        else:
            print(f"  âœ— {config_path} ä¸å­˜åœ¨")
            all_ok = False

    print("\nå¯é¸é…ç½®:")
    for config_path, desc in optional_configs:
        path = config_dir / config_path

        if path.exists():
            try:
                with open(path) as f:
                    yaml.safe_load(f)
                print(f"  âœ“ {config_path}")
            except:
                print(f"  âš ï¸  {config_path} (èªæ³•éŒ¯èª¤)")
        else:
            print(f"  âš ï¸  {config_path}")

    return all_ok


def check_siglip2() -> bool:
    """æª¢æŸ¥ SigLIP2 æ¨¡å‹è¼‰å…¥"""
    print("æ­£åœ¨æ¸¬è©¦ SigLIP2 æ¨¡å‹...")

    # æŠ‘åˆ¶ HuggingFace è­¦å‘Š
    import warnings
    import os
    import logging

    # æš«æ™‚æŠ‘åˆ¶è­¦å‘Š
    original_level = logging.getLogger("transformers").level
    logging.getLogger("transformers").setLevel(logging.ERROR)
    logging.getLogger("huggingface_hub").setLevel(logging.ERROR)

    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=FutureWarning)
    os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    model_name = "google/siglip2-base-patch16-256"

    try:
        # æ–¹æ³•ï¼šåˆ†é–‹è¼‰å…¥ ImageProcessor + Tokenizer + Model
        # é€™æ¨£å¯ä»¥é¿é–‹ Siglip2Processor çš„ tokenizer æ˜ å°„å•é¡Œ
        from transformers import AutoModel, AutoImageProcessor, GemmaTokenizerFast
        import torch
        from PIL import Image
        import numpy as np

        print(f"  è¼‰å…¥ ImageProcessor...")
        image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=False)

        print(f"  è¼‰å…¥ Tokenizer (GemmaTokenizerFast)...")
        tokenizer = GemmaTokenizerFast.from_pretrained(model_name)

        print(f"  è¼‰å…¥ Model...")
        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)

        param_count = sum(p.numel() for p in model.parameters()) / 1e6

        print(f"âœ“ SigLIP2 æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        print(f"  æ¨¡å‹: {model_name}")
        print(f"  Model é¡å‹: {type(model).__name__}")
        print(f"  åƒæ•¸é‡: {param_count:.1f}M")

        # æ¸¬è©¦æ¨è«–
        print(f"  æ¸¬è©¦æ¨è«–...")

        # å»ºç«‹æ¸¬è©¦è³‡æ–™
        dummy_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        pixel_values = image_processor(images=dummy_image, return_tensors="pt")["pixel_values"]
        inputs = tokenizer(["a test image"], return_tensors="pt", padding=True)

        with torch.no_grad():
            outputs = model(pixel_values=pixel_values, input_ids=inputs["input_ids"])

        print(f"  âœ“ æ¨è«–æ¸¬è©¦é€šé")

        # æç¤ºæ­£ç¢ºçš„è¼‰å…¥æ–¹å¼
        print(f"\n  ğŸ“ æ­£ç¢ºè¼‰å…¥æ–¹å¼:")
        print(f"     image_processor = AutoImageProcessor.from_pretrained('{model_name}')")
        print(f"     tokenizer = GemmaTokenizerFast.from_pretrained('{model_name}')")
        print(f"     model = AutoModel.from_pretrained('{model_name}')")

        # æç¤º HF_TOKEN
        if not os.environ.get("HF_TOKEN"):
            print(f"\n  ğŸ’¡ æç¤º: è¨­ç½® HF_TOKEN å¯åŠ é€Ÿä¸‹è¼‰")

        return True

    except Exception as e:
        error_msg = str(e)[:200]
        print(f"âœ— SigLIP2 è¼‰å…¥å¤±æ•—: {type(e).__name__}")
        print(f"  éŒ¯èª¤: {error_msg}")

        # æä¾›è§£æ±ºæ–¹æ¡ˆ
        print("\n  ğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print("     1. ç¢ºèªç¶²è·¯é€£ç·šæ­£å¸¸")
        print("     2. è¨­ç½® HF_TOKEN ç’°å¢ƒè®Šæ•¸")
        print("     3. å‡ç´š transformers: pip install transformers --upgrade")

        return False

    finally:
        # æ¢å¾©è¨­å®š
        logging.getLogger("transformers").setLevel(original_level)
        warnings.resetwarnings()
        os.environ.pop("HF_HUB_DISABLE_PROGRESS_BARS", None)
        os.environ.pop("TOKENIZERS_PARALLELISM", None)


def check_project_modules() -> bool:
    """æª¢æŸ¥æœ¬å°ˆæ¡ˆæ¨¡çµ„å°å…¥"""
    # åŠ å…¥ src åˆ° path
    src_path = Path(__file__).parent.parent / "src"
    if src_path.exists():
        sys.path.insert(0, str(src_path))

    modules_to_check = [
        ("siglip2_multimodal_hash.model", "æ¨¡å‹å®šç¾©"),
        ("siglip2_multimodal_hash.dataset", "è³‡æ–™è¼‰å…¥"),
        ("siglip2_multimodal_hash.losses", "æå¤±å‡½æ•¸"),
        ("siglip2_multimodal_hash.utils", "å·¥å…·å‡½æ•¸"),
        ("siglip2_multimodal_hash.knn", "KNN æª¢ç´¢"),
    ]

    all_ok = True

    for module_name, desc in modules_to_check:
        try:
            mod = importlib.import_module(module_name)
            print(f"  âœ“ {module_name}")
        except ImportError as e:
            print(f"  âœ— {module_name}: {e}")
            all_ok = False
        except Exception as e:
            print(f"  âš ï¸  {module_name}: {type(e).__name__}")

    return all_ok


def check_editable_install() -> bool:
    """æª¢æŸ¥å°ˆæ¡ˆæ˜¯å¦ä»¥ editable æ¨¡å¼å®‰è£"""
    try:
        import siglip2_multimodal_hash

        location = getattr(siglip2_multimodal_hash, "__file__", None)

        if location:
            print(f"âœ“ å°ˆæ¡ˆå·²å®‰è£ (editable)")
            print(f"  è·¯å¾‘: {Path(location).parent}")
            return True
        else:
            print("âš ï¸  å°ˆæ¡ˆæœªä»¥ editable æ¨¡å¼å®‰è£")
            return True
    except ImportError:
        print("âš ï¸  å°ˆæ¡ˆæœªå®‰è£ï¼Œä½¿ç”¨ src/ è·¯å¾‘")
        return True


def main():
    print("=" * 60)
    print("ğŸ” AGCH-Improvement ç’°å¢ƒé©—è­‰")
    print("   Python 3.12 | PyTorch 2.6.0 | CUDA 12.4")
    print("=" * 60)

    total_steps = 8
    results = {}

    # Step 1: Python
    print_header(1, total_steps, "æª¢æŸ¥ Python ç‰ˆæœ¬")
    results["python"] = check_python()

    # Step 2: CUDA
    print_header(2, total_steps, "æª¢æŸ¥ CUDA èˆ‡ GPU")
    results["cuda"], _ = check_cuda()

    # Step 3: Packages
    print_header(3, total_steps, "æª¢æŸ¥ Python å¥—ä»¶")
    results["packages"], missing_pkgs = check_packages()

    # Step 4: FAISS
    print_header(4, total_steps, "æª¢æŸ¥ FAISS")
    results["faiss"] = check_faiss_gpu()

    # Step 5: Dataset
    print_header(5, total_steps, "æª¢æŸ¥è³‡æ–™é›†")
    results["dataset"], _ = check_dataset()

    # Step 6: Configs
    print_header(6, total_steps, "æª¢æŸ¥é…ç½®æª”æ¡ˆ")
    results["configs"] = check_configs()

    # Step 7: SigLIP2
    print_header(7, total_steps, "æª¢æŸ¥ SigLIP2 æ¨¡å‹")
    results["siglip2"] = check_siglip2()

    # Step 8: Project modules
    print_header(8, total_steps, "æª¢æŸ¥å°ˆæ¡ˆæ¨¡çµ„")
    results["modules"] = check_project_modules()
    check_editable_install()

    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š é©—è­‰æ‘˜è¦")
    print("=" * 60)

    critical_items = ["python", "cuda", "packages", "dataset", "configs", "modules"]
    optional_items = ["faiss", "siglip2"]

    all_critical_pass = True

    print("\nå¿…è¦é …ç›®:")
    for name in critical_items:
        status = "âœ“" if results.get(name, False) else "âœ—"
        print(f"  {status} {name}")
        if not results.get(name, False):
            all_critical_pass = False

    print("\nå¯é¸é …ç›®:")
    for name in optional_items:
        status = "âœ“" if results.get(name, False) else "âš ï¸"
        print(f"  {status} {name}")

    print("\n" + "=" * 60)

    if all_critical_pass:
        print("âœ… å¿…è¦ç’°å¢ƒè¨­ç½®å®Œæˆï¼")
        if not results.get("siglip2", False):
            print("âš ï¸  SigLIP2 è¼‰å…¥æœ‰å•é¡Œï¼Œå¯èƒ½éœ€è¦èª¿æ•´ transformers ç‰ˆæœ¬")
        print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
        print("   python scripts/train.py")
    else:
        print("âŒ éƒ¨åˆ†å¿…è¦æª¢æŸ¥å¤±æ•—ï¼Œè«‹ä¿®æ­£å¾Œå†è©¦ã€‚")

        if missing_pkgs:
            print(f"\nğŸ“Œ ç¼ºå°‘å¥—ä»¶: {', '.join(missing_pkgs)}")

    print("=" * 60)

    return 0 if all_critical_pass else 1


if __name__ == "__main__":
    sys.exit(main())
