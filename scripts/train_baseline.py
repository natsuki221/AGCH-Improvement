# scripts/train_baseline.py
"""
Baseline è¨“ç·´è…³æœ¬ - SigLIP2-MLP
ç”¨æ–¼å°æ¯”é©—è­‰æ”¹é€²æ–¹æ³• (MultimodalHashKNN) çš„æ•ˆæœ
"""

import os
import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

import torch
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
import hydra
from omegaconf import DictConfig, OmegaConf
from tqdm import tqdm
import numpy as np
from sklearn.metrics import (
    average_precision_score,
    f1_score,
    roc_auc_score,
    precision_score,
    recall_score,
    hamming_loss,
    coverage_error,
    label_ranking_loss,
    label_ranking_average_precision_score,
)

# æœ¬å°ˆæ¡ˆæ¨¡çµ„
from siglip2_multimodal_hash.baseline_model import SigLIP2MLPBaseline
from siglip2_multimodal_hash.dataset import create_dataloader
from siglip2_multimodal_hash.utils import set_seed, get_gpu_memory_info


def compute_baseline_loss(outputs, labels, config):
    """
    è¨ˆç®— Baseline æå¤±ï¼ˆåªæœ‰ BCEï¼Œç„¡ cosine/hash æ­£å‰‡åŒ–ï¼‰

    Args:
        outputs: æ¨¡å‹è¼¸å‡ºå­—å…¸ (åŒ…å« logits)
        labels: ground truth labels
        config: loss é…ç½®

    Returns:
        loss_dict: åŒ…å«å„æå¤±åˆ†é‡çš„å­—å…¸
    """
    logits = outputs["logits"]

    # BCE Lossï¼ˆä¸»è¦æå¤±ï¼‰
    loss_bce = F.binary_cross_entropy_with_logits(logits, labels)

    return {
        "total": loss_bce,
        "bce": loss_bce,
    }


def train_epoch(model, train_loader, optimizer, scheduler, scaler, config):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()

    total_losses = {"total": 0, "bce": 0}
    num_batches = 0
    accumulation_steps = config.training.gradient_accumulation_steps

    optimizer.zero_grad()

    pbar = tqdm(train_loader, desc="Training [Baseline]")

    for batch_idx, batch in enumerate(pbar):
        # ç§»å‹•è³‡æ–™åˆ° GPU
        pixel_values = batch["pixel_values"].cuda()
        input_ids = batch["input_ids"].cuda()
        attention_mask = batch.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.cuda()
        labels = batch["labels"].cuda()

        # æ··åˆç²¾åº¦å‰å‘å‚³æ’­
        with autocast(enabled=config.memory_optimization.mixed_precision):
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_components=True,
            )

            loss_dict = compute_baseline_loss(outputs=outputs, labels=labels, config=config.loss)

            loss = loss_dict["total"] / accumulation_steps

        # åå‘å‚³æ’­
        scaler.scale(loss).backward()

        # æ¢¯åº¦ç´¯ç©
        if (batch_idx + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.gradient_clip_norm)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            if scheduler is not None:
                scheduler.step()

        # ç´¯è¨ˆæå¤±
        total_losses["total"] += loss_dict["total"].item()
        total_losses["bce"] += loss_dict["bce"].item()
        num_batches += 1

        # æ›´æ–°é€²åº¦æ¢
        pbar.set_postfix(
            {
                "loss": f"{loss_dict['total'].item():.4f}",
                "GPU": f"{get_gpu_memory_info()['allocated_gb']:.1f}GB",
            }
        )

    # å¹³å‡æå¤±
    for key in total_losses:
        total_losses[key] /= num_batches

    return total_losses


@torch.no_grad()
def validate(model, val_loader, config):
    """é©—è­‰æ¨¡å‹ï¼Œè¨ˆç®—å¤šæ¨™ç±¤åˆ†é¡å¸¸ç”¨æŒ‡æ¨™"""
    model.eval()

    all_preds = []
    all_labels = []
    total_loss = 0
    num_batches = 0

    for batch in tqdm(val_loader, desc="Validation [Baseline]"):
        # ç§»å‹•è³‡æ–™åˆ° GPU
        pixel_values = batch["pixel_values"].cuda()
        input_ids = batch["input_ids"].cuda()
        attention_mask = batch.get("attention_mask", None)
        if attention_mask is not None:
            attention_mask = attention_mask.cuda()
        labels = batch["labels"].cuda()

        # å‰å‘å‚³æ’­
        with autocast(enabled=config.memory_optimization.mixed_precision):
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_components=True,
            )

            loss_dict = compute_baseline_loss(outputs=outputs, labels=labels, config=config.loss)

        total_loss += loss_dict["total"].item()
        num_batches += 1

        # æ”¶é›†é æ¸¬
        probs = torch.sigmoid(outputs["logits"]).cpu().numpy()
        all_preds.append(probs)
        all_labels.append(labels.cpu().numpy())

    # åˆä½µçµæœ
    all_preds = np.concatenate(all_preds, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    pred_binary = (all_preds > 0.5).astype(int)

    # ============================================================
    # å¤šæ¨™ç±¤åˆ†é¡ç ”ç©¶å¸¸ç”¨æŒ‡æ¨™
    # ============================================================

    # 1. Mean Average Precision (mAP) - ä¸»è¦æŒ‡æ¨™
    mAP = average_precision_score(all_labels, all_preds, average="macro")

    # 2. AUC-ROC (macro/micro)
    try:
        auc_macro = roc_auc_score(all_labels, all_preds, average="macro")
        auc_micro = roc_auc_score(all_labels, all_preds, average="micro")
    except ValueError:
        auc_macro = 0.0
        auc_micro = 0.0

    # 3. F1 Scores
    f1_micro = f1_score(all_labels, pred_binary, average="micro", zero_division=0)
    f1_macro = f1_score(all_labels, pred_binary, average="macro", zero_division=0)

    # 4. Precision & Recall
    precision_micro = precision_score(all_labels, pred_binary, average="micro", zero_division=0)
    precision_macro = precision_score(all_labels, pred_binary, average="macro", zero_division=0)
    recall_micro = recall_score(all_labels, pred_binary, average="micro", zero_division=0)
    recall_macro = recall_score(all_labels, pred_binary, average="macro", zero_division=0)

    # 5. Hamming Loss (è¶Šä½è¶Šå¥½)
    h_loss = hamming_loss(all_labels, pred_binary)

    # 6. Ranking æŒ‡æ¨™
    try:
        cov_error = coverage_error(all_labels, all_preds)
        rank_loss = label_ranking_loss(all_labels, all_preds)
        lrap = label_ranking_average_precision_score(all_labels, all_preds)
    except ValueError:
        cov_error = 0.0
        rank_loss = 0.0
        lrap = 0.0

    # 7. Mean Absolute Error
    mae = np.mean(np.abs(all_preds - all_labels))

    return {
        "loss": total_loss / num_batches,
        "mAP": mAP,
        "auc_macro": auc_macro,
        "auc_micro": auc_micro,
        "f1_micro": f1_micro,
        "f1_macro": f1_macro,
        "precision_micro": precision_micro,
        "precision_macro": precision_macro,
        "recall_micro": recall_micro,
        "recall_macro": recall_macro,
        "hamming_loss": h_loss,
        "coverage_error": cov_error,
        "ranking_loss": rank_loss,
        "lrap": lrap,
        "mae": mae,
    }


@hydra.main(
    config_path="../configs", config_name="experiments/siglip2_mlp_baseline", version_base=None
)
def main(raw_config: DictConfig):
    """ä¸»è¨“ç·´å‡½æ•¸"""
    # Hydra æœƒå°‡é…ç½®åŒ…åœ¨è³‡æ–™å¤¾åä¸‹ï¼Œéœ€è¦è§£é–‹
    if "experiments" in raw_config:
        config = raw_config.experiments
    elif "hardware" in raw_config:
        config = raw_config.hardware
    else:
        config = raw_config

    print("=" * 60)
    print("ğŸ“Š SigLIP2-MLP BASELINE è¨“ç·´è…³æœ¬")
    print("=" * 60)
    print("âš ï¸  é€™æ˜¯ Baseline æ¨¡å‹ï¼Œç”¨æ–¼å°æ¯”é©—è­‰æ”¹é€²æ–¹æ³•çš„æ•ˆæœ")
    print("    - ç„¡ Direction/Magnitude åˆ†è§£")
    print("    - ç„¡ Hadamard èåˆ")
    print("    - ç„¡ Hash å±¤")
    print("    - ç„¡ KNN æ¨è«–")
    print("=" * 60)

    # é¡¯ç¤ºé…ç½®
    print(f"\nğŸ“‹ å¯¦é©—: {config.experiment.name}")
    print(
        f"ğŸ“‹ Batch size: {config.training.batch_size} x {config.training.gradient_accumulation_steps} = {config.training.batch_size * config.training.gradient_accumulation_steps}"
    )
    print(f"ğŸ“‹ Epochs: {config.training.num_epochs}")

    # è¨­å®š seed
    set_seed(config.experiment.seed)

    # æª¢æŸ¥ CUDA
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA ä¸å¯ç”¨ï¼æ­¤è…³æœ¬éœ€è¦ GPU")

    print(f"\nğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ–¥ï¸  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # åˆå§‹åŒ– wandb (å¯é¸)
    if config.logging.use_wandb:
        try:
            import wandb

            wandb.init(
                project=config.logging.wandb_project,
                entity=(
                    config.logging.wandb_entity
                    if config.logging.wandb_entity != "your-username"
                    else None
                ),
                config=OmegaConf.to_container(config, resolve=True),
                name=config.experiment.name,
                tags=["baseline"],
            )
            use_wandb = True
        except Exception as e:
            print(f"âš ï¸  Wandb åˆå§‹åŒ–å¤±æ•—: {e}")
            use_wandb = False
    else:
        use_wandb = False

    # å»ºç«‹æ¨¡å‹
    print("\nğŸ“¦ å»ºç«‹ Baseline æ¨¡å‹...")
    model = SigLIP2MLPBaseline(config.model).cuda()

    # é¡¯ç¤ºè¨˜æ†¶é«”è³‡è¨Š
    mem_info = get_gpu_memory_info()
    print(
        f"âœ“ æ¨¡å‹è¼‰å…¥å¾Œ GPU è¨˜æ†¶é«”: {mem_info['allocated_gb']:.2f}GB / {mem_info['total_gb']:.1f}GB"
    )

    # è¨ˆç®—å¯è¨“ç·´åƒæ•¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(
        f"âœ“ å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)"
    )

    # å»ºç«‹ DataLoader
    print("\nğŸ“‚ å»ºç«‹ DataLoader...")

    # æª¢æŸ¥æ˜¯å¦ç‚º K-Fold æ¨¡å¼
    use_k_fold = config.get("k_fold", {}).get("enabled", False)
    fold_idx = config.get("k_fold", {}).get("current_fold", None) if use_k_fold else None

    if use_k_fold:
        print(f"ğŸ“‹ K-Fold æ¨¡å¼: Fold {fold_idx}")

    train_loader = create_dataloader(config, split="train", fold_idx=fold_idx)
    val_loader = create_dataloader(config, split="val", fold_idx=fold_idx)
    print(f"âœ“ è¨“ç·´é›†: {len(train_loader)} batches")
    print(f"âœ“ é©—è­‰é›†: {len(val_loader)} batches")

    # å»ºç«‹ optimizer
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.optimizer.lr,
        weight_decay=config.optimizer.weight_decay,
        betas=tuple(config.optimizer.betas),
    )

    # å»ºç«‹ scheduler
    total_steps = (
        len(train_loader)
        * config.training.num_epochs
        // config.training.gradient_accumulation_steps
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=total_steps, eta_min=config.scheduler.min_lr
    )

    # æ··åˆç²¾åº¦ scaler
    scaler = GradScaler(enabled=config.memory_optimization.mixed_precision)

    # å»ºç«‹å„²å­˜ç›®éŒ„
    base_save_dir = Path("outputs/checkpoints")
    exp_name = config.experiment.name
    save_dir = base_save_dir / exp_name
    save_dir.mkdir(parents=True, exist_ok=True)

    # è¨“ç·´è¿´åœˆ
    best_val_map = 0
    patience_counter = 0

    print("\nğŸš€ é–‹å§‹ Baseline è¨“ç·´...")
    for epoch in range(config.training.num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{config.training.num_epochs} [BASELINE]")
        print(f"{'='*60}")

        # è¨“ç·´
        train_losses = train_epoch(model, train_loader, optimizer, scheduler, scaler, config)
        print(f"Train Loss: {train_losses['total']:.4f} (BCE only)")

        # é©—è­‰
        val_metrics = validate(model, val_loader, config)
        print(
            f"Val Loss: {val_metrics['loss']:.4f}, "
            f"mAP: {val_metrics['mAP']:.4f}, "
            f"AUC: {val_metrics['auc_macro']:.4f}, "
            f"F1-Macro: {val_metrics['f1_macro']:.4f}"
        )

        # è¨˜éŒ„åˆ° wandb
        if use_wandb:
            wandb.log(
                {
                    "epoch": epoch,
                    "train/loss": train_losses["total"],
                    "train/loss_bce": train_losses["bce"],
                    "val/loss": val_metrics["loss"],
                    "val/mAP": val_metrics["mAP"],
                    "val/auc_macro": val_metrics["auc_macro"],
                    "val/f1_macro": val_metrics["f1_macro"],
                    "lr": optimizer.param_groups[0]["lr"],
                    "gpu_memory_gb": get_gpu_memory_info()["allocated_gb"],
                }
            )

        # å„²å­˜æœ€ä½³æ¨¡å‹
        if val_metrics["mAP"] > best_val_map:
            best_val_map = val_metrics["mAP"]
            patience_counter = 0

            checkpoint = {
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict(),
                "val_metrics": val_metrics,
                "val_mAP": val_metrics["mAP"],
                "config": OmegaConf.to_container(config, resolve=True),
                "model_type": "baseline",  # æ¨™è¨˜ç‚º baseline
            }
            checkpoint_path = save_dir / "best_model.pth"
            torch.save(checkpoint, checkpoint_path)
            print(f"âœ“ å„²å­˜æœ€ä½³ Baseline æ¨¡å‹: {checkpoint_path} (mAP: {val_metrics['mAP']:.4f})")
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= config.training.early_stopping_patience:
            print(f"\nâš ï¸  Early stopping triggered after {epoch+1} epochs")
            break

    print("\n" + "=" * 60)
    print("âœ… Baseline è¨“ç·´å®Œæˆï¼")
    print(f"æœ€ä½³ Val mAP: {best_val_map:.4f}")
    print("=" * 60)

    if use_wandb:
        wandb.finish()


if __name__ == "__main__":
    main()
