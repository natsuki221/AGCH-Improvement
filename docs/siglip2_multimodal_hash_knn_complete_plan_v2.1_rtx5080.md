# å¤šæ¨¡æ…‹åœ–æ–‡å¤šæ¨™ç±¤åˆ†é¡å®Œæ•´å¯¦é©—è¨ˆç•«
# SigLIP 2 + æ–¹å‘/å¹…åº¦åˆ†è§£ + Hadamard èåˆ + Hash + KNN

> **ç‰ˆæœ¬**: v2.1 (RTX 5080 16GB å„ªåŒ–ç‰ˆ)  
> **æ—¥æœŸ**: 2026-01-30  
> **ç¡¬é«”**: RTX 5080 16GB | 32-core CPU | 42GB RAM | CUDA 13.0  
> **ç›®æ¨™**: åœ¨ MS-COCO è³‡æ–™é›†ä¸Šå¯¦ç¾é«˜æ•ˆèƒ½çš„åœ–æ–‡å¤šæ¨™ç±¤åˆ†é¡ç³»çµ±

---

## ğŸ“‹ æ›´æ–°æ—¥èªŒ (v2.1)

### é‡å° RTX 5080 16GB çš„ä¸»è¦å„ªåŒ–
- âœ… Batch size: 64 â†’ **32** (é…åˆæ¢¯åº¦ç´¯ç©æ¨¡æ“¬ batch 64)
- âœ… æ··åˆç²¾åº¦è¨“ç·´: å»ºè­° â†’ **å¿…é ˆå•Ÿç”¨** (ç¯€çœ 40% VRAM)
- âœ… æ¢¯åº¦ç´¯ç©: å¯é¸ â†’ **å¿…é ˆä½¿ç”¨** (2-4 æ­¥)
- âœ… è¨˜æ†¶é«”ç›£æ§: æ–°å¢ VRAM å¯¦æ™‚è¿½è¹¤èˆ‡è­¦å‘Š
- âœ… DataLoader å„ªåŒ–: åˆ©ç”¨ 32 æ ¸å¿ƒ CPU (`num_workers=16`)
- âœ… PyTorch ç‰ˆæœ¬: æ›´æ–°è‡³æ”¯æ´ CUDA 13.0 çš„ç‰ˆæœ¬
- âœ… è¨˜æ†¶é«”ä¼°ç®—: æä¾›è©³ç´°çš„ 16GB VRAM åˆ†é…è¡¨
- âœ… ç·Šæ€¥æªæ–½: æ–°å¢ OOM æ™‚çš„é™ç´šæ–¹æ¡ˆ

---

## ç›®éŒ„
1. [å•é¡Œå®šç¾©èˆ‡æ ¸å¿ƒæ€æƒ³](#1-å•é¡Œå®šç¾©èˆ‡æ ¸å¿ƒæ€æƒ³)
2. [è³‡æ–™é›†å”è­°](#2-è³‡æ–™é›†å”è­°)
3. [æ¨¡å‹æ¶æ§‹](#3-æ¨¡å‹æ¶æ§‹)
4. [ç†è«–åŸºç¤èˆ‡æ•¸å­¸å…¬å¼](#4-ç†è«–åŸºç¤èˆ‡æ•¸å­¸å…¬å¼)
5. [æå¤±å‡½æ•¸è¨­è¨ˆ](#5-æå¤±å‡½æ•¸è¨­è¨ˆ)
6. [æ¨è«–ç­–ç•¥](#6-æ¨è«–ç­–ç•¥)
7. [å¯¦é©—è¨­è¨ˆ](#7-å¯¦é©—è¨­è¨ˆ)
8. [è¶…åƒæ•¸é…ç½® (â­ RTX 5080 å„ªåŒ–)](#8-è¶…åƒæ•¸é…ç½®-rtx-5080-å„ªåŒ–)
9. [å¯¦ä½œç´°ç¯€ (â­ å«è¨˜æ†¶é«”å„ªåŒ–)](#9-å¯¦ä½œç´°ç¯€-å«è¨˜æ†¶é«”å„ªåŒ–)
10. [è©•ä¼°æŒ‡æ¨™](#10-è©•ä¼°æŒ‡æ¨™)
11. [ç¡¬é«”ç‰¹å®šå„ªåŒ– (â­ æ–°å¢)](#11-ç¡¬é«”ç‰¹å®šå„ªåŒ–)
12. [åƒè€ƒæ–‡ç»](#12-åƒè€ƒæ–‡ç»)
13. [é™„éŒ„](#13-é™„éŒ„)

---

## 1) å•é¡Œå®šç¾©èˆ‡æ ¸å¿ƒæ€æƒ³

### 1.1 ä»»å‹™å®šç¾©
- **è¼¸å…¥**: åœ–ç‰‡ `image` + å°æ‡‰æ–‡å­—æ•˜è¿° `caption`
- **è¼¸å‡º**: `C` å€‹ tags çš„ multi-hot å‘é‡ $y \in \{0,1\}^C$
- **è³‡æ–™é›†**: MS-COCO (80 å€‹ç‰©ä»¶é¡åˆ¥)

### 1.2 æ ¸å¿ƒå‰µæ–°é»
æœ¬ç ”ç©¶æå‡ºä¸€å€‹çµåˆç›£ç£å¼å­¸ç¿’èˆ‡è¿‘é„°æª¢ç´¢çš„æ··åˆæ¶æ§‹ï¼š

1. **æ–¹å‘/å¹…åº¦åˆ†è§£ (æ–¹æ¡ˆ B)**
   - å°‡ embedding åˆ†è§£ç‚ºã€Œæ–¹å‘ã€ï¼ˆèªæ„ï¼‰èˆ‡ã€Œå¹…åº¦ã€ï¼ˆç½®ä¿¡åº¦ï¼‰
   - ç†è«–å‹•æ©Ÿï¼šä¿ç•™å‘é‡çš„å¼·åº¦è³‡è¨Šï¼Œé¿å… L2 æ­£è¦åŒ–éæ—©æ¶ˆé™¤ç½®ä¿¡åº¦è¨Šè™Ÿ

2. **Hadamard ä¹˜ç©èåˆ**
   - æ•æ‰è·¨æ¨¡æ…‹çš„ dimension-wise å…±ç¾æ¨¡å¼ (co-activation pattern)
   - åƒè€ƒ VQA é ˜åŸŸçš„ MCB/MUTAN æ–¹æ³•

3. **å¯å­¸ç¿’ Hash å±¤**
   - åˆ©æ–¼é«˜æ•ˆè¿‘é„°æª¢ç´¢ (Hamming space)
   - æ”¯æ´å¤§è¦æ¨¡è³‡æ–™åº«çš„å¿«é€Ÿæª¢ç´¢

4. **KNN åŠ æ¬ŠæŠ•ç¥¨**
   - çµåˆç›£ç£å¼èˆ‡éåƒæ•¸å¼åˆ†é¡çš„å„ªå‹¢
   - æä¾›å¯è§£é‡‹æ€§ï¼ˆå¯è¦–è¦ºåŒ–é„°å±…æ¨£æœ¬ï¼‰

### 1.3 æ–¹æ³•å„ªå‹¢
- **å¯æ“´å±•æ€§**: Hash å±¤æ”¯æ´ç™¾è¬ç´šè³‡æ–™åº«æª¢ç´¢
- **å¯è§£é‡‹æ€§**: KNN æä¾›è¦–è¦ºåŒ–è§£é‡‹è·¯å¾‘
- **éˆæ´»æ€§**: å¯å‹•æ…‹æ–°å¢é¡åˆ¥ï¼ˆæ›´æ–° indexï¼‰è€Œç„¡éœ€é‡æ–°è¨“ç·´åˆ†é¡å™¨

---

## 2) è³‡æ–™é›†å”è­°

### 2.1 MS-COCO åŸºæœ¬è³‡è¨Š
- **ç‰ˆæœ¬**: COCO 2014 (train2014 + val2014)
- **å½±åƒæ•¸é‡**: 
  - è¨“ç·´é›†: ~82,783 å¼µ
  - é©—è­‰é›†: ~40,504 å¼µ
- **ç‰©ä»¶é¡åˆ¥**: 80 å€‹ (detection annotations)
- **Captions**: æ¯å¼µåœ–ç‰‡æœ‰ 5 å€‹äººå·¥æ¨™è¨»çš„ captions

### 2.2 å¯¦é©—åˆ‡åˆ†å”è­°
æ¡ç”¨ **Karpathy split**ï¼ˆå½±åƒæª¢ç´¢èˆ‡ captioning ç¤¾ç¾¤æ¨™æº–ï¼‰ï¼š

| Split | å½±åƒæ•¸é‡ | ç”¨é€” |
|-------|---------|------|
| Train | 113,287 | æ¨¡å‹è¨“ç·´ |
| Val | 5,000 | è¶…åƒæ•¸èª¿æ•´ã€early stopping |
| Test | 5,000 | æœ€çµ‚è©•ä¼° |

**è¨»**: Karpathy split é‡æ–°çµ„ç¹”äº† COCO 2014 train/valï¼Œæ›´é©åˆ caption-image é…å°ä»»å‹™ã€‚

### 2.3 æ¨™ç±¤å®šç¾©
- **Tag ä¾†æº**: ä½¿ç”¨ COCO instance annotations ä¸­çš„ 80 å€‹ç‰©ä»¶é¡åˆ¥
- **æ¨™ç±¤æ ¼å¼**: Multi-hot vector $y \in \{0,1\}^{80}$
- **æ­£æ¨£æœ¬å®šç¾©**: è‹¥å½±åƒä¸­å‡ºç¾è©²ç‰©ä»¶é¡åˆ¥ï¼ˆä¸é™ instance æ•¸é‡ï¼‰

### 2.4 Caption è™•ç†
- **è¨“ç·´æ™‚**: æ¯å¼µåœ–ç‰‡éš¨æ©ŸæŠ½æ¨£ 1 å€‹ captionï¼ˆdata augmentationï¼‰
- **é©—è­‰/æ¸¬è©¦æ™‚**: ä½¿ç”¨ç¬¬ 1 å€‹ captionï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰
- **æ–‡å­—é è™•ç†**: ä½¿ç”¨ SigLIP2Processor çš„æ¨™æº– tokenization

### 2.5 å½±åƒé è™•ç†
```python
# ä½¿ç”¨ NaFlex æ¨¡å¼ï¼ˆå‹•æ…‹è§£æåº¦ï¼‰
processor = Siglip2Processor.from_pretrained(
    "google/siglip2-base-patch16-256",
    size={"max_num_patches": 256}  # RTX 5080 16GB å¯æ‰¿å—
)
# è‡ªå‹•è™•ç† resizeã€normalizeï¼ˆä½¿ç”¨ SigLIP2 é è¨“ç·´çš„çµ±è¨ˆå€¼ï¼‰
```

---

## 3) æ¨¡å‹æ¶æ§‹

### 3.1 å®Œæ•´æ¶æ§‹åœ–

```mermaid
flowchart TB

subgraph INPUT["è¼¸å…¥å±¤"]
    I["Image<br/>(ä»»æ„è§£æåº¦)"]
    T["Caption<br/>(æ–‡å­—æ•˜è¿°)"]
end

subgraph ENCODER["ç·¨ç¢¼å™¨å±¤ (SigLIP2)"]
    P1["SigLIP2 Processor<br/>(NaFlex mode)"]
    P2["SigLIP2 Processor<br/>(Text tokenizer)"]
    
    IT["Image Tower<br/>(Vision Transformer)<br/>âš ï¸ å¿…é ˆå‡çµ"]
    TT["Text Tower<br/>(Language Transformer)<br/>âš ï¸ å¿…é ˆå‡çµ"]
    
    I --> P1
    T --> P2
    P1 --> IT
    P2 --> TT
    
    IT --> V_img["v_img âˆˆ â„áµˆ<br/>(raw image embedding)"]
    TT --> V_txt["v_txt âˆˆ â„áµˆ<br/>(raw text embedding)"]
end

subgraph DECOMPOSE["æ–¹å‘/å¹…åº¦åˆ†è§£å±¤ (æ–¹æ¡ˆ B)"]
    V_img --> CALC_IMG["è¨ˆç®—:<br/>n_img = ||v_img||â‚‚<br/>d_img = v_img / (n_img + Îµ)<br/>m_img = log(n_img + Îµ)"]
    V_txt --> CALC_TXT["è¨ˆç®—:<br/>n_txt = ||v_txt||â‚‚<br/>d_txt = v_txt / (n_txt + Îµ)<br/>m_txt = log(n_txt + Îµ)"]
    
    CALC_IMG --> D_img["d_img<br/>(æ–¹å‘, unit vector)"]
    CALC_IMG --> M_img["m_img<br/>(å¹…åº¦, log-norm)"]
    
    CALC_TXT --> D_txt["d_txt<br/>(æ–¹å‘, unit vector)"]
    CALC_TXT --> M_txt["m_txt<br/>(å¹…åº¦, log-norm)"]
end

subgraph FUSION["èåˆå±¤ (å¯è¨“ç·´)"]
    D_img --> HADAMARD["Hadamard ä¹˜ç©:<br/>p_dir = d_img âŠ™ d_txt"]
    D_txt --> HADAMARD
    
    D_img --> CONCAT["æ‹¼æ¥:<br/>[d_img; d_txt; p_dir; m_img; m_txt]"]
    D_txt --> CONCAT
    HADAMARD --> CONCAT
    M_img --> CONCAT
    M_txt --> CONCAT
    
    CONCAT --> MLP["Fusion MLP<br/>[3d+2 â†’ 1024 â†’ 512]<br/>+ Dropout + ReLU"]
    MLP --> Z["z âˆˆ â„âµÂ¹Â²<br/>(èåˆ embedding)"]
end

subgraph HASH["Hash å±¤ (å¯è¨“ç·´)"]
    Z --> H_LAYER["Hash Transform:<br/>h = tanh(W_h Â· z + b_h)<br/>h âˆˆ â„á´® (B=64)"]
end

subgraph TRAIN["è¨“ç·´åˆ†æ”¯ (Supervised)"]
    H_LAYER --> HEAD["åˆ†é¡ Head:<br/>logits = W_cls Â· h + b_cls<br/>logits âˆˆ â„á¶œ (C=80)"]
    HEAD --> BCE["BCEWithLogitsLoss"]
    GT["Ground Truth<br/>y_true âˆˆ {0,1}â¸â°"] --> BCE
    
    D_img --> L_COS["Cosine Alignment Loss:<br/>L_cos = 1 - cos(d_img, d_txt)"]
    D_txt --> L_COS
    
    H_LAYER --> L_HASH["Hash Regularization:<br/>L_quant + L_balance + L_decorr"]
    
    BCE --> TOTAL["Total Loss:<br/>L = L_bce + Î±Â·L_cos + Î³Â·L_hash"]
    L_COS --> TOTAL
    L_HASH --> TOTAL
end

subgraph INFERENCE["æ¨è«–åˆ†æ”¯ (KNN)"]
    H_LAYER --> INDEX["Hash Index<br/>(FAISS binary index)"]
    INDEX --> KNN_SEARCH["KNN Search<br/>(Hamming distance)"]
    KNN_SEARCH --> NEIGHBORS["Top-K Neighbors<br/>{(h_i, y_i, sim_i)}"]
    NEIGHBORS --> VOTE["åŠ æ¬ŠæŠ•ç¥¨:<br/>score_c = Î£ w_i Â· y_i,c<br/>w_i = softmax(sim_i / Ï„)"]
    VOTE --> OUTPUT["Top-N Tags<br/>(sorted by score)"]
end

subgraph MEMORY["âš ï¸ è¨˜æ†¶é«”ç®¡ç† (16GB VRAM)"]
    MEM1["Mixed Precision (FP16)<br/>ç¯€çœ 40% VRAM"]
    MEM2["Gradient Accumulation<br/>æ¨¡æ“¬å¤§ batch size"]
    MEM3["Gradient Checkpointing<br/>ç¯€çœ 30% VRAM"]
    MEM4["å®šæœŸæ¸…ç† CUDA å¿«å–"]
end

style INPUT fill:#e1f5ff
style ENCODER fill:#fff4e1
style DECOMPOSE fill:#ffe1f5
style FUSION fill:#e1ffe1
style HASH fill:#ffe1e1
style TRAIN fill:#f5e1ff
style INFERENCE fill:#e1ffff
style MEMORY fill:#ffcccc
```

### 3.2 å„å±¤è©³ç´°èªªæ˜

#### 3.2.1 ç·¨ç¢¼å™¨å±¤ (SigLIP2) âš ï¸ é‡å° 16GB å„ªåŒ–
- **æ¨¡å‹**: `google/siglip2-base-patch16-256` (**ä¸è¦ç”¨ largeï¼**)
- **åƒæ•¸é‡**: ~87M (base)
- **è¼¸å‡ºç¶­åº¦**: $d = 768$ (base)
- **è¨“ç·´ç­–ç•¥**: **å¿…é ˆå‡çµåƒæ•¸**ï¼ˆå¦å‰‡ OOMï¼‰

**NaFlex æ¨¡å¼èªªæ˜**:
- Native Flexible Resolutionï¼ˆåŸç”Ÿå½ˆæ€§è§£æåº¦ï¼‰
- è‡ªå‹•æ ¹æ“šè¼¸å…¥åœ–ç‰‡èª¿æ•´ patch æ•¸é‡ï¼ˆæœ€å¤š `max_num_patches=256`ï¼‰
- å„ªå‹¢ï¼šä¿ç•™ç´°ç¯€çš„åŒæ™‚æ§åˆ¶è¨ˆç®—é‡

**âš ï¸ è¨˜æ†¶é«”å½±éŸ¿**:
- å‡çµæ™‚: ~2.5 GB VRAM (åƒ… forward pass)
- è§£å‡æ™‚: ~10 GB VRAM (å« gradients & optimizer states) âŒ **ä¸å¯è¡Œ**

#### 3.2.2 æ–¹å‘/å¹…åº¦åˆ†è§£å±¤
**ç†è«–å‹•æ©Ÿ**:
- **æ–¹å‘ ($d$)**: æ•æ‰èªæ„ç›¸ä¼¼æ€§ï¼ˆç”¨æ–¼ cosine alignmentï¼‰
- **å¹…åº¦ ($m$)**: ä¿ç•™ã€Œç½®ä¿¡åº¦ã€æˆ–ã€Œç‰¹å¾µå¼·åº¦ã€è¨Šè™Ÿ
  - å‡è¨­ï¼šé è¨“ç·´æ¨¡å‹åœ¨é«˜ç½®ä¿¡åº¦æ¨£æœ¬ä¸Šç”¢ç”Ÿè¼ƒå¤§ norm
  - ä½¿ç”¨ log è®Šæ›ä»¥å£“ç¸®æ•¸å€¼ç¯„åœä¸¦å¹³æ»‘æ¢¯åº¦

**æ•¸å­¸å®šç¾©**:
$$
\begin{aligned}
n &= \|v\|_2 \\
d &= \frac{v}{n + \epsilon} \quad &\text{(unit vector)} \\
m &= \log(n + \epsilon) \quad &\text{(log-norm)}
\end{aligned}
$$

#### 3.2.3 Hadamard èåˆå±¤
**ç†è«–åŸºç¤**:
- Hadamard ä¹˜ç© ($\odot$) æ•æ‰ **dimension-wise çš„ç‰¹å¾µå…±ç¾**
- åœ¨ VQA é ˜åŸŸè¢«è­‰æ˜æœ‰æ•ˆï¼ˆMCB, MUTAN, BAN ç­‰æ–¹æ³•ï¼‰

**ç‰¹å¾µçµ„åˆ**:
$$
x = [d_{img}; d_{txt}; d_{img} \odot d_{txt}; m_{img}; m_{txt}] \in \mathbb{R}^{3d+2}
$$

**è¨˜æ†¶é«”ä½”ç”¨**: ~0.3 GB (å¯è¨“ç·´éƒ¨åˆ†)

#### 3.2.4 Hash å±¤
**è¨­è¨ˆé¸æ“‡**:
- ä½¿ç”¨ $\tanh$ è€Œé $\text{sign}$ ä»¥æ”¯æ´åå‘å‚³æ’­
- è¨“ç·´æ™‚ï¼šsoft binary ($h \in [-1, 1]^B$)
- æ¨è«–æ™‚ï¼šhard binary ($\text{sign}(h) \in \{-1, 1\}^B$)

**è¨˜æ†¶é«”ä½”ç”¨**: ~0.1 GB

---

## 4) ç†è«–åŸºç¤èˆ‡æ•¸å­¸å…¬å¼

### 4.1 SigLIP2 é è¨“ç·´ç›®æ¨™ï¼ˆèƒŒæ™¯çŸ¥è­˜ï¼‰
SigLIP2 ä½¿ç”¨ **Sigmoid Loss** å–ä»£ CLIP çš„ Softmax Lossï¼š

$$
\mathcal{L}_{\text{SigLIP}} = -\sum_{i,j} \left[ y_{ij} \log \sigma(z_{ij}) + (1-y_{ij}) \log(1-\sigma(z_{ij})) \right]
$$

å…¶ä¸­ $z_{ij} = \text{cos}(v_i^{img}, v_j^{txt})$ï¼Œ$y_{ij}$ è¡¨ç¤ºç¬¬ $i$ å¼µåœ–èˆ‡ç¬¬ $j$ å€‹æ–‡å­—æ˜¯å¦åŒ¹é…ã€‚

**å„ªå‹¢**: ç›¸æ¯” CLIPï¼ŒSigLIP ä¸ä¾è³´ batch å…§è² æ¨£æœ¬ï¼Œè¨“ç·´æ›´ç©©å®šã€‚

### 4.2 æ–¹å‘/å¹…åº¦åˆ†è§£çš„æ•¸å­¸è¡¨ç¤º

å°æ–¼ä»»æ„å‘é‡ $v \in \mathbb{R}^d$ï¼š

$$
\begin{aligned}
\|v\|_2 &= \sqrt{\sum_{i=1}^d v_i^2} \\
d &= \frac{v}{\|v\|_2 + \epsilon} \quad &\text{(æ–¹å‘ï¼Œæ»¿è¶³ } \|d\|_2 = 1 \text{)} \\
m &= \log(\|v\|_2 + \epsilon) \quad &\text{(å¹…åº¦ï¼Œæ¨™é‡)}
\end{aligned}
$$

**ç‚ºä»€éº¼ç”¨ log?**
- å£“ç¸®å‹•æ…‹ç¯„åœï¼ˆé¿å…éå¤§çš„ norm ä¸»å°æ¢¯åº¦ï¼‰
- å°æ‡‰æ–¼è³‡è¨Šç†è«–ä¸­çš„ã€Œsurpriseã€æˆ–ã€Œç†µã€æ¦‚å¿µ

### 4.3 Hadamard ä¹˜ç©çš„èªæ„è§£é‡‹

$$
p = d_{img} \odot d_{txt} = \begin{bmatrix} d_{img,1} \cdot d_{txt,1} \\ d_{img,2} \cdot d_{txt,2} \\ \vdots \\ d_{img,d} \cdot d_{txt,d} \end{bmatrix}
$$

**è§£é‡‹**:
- ç¬¬ $i$ ç¶­çš„å€¼ $p_i$ åæ˜ äº†ã€Œè©²ç¶­åº¦ä¸Šå…©å€‹æ¨¡æ…‹çš„æ¿€æ´»ä¸€è‡´æ€§ã€
- è‹¥ $p_i > 0$ï¼šå…©è€…åœ¨è©²ç¶­åº¦ä¸ŠåŒå‘ï¼ˆå¯èƒ½ä»£è¡¨å…±äº«çš„èªæ„ç‰¹å¾µï¼‰
- è‹¥ $p_i < 0$ï¼šå…©è€…åå‘ï¼ˆå¯èƒ½ä»£è¡¨äº’è£œæˆ–çŸ›ç›¾çš„ç‰¹å¾µï¼‰

**èˆ‡å…§ç©çš„å€åˆ¥**:
- å…§ç© $d_{img}^\top d_{txt}$ æ˜¯å–®ä¸€æ¨™é‡ï¼ˆå…¨å±€ç›¸ä¼¼åº¦ï¼‰
- Hadamard ä¿ç•™ $d$ ç¶­è³‡è¨Šï¼ˆå±€éƒ¨äº¤äº’æ¨¡å¼ï¼‰

### 4.4 Hash å‡½æ•¸èˆ‡äºŒå€¼åŒ–

**è¨“ç·´æ™‚çš„ soft hash**:
$$
h = \tanh(W_h z + b_h), \quad h \in [-1, 1]^B
$$

**æ¨è«–æ™‚çš„ hard hash**:
$$
b = \text{sign}(h) = \begin{cases} +1 & \text{if } h_i \geq 0 \\ -1 & \text{if } h_i < 0 \end{cases}
$$

**Hamming distance**:
$$
d_H(b_1, b_2) = \frac{1}{2} \|b_1 - b_2\|_0 = \frac{B - b_1^\top b_2}{2}
$$

### 4.5 KNN åŠ æ¬ŠæŠ•ç¥¨

çµ¦å®š query $q$ åŠå…¶ Top-K é„°å±…é›†åˆ $\mathcal{N}_K(q) = \{(h_i, y_i, s_i)\}_{i=1}^K$ï¼š

**Softmax weighting**:
$$
w_i = \frac{\exp(s_i / \tau)}{\sum_{j=1}^K \exp(s_j / \tau)}
$$

**Tag score aggregation**:
$$
\text{score}_c = \sum_{i=1}^K w_i \cdot y_{i,c}, \quad c = 1, \ldots, C
$$

**è¼¸å‡º Top-N tags**:
$$
\hat{T} = \text{argsort}(\text{score})[-N:]
$$

---

## 5) æå¤±å‡½æ•¸è¨­è¨ˆ

### 5.1 ç¸½é«”æå¤±å‡½æ•¸

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{bce}} + \alpha \mathcal{L}_{\text{cos}} + \gamma \mathcal{L}_{\text{hash}}
$$

**æ³¨æ„**: ç§»é™¤ Euclidean lossï¼ˆå› å…¶èˆ‡ cosine é«˜åº¦è€¦åˆï¼‰

### 5.2 Binary Cross-Entropy Loss (ä¸»è¦ç›£ç£è¨Šè™Ÿ)

$$
\mathcal{L}_{\text{bce}} = -\frac{1}{C} \sum_{c=1}^C \left[ y_c \log \hat{y}_c + (1-y_c) \log(1-\hat{y}_c) \right]
$$

å…¶ä¸­ï¼š
$$
\hat{y}_c = \sigma(\text{logit}_c), \quad \text{logit}_c = (W_{\text{cls}} h + b_{\text{cls}})_c
$$

**é¡åˆ¥ä¸å¹³è¡¡è™•ç†**:
- è€ƒæ…®ä½¿ç”¨ **Focal Loss** æˆ– **class-balanced weights**
- COCO 80 é¡åˆ¥åˆ†å¸ƒä¸å‡ï¼ˆperson å‡ºç¾é »ç‡é é«˜æ–¼ toothbrushï¼‰

### 5.3 Cosine Alignment Loss

$$
\mathcal{L}_{\text{cos}} = 1 - \cos(d_{img}, d_{txt}) = 1 - \frac{d_{img}^\top d_{txt}}{\|d_{img}\|_2 \|d_{txt}\|_2}
$$

ç”±æ–¼ $d$ å·²æ˜¯ unit vectorï¼š
$$
\mathcal{L}_{\text{cos}} = 1 - d_{img}^\top d_{txt}
$$

**ç‰©ç†æ„ç¾©**: é¼“å‹µé…å°çš„åœ–æ–‡åœ¨æ–¹å‘ç©ºé–“ä¸­å°é½Š

### 5.4 Hash Regularization (ä¸‰é …çµ„åˆ)

#### 5.4.1 Quantization Loss (æ¨å‘ Â±1)
$$
\mathcal{L}_{\text{quant}} = \frac{1}{B} \sum_{i=1}^B (|h_i| - 1)^2
$$

#### 5.4.2 Bit Balance Loss (é¿å…æ‰€æœ‰ bit åå‘åŒä¸€æ¥µ)
$$
\mathcal{L}_{\text{balance}} = \frac{1}{B} \sum_{i=1}^B \left( \frac{1}{N} \sum_{n=1}^N h_{n,i} \right)^2
$$

å…¶ä¸­ $N$ æ˜¯ batch sizeï¼Œ$h_{n,i}$ æ˜¯ç¬¬ $n$ å€‹æ¨£æœ¬çš„ç¬¬ $i$ å€‹ bitã€‚

**ç‰©ç†æ„ç¾©**: å¸Œæœ›æ¯å€‹ bit åœ¨ batch ä¸­çš„å‡å€¼æ¥è¿‘ 0ï¼ˆä¸€åŠ +1ï¼Œä¸€åŠ -1ï¼‰

#### 5.4.3 Bit Decorrelation Loss (é¼“å‹µ bit ç¨ç«‹)
$$
\mathcal{L}_{\text{decorr}} = \frac{1}{B^2} \sum_{i \neq j} (\text{Cov}(h_i, h_j))^2
$$

ç°¡åŒ–å¯¦ä½œï¼ˆä½¿ç”¨ Frobenius normï¼‰:
$$
\mathcal{L}_{\text{decorr}} = \|\text{Cov}(H)\|_F^2 - \text{trace}(\text{Cov}(H)^2)
$$

**ç¸½ hash loss**:
$$
\mathcal{L}_{\text{hash}} = \mathcal{L}_{\text{quant}} + \lambda_1 \mathcal{L}_{\text{balance}} + \lambda_2 \mathcal{L}_{\text{decorr}}
$$

---

## 6) æ¨è«–ç­–ç•¥

### 6.1 å»ºç«‹ Hash Index

```python
import faiss
import numpy as np

# 1. æå–è¨“ç·´é›†çš„ hash codes
train_hashes = []  # List of np.ndarray, shape (B,)
train_labels = []  # List of np.ndarray, shape (C,)

for batch in train_loader:
    with torch.no_grad():
        h = model.get_hash(batch)  # shape: (batch_size, B)
        train_hashes.append(h.cpu().numpy())
        train_labels.append(batch['labels'].cpu().numpy())

train_hashes = np.vstack(train_hashes)  # (N_train, B)
train_labels = np.vstack(train_labels)  # (N_train, C)

# 2. äºŒå€¼åŒ–ï¼ˆhard binaryï¼‰
train_binary = (train_hashes > 0).astype(np.uint8)  # {0, 1}^B

# 3. å»ºç«‹ FAISS binary index
index = faiss.IndexBinaryFlat(B)  # Hamming distance index
index.add(train_binary)
```

### 6.2 KNN æª¢ç´¢èˆ‡æŠ•ç¥¨

```python
def predict_tags(query_hash, index, train_labels, K=20, tau=0.07, top_n=5):
    """
    Args:
        query_hash: (B,) torch.Tensor or np.ndarray
        index: faiss.IndexBinaryFlat
        train_labels: (N_train, C) np.ndarray
        K: number of neighbors
        tau: temperature for softmax
        top_n: number of tags to return
    
    Returns:
        predicted_tags: (top_n,) np.ndarray (tag indices)
        scores: (top_n,) np.ndarray (confidence scores)
    """
    # 1. äºŒå€¼åŒ– query
    query_binary = (query_hash > 0).astype(np.uint8).reshape(1, -1)
    
    # 2. KNN æœå°‹ï¼ˆè¿”å› Hamming distancesï¼‰
    distances, indices = index.search(query_binary, K)  # (1, K)
    distances = distances[0]  # (K,)
    indices = indices[0]  # (K,)
    
    # 3. è½‰æ›ç‚º similarityï¼ˆHamming -> cosine-likeï¼‰
    similarities = 1 - distances / B  # [0, 1] range
    
    # 4. Softmax weighting
    weights = np.exp(similarities / tau)
    weights = weights / weights.sum()
    
    # 5. åŠ æ¬ŠæŠ•ç¥¨
    neighbor_labels = train_labels[indices]  # (K, C)
    tag_scores = (weights[:, None] * neighbor_labels).sum(axis=0)  # (C,)
    
    # 6. Top-N
    top_indices = np.argsort(tag_scores)[-top_n:][::-1]
    top_scores = tag_scores[top_indices]
    
    return top_indices, top_scores
```

---

## 7) å¯¦é©—è¨­è¨ˆ

### 7.1 Baseline æ–¹æ³•å°æ¯”

| æ–¹æ³• | æè¿° | ç”¨é€” |
|------|------|------|
| **SigLIP2-MLP** | ç›´æ¥ç”¨ MLP åˆ†é¡å™¨ on `[v_img, v_txt]`ï¼ˆç„¡ decomposition, ç„¡ hash, ç„¡ KNNï¼‰ | è­‰æ˜ hash+KNN çš„å¿…è¦æ€§ |
| **SigLIP2-ZeroShot** | è¨ˆç®— image embedding èˆ‡æ¯å€‹ tag prototypeï¼ˆå¾ tag name ç·¨ç¢¼ï¼‰çš„ cosine similarityï¼Œå– Top-N | è­‰æ˜ç›£ç£å¼è¨“ç·´çš„åƒ¹å€¼ |
| **æ–¹æ¡ˆ A (Direction only)** | æ‹¿æ‰ magnitude åˆ†æ”¯ï¼ˆåƒ…ç”¨ `[d_img, d_txt, p_dir]`ï¼‰ | è­‰æ˜æ–¹æ¡ˆ B çš„åƒ¹å€¼ |
| **Ours-Full** | å®Œæ•´æ¶æ§‹ï¼ˆæ–¹æ¡ˆ B + Hadamard + Hash + KNNï¼‰ | ä¸»è¦æ–¹æ³• |

### 7.2 ç³»çµ±åŒ– Ablation Study

#### Tier 1: æ ¸å¿ƒæ¶æ§‹é¸æ“‡ï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰

| ID | è®Šé‡ | é¸é … | å›ºå®šåƒæ•¸ |
|----|------|------|----------|
| **A1** | Fusion ç­–ç•¥ | concat / +Hadamard / +Hadamard+Magnitude | B=64, K=20, freeze |
| **A2** | Hash bits | ç„¡ hash / 32 / 64 / 128 | å…¶é¤˜åŒ baseline |
| **A3** | KNN vs MLP head | KNN / ç›´æ¥ç”¨åˆ†é¡å™¨ / hybrid | åŒä¸Š |

#### Tier 2: è¨“ç·´ç­–ç•¥ï¼ˆä¸­ç­‰å„ªå…ˆç´šï¼‰

| ID | è®Šé‡ | é¸é … | èªªæ˜ |
|----|------|------|------|
| **B1** | æ˜¯å¦ freeze towers | freeze / âš ï¸ **ä¸å¯è§£å‡** (OOM) | RTX 5080 16GB é™åˆ¶ |
| **B2** | Loss weights | (Î±, Î³, Î»â‚, Î»â‚‚) çµ„åˆ | Grid search: Î± âˆˆ {0.5, 1.0}, Î³ âˆˆ {0.05, 0.1} |
| **B3** | max_num_patches | 256 / âš ï¸ 512 éœ€ç›£æ§ | è©•ä¼°è§£æåº¦å½±éŸ¿ |
| **B4** | é¡åˆ¥ä¸å¹³è¡¡è™•ç† | ç„¡ / Focal Loss / Class Weights | COCO é¡åˆ¥åˆ†å¸ƒä¸å‡ |

#### Tier 3: KNN è¶…åƒæ•¸ï¼ˆæ¬¡è¦å„ªå…ˆç´šï¼‰

| ID | è®Šé‡ | é¸é … | èªªæ˜ |
|----|------|------|------|
| **C1** | K å€¼ | 5 / 10 / 20 / 50 | é„°å±…æ•¸é‡ |
| **C2** | è·é›¢å‡½æ•¸ | cosine(h) / hamming(sign(h)) / hybrid | æª¢ç´¢ç­–ç•¥ |
| **C3** | Voting ç­–ç•¥ | uniform / softmax / rank-based / threshold | åŠ æ¬Šæ–¹å¼ |
| **C4** | tau (temperature) | 0.03 / 0.07 / 0.2 | softmax å¹³æ»‘åº¦ |

### 7.3 å¯¦é©—æµç¨‹

#### éšæ®µ 1: Baseline é©—è­‰ï¼ˆ1-2 å¤©ï¼‰
1. å¯¦ä½œ SigLIP2-MLP baseline
2. å¯¦ä½œ SigLIP2-ZeroShot baseline
3. ç¢ºèªè³‡æ–™è™•ç† pipeline æ­£ç¢º
4. å»ºç«‹è©•ä¼°æµç¨‹

#### éšæ®µ 2: æ ¸å¿ƒæ¶æ§‹å¯¦é©—ï¼ˆ3-5 å¤©ï¼‰
1. å¯¦ä½œå®Œæ•´æ¶æ§‹
2. åŸ·è¡Œ Tier 1 ablations (A1-A3)
3. é¸å‡ºæœ€ä½³é…ç½®

#### éšæ®µ 3: è¨“ç·´ç­–ç•¥å„ªåŒ–ï¼ˆ3-5 å¤©ï¼‰
1. åŸ·è¡Œ Tier 2 ablations (B1-B4)
2. è¶…åƒæ•¸ grid search
3. å­¸ç¿’ç‡èª¿åº¦å¯¦é©—

#### éšæ®µ 4: KNN èª¿å„ªï¼ˆ2-3 å¤©ï¼‰
1. åŸ·è¡Œ Tier 3 ablations (C1-C4)
2. æª¢ç´¢æ•ˆç‡åˆ†æ
3. å¯è§£é‡‹æ€§å¯¦é©—

#### éšæ®µ 5: æœ€çµ‚è©•ä¼°èˆ‡åˆ†æï¼ˆ2-3 å¤©ï¼‰
1. Test set è©•ä¼°
2. éŒ¯èª¤åˆ†æ
3. è¦–è¦ºåŒ–å±•ç¤º
4. æ’°å¯«å ±å‘Š

---

## 8) è¶…åƒæ•¸é…ç½® (â­ RTX 5080 å„ªåŒ–)

### 8.1 ç¡¬é«”è³‡è¨Šç¸½è¦½

```yaml
# å¯¦éš›ç¡¬é«”è¦æ ¼
hardware_info:
  gpu:
    model: "NVIDIA GeForce RTX 5080"
    vram_gb: 16  # âš ï¸ é—œéµé™åˆ¶
    cuda_version: "13.0"
    driver_version: "580.126.09"
    compute_capability: "8.9"  # Ada Lovelace
  
  cpu:
    cores: 32
    threads: 64  # å‡è¨­æ”¯æ´è¶…åŸ·è¡Œç·’
    model: "é«˜éšå·¥ä½œç«™è™•ç†å™¨"
  
  memory:
    ram_gb: 42
    swap_gb: 8  # å»ºè­°è¨­å®š
  
  storage:
    total_tb: 1.1
    ssd: true
```

### 8.2 è¨˜æ†¶é«”ä½”ç”¨ä¼°ç®—è¡¨ï¼ˆ16GB VRAMï¼‰

| çµ„ä»¶ | è¨˜æ†¶é«”ä½”ç”¨ | èªªæ˜ |
|------|-----------|------|
| **SigLIP2-base (å‡çµ)** | ~2.5 GB | åƒ… forward passï¼Œç„¡ gradients |
| **Fusion MLP** | ~0.3 GB | å¯è¨“ç·´åƒæ•¸ |
| **Hash Layer** | ~0.1 GB | å¯è¨“ç·´åƒæ•¸ |
| **Classifier Head** | ~0.05 GB | å¯è¨“ç·´åƒæ•¸ |
| **Optimizer States (AdamW)** | ~1.2 GB | 2x å¯è¨“ç·´åƒæ•¸é‡ |
| **Batch Data (32, mixed precision)** | ~4.0 GB | Images + embeddings (FP16) |
| **Gradients** | ~0.5 GB | åƒ…å¯è¨“ç·´éƒ¨åˆ† |
| **CUDA Kernels & PyTorch** | ~0.5 GB | Framework overhead |
| **é ç•™ç·©è¡** | ~1.0 GB | å®‰å…¨é‚Šç•Œ |
| **ç¸½è¨ˆ** | **~10.2 GB** | âœ… åœ¨ 16GB å…§å®‰å…¨ (63% ä½¿ç”¨ç‡) |

### 8.3 å„ªåŒ–å¾Œçš„é…ç½®æ–‡ä»¶

```yaml
# configs/hardware/rtx5080_16gb.yaml

# ==========================================
# ç¡¬é«”å„ªåŒ–é…ç½® - RTX 5080 16GB å°ˆç”¨
# ==========================================

experiment:
  name: "baseline_rtx5080_16gb"
  version: "v2.1"
  seed: 42
  deterministic: false  # true æœƒæ…¢å¾ˆå¤š

# æ¨¡å‹æ¶æ§‹
model:
  siglip2_variant: "google/siglip2-base-patch16-256"  # âš ï¸ ä¸è¦ç”¨ largeï¼
  max_num_patches: 256  # ä¿å®ˆè¨­å®šï¼Œå¯å˜—è©¦ 512 ä½†éœ€ç›£æ§
  text_max_length: 64
  freeze_towers: true  # âš ï¸ å¿…é ˆç‚º trueï¼Œå¦å‰‡ OOMï¼
  
  # åˆ†è§£å±¤
  decomposer:
    eps: 1.0e-6
  
  # èåˆå±¤
  fusion:
    type: "hadamard_with_magnitude"  # æ–¹æ¡ˆ B
    mlp_dims: [1024, 512]  # è¼¸å…¥: 3*768+2 = 2306
    dropout: 0.1
    activation: "relu"
  
  # Hash å±¤
  hash:
    bits: 64  # 32=å¿«é€Ÿ, 64=å¹³è¡¡, 128=é«˜ç²¾åº¦(éœ€æ›´å¤šè¨˜æ†¶é«”)
    activation: "tanh"
  
  # åˆ†é¡é ­
  classifier:
    num_classes: 80  # COCO categories
    use_bias: true

# æå¤±å‡½æ•¸
loss:
  # BCE Loss (ä¸»è¦)
  bce_weight: 1.0
  use_focal_loss: false  # å¯é¸ï¼šè™•ç†é¡åˆ¥ä¸å¹³è¡¡
  focal_alpha: 0.25
  focal_gamma: 2.0
  
  # Cosine Alignment Loss
  cosine_weight: 1.0  # Î±
  
  # Hash Regularization
  hash_weight: 0.1  # Î³
  hash_reg:
    lambda_balance: 0.1  # Î»â‚
    lambda_decorr: 0.01  # Î»â‚‚

# è¨“ç·´é…ç½® (â­ RTX 5080 å„ªåŒ–)
training:
  # æ‰¹æ¬¡å¤§å° (é—œéµï¼)
  batch_size: 32  # âš ï¸ å¾ 64 é™åˆ° 32
  gradient_accumulation_steps: 2  # âš ï¸ å¿…é ˆä½¿ç”¨ï¼Œæ¨¡æ“¬ batch_size=64
  effective_batch_size: 64  # 32 * 2 = 64
  
  # Epoch èˆ‡é©—è­‰
  num_epochs: 30
  warmup_epochs: 2
  val_every_n_epochs: 1
  
  # æ¢¯åº¦ç®¡ç†
  gradient_clip_norm: 1.0
  max_grad_norm: 1.0
  
  # Early Stopping
  early_stopping_patience: 5
  save_top_k: 3
  monitor_metric: "val_mAP"  # æˆ– "val_f1_macro"

# Optimizer
optimizer:
  type: "adamw"
  lr: 2.0e-4  # âš ï¸ æ¯”åŸæœ¬ 3e-4 ç•¥å°ï¼ˆå›  effective batch size ä¸€æ¨£ï¼‰
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler
scheduler:
  type: "cosine_with_warmup"
  warmup_ratio: 0.1  # warmup_epochs / num_epochs
  min_lr: 1.0e-6
  cosine_cycles: 1

# DataLoader (â­ åˆ©ç”¨ 32 æ ¸å¿ƒ CPU)
dataloader:
  num_workers: 16  # âš ï¸ ä½ æœ‰ 32 æ ¸å¿ƒï¼Œå¯ä»¥ç”¨æ›´å¤š
  prefetch_factor: 3  # é è¼‰å…¥ 3 æ‰¹æ¬¡è³‡æ–™
  pin_memory: true  # åŠ é€Ÿ CPU->GPU å‚³è¼¸
  persistent_workers: true  # ä¿æŒ workers å­˜æ´»
  drop_last: true  # ä¸Ÿæ£„ä¸å®Œæ•´çš„æœ€å¾Œä¸€æ‰¹

# è¨˜æ†¶é«”å„ªåŒ– (â­ é—œéµè¨­å®š)
memory_optimization:
  # æ··åˆç²¾åº¦ (å¿…é ˆï¼)
  mixed_precision: true  # âš ï¸ ç¯€çœ 40% VRAM
  amp_dtype: "float16"  # æˆ– "bfloat16" (å¦‚æœæ”¯æ´)
  
  # Gradient Checkpointing (å¯é¸ï¼Œç¯€çœæ›´å¤šè¨˜æ†¶é«”)
  gradient_checkpointing: false  # å‡çµ towers æ™‚ä¸éœ€è¦
  
  # å¿«å–ç®¡ç†
  empty_cache_steps: 100  # æ¯ 100 æ­¥æ¸…ç†ä¸€æ¬¡ CUDA å¿«å–
  
  # VRAM ç›£æ§
  log_gpu_memory: true
  alert_vram_threshold_gb: 14.5  # è¶…é 14.5GB ç™¼å‡ºè­¦å‘Š

# KNN æ¨è«–é…ç½®
knn:
  K: 20  # number of neighbors
  distance_metric: "hamming"  # or "cosine"
  voting_strategy: "softmax"  # or "uniform", "rank_based"
  tau: 0.07  # temperature for softmax
  top_n_tags: 5  # output top N predictions
  
  # æ¨è«–æ™‚çš„æ‰¹æ¬¡å¤§å°ï¼ˆå¯ä»¥æ¯”è¨“ç·´å¤§ï¼‰
  inference_batch_size: 64

# æ—¥èªŒèˆ‡ç›£æ§
logging:
  log_every_n_steps: 50
  log_gradients: false  # åƒ…åœ¨ debug æ™‚å•Ÿç”¨
  log_weights: false
  
  # Weights & Biases
  use_wandb: true
  wandb_project: "siglip2-multimodal-hash"
  wandb_entity: "your-username"
  
  # TensorBoard
  use_tensorboard: true
  tensorboard_dir: "experiments/tensorboard"

# æª¢æŸ¥é»
checkpointing:
  save_dir: "experiments/checkpoints"
  save_every_n_epochs: 5
  save_last: true
  save_top_k: 3
  filename_format: "epoch={epoch:02d}-val_mAP={val_mAP:.4f}"

# è³‡æ–™å¢å¼· (å¯é¸)
augmentation:
  use_augmentation: false  # SigLIP2 å·²ç¶“å¾ˆå¼·ï¼Œå¯èƒ½ä¸éœ€è¦
  random_flip: false
  color_jitter: false
  random_crop: false
```

### 8.4 ç·Šæ€¥é™ç´šæ–¹æ¡ˆï¼ˆå¦‚æœé‚„æ˜¯ OOMï¼‰

```yaml
# configs/hardware/rtx5080_16gb_emergency.yaml
# ç•¶ baseline é…ç½®ä»ç„¶ OOM æ™‚ä½¿ç”¨

training:
  batch_size: 16  # âš ï¸ å¾ 32 é™åˆ° 16
  gradient_accumulation_steps: 4  # æ¨¡æ“¬ batch_size=64

model:
  max_num_patches: 196  # âš ï¸ å¾ 256 é™åˆ° 196 (14x14 patches)

memory_optimization:
  gradient_checkpointing: true  # âš ï¸ å•Ÿç”¨ï¼Œç¯€çœ 30% VRAM
  empty_cache_steps: 50  # æ›´é »ç¹æ¸…ç†
```

### 8.5 Grid Search é…ç½®ï¼ˆä¾›è‡ªå‹•åŒ–å¯¦é©—ï¼‰

```yaml
# configs/grid_search.yaml

grid_search:
  # Tier 1: æ ¸å¿ƒæ¶æ§‹
  hash_bits: [32, 64, 128]
  fusion_type: ["concat_only", "hadamard", "hadamard_with_magnitude"]
  
  # Tier 2: è¨“ç·´ç­–ç•¥
  cosine_weight: [0.5, 1.0, 2.0]
  hash_weight: [0.05, 0.1, 0.2]
  
  # Tier 3: KNN è¶…åƒæ•¸
  K_neighbors: [10, 20, 50]
  tau: [0.03, 0.07, 0.15]
  
  # è¨˜æ†¶é«”ç›¸é—œï¼ˆæ…ç”¨ï¼‰
  max_num_patches: [256]  # 512 é¢¨éšªå¤ªé«˜ï¼Œä¸å»ºè­° grid search
  batch_size: [32]  # å›ºå®šï¼Œä¸å»ºè­°è®Šå‹•

# ç¸½å¯¦é©—æ•¸ï¼š3*3 + 3*3 + 3*3 = 27 çµ„
# é ä¼°æ™‚é–“ï¼š27 * 17.5 å°æ™‚ = ~472 å°æ™‚ (åˆ†æ•£å¤š GPU åŸ·è¡Œ)
```

---

## 9) å¯¦ä½œç´°ç¯€ (â­ å«è¨˜æ†¶é«”å„ªåŒ–)

### 9.1 é—œéµç¨‹å¼ç¢¼ç‰‡æ®µ

#### 9.1.1 æ–¹å‘/å¹…åº¦åˆ†è§£

```python
import torch
import torch.nn as nn

class DirectionMagnitudeDecomposer(nn.Module):
    def __init__(self, eps=1e-6):
        super().__init__()
        self.eps = eps
    
    def forward(self, v):
        """
        Args:
            v: (batch_size, dim) raw embedding
        Returns:
            direction: (batch_size, dim) unit vector
            magnitude: (batch_size, 1) log-norm
        """
        norm = torch.norm(v, p=2, dim=1, keepdim=True)  # (B, 1)
        direction = v / (norm + self.eps)  # (B, D)
        magnitude = torch.log(norm + self.eps)  # (B, 1)
        return direction, magnitude
```

#### 9.1.2 Hadamard èåˆ

```python
class HadamardFusion(nn.Module):
    def __init__(self, embed_dim, mlp_dims, dropout=0.1):
        super().__init__()
        # Input: [d_img, d_txt, p_dir, m_img, m_txt]
        input_dim = embed_dim * 3 + 2  # 3*768+2 for base
        
        layers = []
        prev_dim = input_dim
        for hidden_dim in mlp_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        self.mlp = nn.Sequential(*layers)
    
    def forward(self, d_img, d_txt, m_img, m_txt):
        """
        Args:
            d_img: (B, D) image direction
            d_txt: (B, D) text direction
            m_img: (B, 1) image magnitude
            m_txt: (B, 1) text magnitude
        Returns:
            z: (B, mlp_dims[-1]) fused embedding
        """
        p_dir = d_img * d_txt  # Hadamard product
        x = torch.cat([d_img, d_txt, p_dir, m_img, m_txt], dim=1)
        z = self.mlp(x)
        return z
```

#### 9.1.3 Hash å±¤èˆ‡æ­£å‰‡åŒ–

```python
class HashLayer(nn.Module):
    def __init__(self, input_dim, hash_bits):
        super().__init__()
        self.fc = nn.Linear(input_dim, hash_bits)
        self.hash_bits = hash_bits
    
    def forward(self, z):
        """Returns soft hash codes in [-1, 1]"""
        h = torch.tanh(self.fc(z))
        return h
    
    def binarize(self, h):
        """For inference: convert to hard binary {-1, 1}"""
        return torch.sign(h)

def hash_regularization(h, lambda_balance=0.1, lambda_decorr=0.01):
    """
    Args:
        h: (batch_size, hash_bits) soft hash codes
    Returns:
        loss_hash: scalar tensor
    """
    # 1. Quantization loss
    loss_quant = torch.mean((torch.abs(h) - 1) ** 2)
    
    # 2. Bit balance loss
    bit_mean = torch.mean(h, dim=0)  # (hash_bits,)
    loss_balance = torch.mean(bit_mean ** 2)
    
    # 3. Bit decorrelation loss
    h_centered = h - torch.mean(h, dim=0, keepdim=True)
    cov = (h_centered.T @ h_centered) / h.size(0)  # (B, B)
    loss_decorr = (torch.sum(cov ** 2) - torch.trace(cov ** 2)) / (h.size(1) ** 2)
    
    loss_hash = loss_quant + lambda_balance * loss_balance + lambda_decorr * loss_decorr
    return loss_hash
```

#### 9.1.4 å®Œæ•´æ¨¡å‹

```python
class MultimodalHashKNN(nn.Module):
    def __init__(self, config):
        super().__init__()
        # SigLIP2 encoders
        self.processor = Siglip2Processor.from_pretrained(config.siglip2_variant)
        self.model = Siglip2Model.from_pretrained(config.siglip2_variant)
        
        # âš ï¸ å¿…é ˆå‡çµ towersï¼ˆRTX 5080 16GB é™åˆ¶ï¼‰
        if config.freeze_towers:
            for param in self.model.parameters():
                param.requires_grad = False
            print("âœ“ SigLIP2 towers frozen (saving ~7.5GB VRAM)")
        
        # Decomposer
        self.decomposer = DirectionMagnitudeDecomposer()
        
        # Fusion
        embed_dim = self.model.config.projection_dim  # 768 for base
        self.fusion = HadamardFusion(embed_dim, config.mlp_dims, config.dropout)
        
        # Hash layer
        self.hash_layer = HashLayer(config.mlp_dims[-1], config.hash_bits)
        
        # Classifier head (for training)
        self.classifier = nn.Linear(config.hash_bits, config.num_classes)
        
        self.config = config
    
    def forward(self, images, texts, return_components=False):
        # Encode
        outputs = self.model(pixel_values=images, input_ids=texts)
        v_img = outputs.image_embeds  # (B, D)
        v_txt = outputs.text_embeds   # (B, D)
        
        # Decompose
        d_img, m_img = self.decomposer(v_img)
        d_txt, m_txt = self.decomposer(v_txt)
        
        # Fuse
        z = self.fusion(d_img, d_txt, m_img, m_txt)
        
        # Hash
        h = self.hash_layer(z)
        
        # Classify
        logits = self.classifier(h)
        
        if return_components:
            return {
                'logits': logits,
                'h': h,
                'd_img': d_img,
                'd_txt': d_txt,
                'm_img': m_img,
                'm_txt': m_txt,
                'z': z
            }
        else:
            return logits
    
    def get_hash(self, images, texts):
        """For inference: return hash codes"""
        with torch.no_grad():
            outputs = self.model(pixel_values=images, input_ids=texts)
            v_img = outputs.image_embeds
            v_txt = outputs.text_embeds
            d_img, m_img = self.decomposer(v_img)
            d_txt, m_txt = self.decomposer(v_txt)
            z = self.fusion(d_img, d_txt, m_img, m_txt)
            h = self.hash_layer(z)
        return h
```

### 9.2 è¨“ç·´è¿´åœˆï¼ˆâ­ å«è¨˜æ†¶é«”å„ªåŒ–ï¼‰

```python
import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

def get_gpu_memory_info():
    """ç²å– GPU è¨˜æ†¶é«”ä½¿ç”¨è³‡è¨Š"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        max_allocated = torch.cuda.max_memory_allocated() / 1e9
        return {
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "max_allocated_gb": max_allocated,
            "free_gb": 16.0 - reserved
        }
    return None

def train_epoch(model, dataloader, optimizer, scheduler, config):
    """å„ªåŒ–çš„è¨“ç·´è¿´åœˆï¼ˆé‡å° RTX 5080 16GBï¼‰"""
    
    model.train()
    scaler = GradScaler()  # âš ï¸ æ··åˆç²¾åº¦å¿…é ˆ
    
    total_loss = 0
    total_loss_bce = 0
    total_loss_cos = 0
    total_loss_hash = 0
    
    accumulation_steps = config.training.gradient_accumulation_steps
    
    for batch_idx, batch in enumerate(dataloader):
        # ç§»åˆ° GPUï¼ˆnon_blocking åŠ é€Ÿï¼‰
        images = batch['images'].to('cuda', non_blocking=True)
        texts = batch['texts'].to('cuda', non_blocking=True)
        labels = batch['labels'].to('cuda', non_blocking=True)  # (B, C) multi-hot
        
        # âš ï¸ æ··åˆç²¾åº¦å‰å‘å‚³æ’­
        with autocast(dtype=torch.float16):
            outputs = model(images, texts, return_components=True)
            logits = outputs['logits']
            h = outputs['h']
            d_img = outputs['d_img']
            d_txt = outputs['d_txt']
            
            # è¨ˆç®—å„é …æå¤±
            loss_bce = F.binary_cross_entropy_with_logits(logits, labels.float())
            loss_cos = 1 - F.cosine_similarity(d_img, d_txt, dim=1).mean()
            loss_hash = hash_regularization(
                h, 
                config.loss.hash_reg.lambda_balance,
                config.loss.hash_reg.lambda_decorr
            )
            
            # çµ„åˆæå¤±
            loss = (
                config.loss.bce_weight * loss_bce + 
                config.loss.cosine_weight * loss_cos + 
                config.loss.hash_weight * loss_hash
            )
            loss = loss / accumulation_steps  # âš ï¸ æ¢¯åº¦ç´¯ç©
        
        # åå‘å‚³æ’­
        scaler.scale(loss).backward()
        
        # âš ï¸ æ¢¯åº¦ç´¯ç©ï¼šæ¯ N æ­¥æ›´æ–°ä¸€æ¬¡
        if (batch_idx + 1) % accumulation_steps == 0:
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), 
                config.training.gradient_clip_norm
            )
            
            # æ›´æ–°åƒæ•¸
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        # ç´¯ç©æå¤±ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
        total_loss += loss.item() * accumulation_steps
        total_loss_bce += loss_bce.item()
        total_loss_cos += loss_cos.item()
        total_loss_hash += loss_hash.item()
        
        # âš ï¸ å®šæœŸç›£æ§è¨˜æ†¶é«”
        if batch_idx % 100 == 0:
            mem_info = get_gpu_memory_info()
            print(f"Batch {batch_idx}/{len(dataloader)}: "
                  f"Loss={loss.item():.4f}, "
                  f"GPU Memory: {mem_info['allocated_gb']:.2f}GB / 16GB "
                  f"({mem_info['allocated_gb']/16*100:.1f}%)")
            
            # âš ï¸ è­¦å‘Šæ©Ÿåˆ¶
            if mem_info['allocated_gb'] > config.memory_optimization.alert_vram_threshold_gb:
                print(f"âš ï¸  WARNING: GPU memory usage high! "
                      f"{mem_info['allocated_gb']:.2f}GB / 16GB")
        
        # âš ï¸ å®šæœŸæ¸…ç†å¿«å–
        if batch_idx % config.memory_optimization.empty_cache_steps == 0:
            torch.cuda.empty_cache()
    
    scheduler.step()
    
    # è¿”å›å¹³å‡æå¤±
    n_batches = len(dataloader)
    return {
        'total': total_loss / n_batches,
        'bce': total_loss_bce / n_batches,
        'cos': total_loss_cos / n_batches,
        'hash': total_loss_hash / n_batches
    }
```

### 9.3 é©—è­‰è¿´åœˆ

```python
@torch.no_grad()
def validate(model, dataloader, config):
    """é©—è­‰è¿´åœˆ"""
    model.eval()
    
    total_loss = 0
    all_logits = []
    all_labels = []
    
    for batch in dataloader:
        images = batch['images'].to('cuda', non_blocking=True)
        texts = batch['texts'].to('cuda', non_blocking=True)
        labels = batch['labels'].to('cuda', non_blocking=True)
        
        # âš ï¸ æ··åˆç²¾åº¦æ¨è«–
        with autocast(dtype=torch.float16):
            outputs = model(images, texts, return_components=True)
            logits = outputs['logits']
            d_img = outputs['d_img']
            d_txt = outputs['d_txt']
            h = outputs['h']
            
            # è¨ˆç®—æå¤±
            loss_bce = F.binary_cross_entropy_with_logits(logits, labels.float())
            loss_cos = 1 - F.cosine_similarity(d_img, d_txt, dim=1).mean()
            loss_hash = hash_regularization(h, config.loss.hash_reg.lambda_balance,
                                           config.loss.hash_reg.lambda_decorr)
            
            loss = (config.loss.bce_weight * loss_bce + 
                    config.loss.cosine_weight * loss_cos + 
                    config.loss.hash_weight * loss_hash)
        
        total_loss += loss.item()
        all_logits.append(logits.cpu())
        all_labels.append(labels.cpu())
    
    # åˆä½µæ‰€æœ‰çµæœ
    all_logits = torch.cat(all_logits, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    # è¨ˆç®—æŒ‡æ¨™
    from sklearn.metrics import average_precision_score, f1_score
    
    y_true = all_labels.numpy()
    y_scores = torch.sigmoid(all_logits).numpy()
    y_pred = (y_scores > 0.5).astype(int)
    
    metrics = {
        'loss': total_loss / len(dataloader),
        'mAP': average_precision_score(y_true, y_scores, average='macro'),
        'f1_micro': f1_score(y_true, y_pred, average='micro'),
        'f1_macro': f1_score(y_true, y_pred, average='macro'),
    }
    
    return metrics
```

### 9.4 å®Œæ•´è¨“ç·´è…³æœ¬

```python
# scripts/train.py
import torch
import hydra
from omegaconf import DictConfig
import wandb
from tqdm import tqdm

@hydra.main(config_path="../configs", config_name="hardware/rtx5080_16gb")
def main(config: DictConfig):
    # è¨­å®š seed
    torch.manual_seed(config.experiment.seed)
    
    # åˆå§‹åŒ– wandb
    if config.logging.use_wandb:
        wandb.init(
            project=config.logging.wandb_project,
            entity=config.logging.wandb_entity,
            config=dict(config),
            name=config.experiment.name
        )
    
    # å»ºç«‹æ¨¡å‹
    print("å»ºç«‹æ¨¡å‹...")
    model = MultimodalHashKNN(config.model).cuda()
    
    # é¡¯ç¤ºè¨˜æ†¶é«”è³‡è¨Š
    mem_info = get_gpu_memory_info()
    print(f"æ¨¡å‹è¼‰å…¥å¾Œ GPU è¨˜æ†¶é«”: {mem_info['allocated_gb']:.2f}GB / 16GB")
    
    # å»ºç«‹ DataLoader
    print("å»ºç«‹ DataLoader...")
    train_loader = create_dataloader(config, split='train')
    val_loader = create_dataloader(config, split='val')
    
    # å»ºç«‹ optimizer èˆ‡ scheduler
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.optimizer.lr,
        weight_decay=config.optimizer.weight_decay,
        betas=config.optimizer.betas
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config.training.num_epochs,
        eta_min=config.scheduler.min_lr
    )
    
    # è¨“ç·´è¿´åœˆ
    best_val_map = 0
    patience_counter = 0
    
    for epoch in range(config.training.num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{config.training.num_epochs}")
        print(f"{'='*60}")
        
        # è¨“ç·´
        train_losses = train_epoch(model, train_loader, optimizer, scheduler, config)
        print(f"Train Loss: {train_losses['total']:.4f} "
              f"(BCE: {train_losses['bce']:.4f}, "
              f"Cos: {train_losses['cos']:.4f}, "
              f"Hash: {train_losses['hash']:.4f})")
        
        # é©—è­‰
        val_metrics = validate(model, val_loader, config)
        print(f"Val Loss: {val_metrics['loss']:.4f}, "
              f"mAP: {val_metrics['mAP']:.4f}, "
              f"F1-Micro: {val_metrics['f1_micro']:.4f}, "
              f"F1-Macro: {val_metrics['f1_macro']:.4f}")
        
        # è¨˜éŒ„åˆ° wandb
        if config.logging.use_wandb:
            wandb.log({
                'epoch': epoch,
                'train/loss': train_losses['total'],
                'train/loss_bce': train_losses['bce'],
                'train/loss_cos': train_losses['cos'],
                'train/loss_hash': train_losses['hash'],
                'val/loss': val_metrics['loss'],
                'val/mAP': val_metrics['mAP'],
                'val/f1_micro': val_metrics['f1_micro'],
                'val/f1_macro': val_metrics['f1_macro'],
                'lr': optimizer.param_groups[0]['lr']
            })
        
        # å„²å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['mAP'] > best_val_map:
            best_val_map = val_metrics['mAP']
            patience_counter = 0
            
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_mAP': val_metrics['mAP'],
                'config': dict(config)
            }
            torch.save(checkpoint, f"best_model_epoch{epoch}_mAP{val_metrics['mAP']:.4f}.pth")
            print(f"âœ“ å„²å­˜æœ€ä½³æ¨¡å‹ (mAP: {val_metrics['mAP']:.4f})")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= config.training.early_stopping_patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break
    
    print("\nè¨“ç·´å®Œæˆï¼")
    print(f"æœ€ä½³ Val mAP: {best_val_map:.4f}")

if __name__ == "__main__":
    main()
```

### 9.5 è¨˜æ†¶é«”ç®¡ç†å·¥å…·

```python
# utils/memory_monitor.py

import torch
import psutil
import GPUtil

class MemoryMonitor:
    """è¨˜æ†¶é«”ç›£æ§å·¥å…·"""
    
    def __init__(self, alert_threshold_gb=14.5):
        self.alert_threshold_gb = alert_threshold_gb
        self.peak_vram = 0
    
    def get_stats(self):
        """ç²å–å®Œæ•´è¨˜æ†¶é«”çµ±è¨ˆ"""
        stats = {}
        
        # GPU è¨˜æ†¶é«”
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            max_allocated = torch.cuda.max_memory_allocated() / 1e9
            
            stats['gpu'] = {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'max_allocated_gb': max_allocated,
                'free_gb': 16.0 - reserved,
                'utilization_%': allocated / 16.0 * 100
            }
            
            # æ›´æ–°å³°å€¼
            self.peak_vram = max(self.peak_vram, allocated)
            
            # æª¢æŸ¥æ˜¯å¦è¶…éé–¾å€¼
            if allocated > self.alert_threshold_gb:
                stats['gpu']['alert'] = True
        
        # CPU è¨˜æ†¶é«”
        ram = psutil.virtual_memory()
        stats['cpu'] = {
            'used_gb': ram.used / 1e9,
            'available_gb': ram.available / 1e9,
            'percent': ram.percent
        }
        
        return stats
    
    def print_stats(self, prefix=""):
        """åˆ—å°è¨˜æ†¶é«”çµ±è¨ˆ"""
        stats = self.get_stats()
        
        if 'gpu' in stats:
            gpu = stats['gpu']
            print(f"{prefix}GPU: {gpu['allocated_gb']:.2f}GB / 16GB "
                  f"({gpu['utilization_%']:.1f}%), "
                  f"Peak: {self.peak_vram:.2f}GB")
            
            if gpu.get('alert'):
                print(f"  âš ï¸  WARNING: VRAM usage high!")
        
        cpu = stats['cpu']
        print(f"{prefix}RAM: {cpu['used_gb']:.1f}GB / {42:.1f}GB "
              f"({cpu['percent']:.1f}%)")
    
    def reset_peak(self):
        """é‡ç½®å³°å€¼çµ±è¨ˆ"""
        torch.cuda.reset_peak_memory_stats()
        self.peak_vram = 0

# ä½¿ç”¨ç¯„ä¾‹
monitor = MemoryMonitor(alert_threshold_gb=14.5)

# è¨“ç·´å‰
monitor.print_stats("è¨“ç·´å‰ - ")

# è¨“ç·´ä¸­ï¼ˆå®šæœŸæª¢æŸ¥ï¼‰
for epoch in range(num_epochs):
    for batch_idx, batch in enumerate(train_loader):
        # ... è¨“ç·´ç¨‹å¼ç¢¼ ...
        
        if batch_idx % 100 == 0:
            monitor.print_stats(f"Epoch {epoch}, Batch {batch_idx} - ")
```

---

## 10) è©•ä¼°æŒ‡æ¨™

### 10.1 Multi-label åˆ†é¡æŒ‡æ¨™

#### 10.1.1 Mean Average Precision (mAP)
**å®šç¾©**: å°æ¯å€‹æ¨£æœ¬è¨ˆç®— APï¼Œç„¶å¾Œå–å¹³å‡ã€‚

$$
\text{AP} = \frac{\sum_{k=1}^n P(k) \cdot \text{rel}(k)}{\text{number of relevant labels}}
$$

å…¶ä¸­ $P(k)$ æ˜¯å‰ $k$ å€‹é æ¸¬çš„ precisionï¼Œ$\text{rel}(k)$ æ˜¯ç¬¬ $k$ å€‹é æ¸¬æ˜¯å¦æ­£ç¢ºï¼ˆ0 æˆ– 1ï¼‰ã€‚

**å¯¦ä½œ**:
```python
from sklearn.metrics import average_precision_score

def compute_map(y_true, y_scores):
    """
    Args:
        y_true: (N, C) binary ground truth
        y_scores: (N, C) predicted scores
    Returns:
        mAP: scalar
    """
    return average_precision_score(y_true, y_scores, average='macro')
```

#### 10.1.2 F1-Score (Micro / Macro)
**Micro F1**: æ‰€æœ‰æ¨£æœ¬èˆ‡é¡åˆ¥çµ±ä¸€è¨ˆç®— TP/FP/FN
**Macro F1**: å°æ¯å€‹é¡åˆ¥è¨ˆç®— F1 å¾Œå–å¹³å‡

```python
from sklearn.metrics import f1_score

# éœ€å…ˆå°‡ scores è½‰ç‚º binary predictionsï¼ˆè¨­å®š thresholdï¼‰
y_pred = (y_scores > threshold).astype(int)

f1_micro = f1_score(y_true, y_pred, average='micro')
f1_macro = f1_score(y_true, y_pred, average='macro')
```

#### 10.1.3 Precision@K / Recall@K
**å®šç¾©**: åªè€ƒæ…® Top-K é æ¸¬çš„ precision/recall

```python
def precision_at_k(y_true, y_scores, k=5):
    """Compute precision@k for each sample, then average"""
    precisions = []
    for i in range(len(y_true)):
        top_k_indices = np.argsort(y_scores[i])[-k:]
        relevant = y_true[i, top_k_indices].sum()
        precisions.append(relevant / k)
    return np.mean(precisions)

def recall_at_k(y_true, y_scores, k=5):
    """Compute recall@k for each sample, then average"""
    recalls = []
    for i in range(len(y_true)):
        top_k_indices = np.argsort(y_scores[i])[-k:]
        relevant = y_true[i, top_k_indices].sum()
        total_relevant = y_true[i].sum()
        recalls.append(relevant / total_relevant if total_relevant > 0 else 0)
    return np.mean(recalls)
```

### 10.2 å®Œæ•´è©•ä¼°å‡½æ•¸

```python
def evaluate_comprehensive(model, dataloader, config):
    """å®Œæ•´è©•ä¼°ï¼ˆåŒ…å«å„ç¨®æŒ‡æ¨™ï¼‰"""
    model.eval()
    
    all_labels = []
    all_scores = []
    all_query_times = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            images = batch['images'].to('cuda')
            texts = batch['texts'].to('cuda')
            labels = batch['labels'].cpu().numpy()
            
            # æ¨è«–
            with autocast(dtype=torch.float16):
                logits = model(images, texts)
                scores = torch.sigmoid(logits).cpu().numpy()
            
            all_labels.append(labels)
            all_scores.append(scores)
    
    all_labels = np.vstack(all_labels)
    all_scores = np.vstack(all_scores)
    
    # è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™
    from sklearn.metrics import (
        average_precision_score,
        f1_score,
        precision_score,
        recall_score,
        hamming_loss
    )
    
    y_pred = (all_scores > 0.5).astype(int)
    
    metrics = {
        # ä¸»è¦æŒ‡æ¨™
        'mAP': average_precision_score(all_labels, all_scores, average='macro'),
        'mAP_micro': average_precision_score(all_labels, all_scores, average='micro'),
        
        # F1 scores
        'f1_macro': f1_score(all_labels, y_pred, average='macro'),
        'f1_micro': f1_score(all_labels, y_pred, average='micro'),
        'f1_weighted': f1_score(all_labels, y_pred, average='weighted'),
        
        # Precision & Recall
        'precision_macro': precision_score(all_labels, y_pred, average='macro'),
        'precision_micro': precision_score(all_labels, y_pred, average='micro'),
        'recall_macro': recall_score(all_labels, y_pred, average='macro'),
        'recall_micro': recall_score(all_labels, y_pred, average='micro'),
        
        # Hamming Loss
        'hamming_loss': hamming_loss(all_labels, y_pred),
        
        # Top-K metrics
        'precision@3': precision_at_k(all_labels, all_scores, k=3),
        'precision@5': precision_at_k(all_labels, all_scores, k=5),
        'recall@3': recall_at_k(all_labels, all_scores, k=3),
        'recall@5': recall_at_k(all_labels, all_scores, k=5),
    }
    
    # Per-class metrics (ç”¨æ–¼åˆ†æ)
    per_class_ap = average_precision_score(all_labels, all_scores, average=None)
    per_class_f1 = f1_score(all_labels, y_pred, average=None)
    
    metrics['per_class'] = {
        'AP': per_class_ap,
        'F1': per_class_f1
    }
    
    return metrics
```

---

## 11) ç¡¬é«”ç‰¹å®šå„ªåŒ– (â­ æ–°å¢)

### 11.1 RTX 5080 16GB è¨˜æ†¶é«”åˆ†é…æœ€ä½³å¯¦è¸

#### 11.1.1 è¨˜æ†¶é«”ä½¿ç”¨å»ºè­°

| éšæ®µ | VRAM ä½¿ç”¨ | èªªæ˜ |
|------|-----------|------|
| æ¨¡å‹è¼‰å…¥ | ~3.0 GB | SigLIP2-base + è‡ªå®šç¾©å±¤ |
| è¨“ç·´ (batch=32, FP16) | ~10.2 GB | åŒ…å« optimizer states |
| æ¨è«– (batch=64, FP16) | ~6.5 GB | ç„¡éœ€ optimizer states |
| **å®‰å…¨ä¸Šé™** | **14.5 GB** | ç•™ 1.5GB ç·©è¡ |

#### 11.1.2 å¦‚æœé‡åˆ° OOMï¼Œä¾åºå˜—è©¦

**Level 1: è»Ÿæ€§å„ªåŒ–ï¼ˆç„¡ç²¾åº¦æå¤±ï¼‰**
```python
# 1. é™ä½ batch size
training.batch_size = 16
training.gradient_accumulation_steps = 4

# 2. æ›´é »ç¹æ¸…ç†å¿«å–
memory_optimization.empty_cache_steps = 50
```

**Level 2: ä¸­åº¦å„ªåŒ–ï¼ˆå¾®å¹…ç²¾åº¦æå¤±ï¼‰**
```python
# 3. é™ä½è§£æåº¦
model.max_num_patches = 196  # å¾ 256 é™åˆ° 196

# 4. å•Ÿç”¨ gradient checkpointing
memory_optimization.gradient_checkpointing = true
```

**Level 3: æ¿€é€²å„ªåŒ–ï¼ˆå¯èƒ½å½±éŸ¿ç²¾åº¦ï¼‰**
```python
# 5. é™ä½ hash bits
model.hash.bits = 32  # å¾ 64 é™åˆ° 32

# 6. æ¸›å° fusion MLP
model.fusion.mlp_dims = [512, 256]  # å¾ [1024, 512] é™ä½
```

### 11.2 åˆ©ç”¨ 32 æ ¸å¿ƒ CPU

#### 11.2.1 DataLoader å„ªåŒ–

```python
# å……åˆ†åˆ©ç”¨ 32 æ ¸å¿ƒ
dataloader:
  num_workers: 16  # ä½¿ç”¨ä¸€åŠæ ¸å¿ƒï¼ˆé¿å…éè¼‰ï¼‰
  prefetch_factor: 3  # æ¯å€‹ worker é è¼‰ 3 æ‰¹æ¬¡
  persistent_workers: true  # ä¿æŒ workers å­˜æ´»
  pin_memory: true  # CPU->GPU å‚³è¼¸åŠ é€Ÿ
```

#### 11.2.2 è³‡æ–™é è™•ç†å¹³è¡ŒåŒ–

```python
import multiprocessing as mp

def preprocess_dataset_parallel(data_dir, num_workers=32):
    """å¹³è¡Œé è™•ç†è³‡æ–™é›†"""
    from functools import partial
    
    # ç²å–æ‰€æœ‰å½±åƒè·¯å¾‘
    image_paths = list(Path(data_dir).glob("*.jpg"))
    
    # å®šç¾©è™•ç†å‡½æ•¸
    def process_image(img_path, processor):
        image = Image.open(img_path)
        # ... é è™•ç† ...
        return processed_data
    
    # å¹³è¡Œè™•ç†
    with mp.Pool(processes=num_workers) as pool:
        results = pool.map(
            partial(process_image, processor=processor),
            image_paths
        )
    
    return results
```

### 11.3 CUDA 13.0 ç‰¹å®šå„ªåŒ–

#### 11.3.1 å•Ÿç”¨æ–°ç‰¹æ€§

```python
# å•Ÿç”¨ TF32 (Tensor Float 32)
# RTX 50 ç³»åˆ—æ”¯æ´ï¼Œå¯åŠ é€ŸçŸ©é™£é‹ç®—
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# å•Ÿç”¨ Flash Attention (å¦‚æœå¯ç”¨)
# éœ€è¦ PyTorch 2.0+
torch.backends.cuda.enable_flash_sdp(True)
```

#### 11.3.2 ç·¨è­¯å„ªåŒ–ï¼ˆPyTorch 2.0+ï¼‰

```python
# ä½¿ç”¨ torch.compile åŠ é€Ÿæ¨¡å‹
# éœ€è¦ PyTorch 2.0+
if hasattr(torch, 'compile'):
    model = torch.compile(
        model,
        mode="reduce-overhead",  # æˆ– "max-autotune"
        fullgraph=True
    )
    print("âœ“ Model compiled with torch.compile")
```

### 11.4 é æœŸæ•ˆèƒ½ï¼ˆä½ çš„ç¡¬é«”ï¼‰

#### 11.4.1 è¨“ç·´é€Ÿåº¦ä¼°ç®—

| é…ç½® | é€Ÿåº¦ (iter/s) | æ¯ Epoch | 30 Epochs |
|------|--------------|----------|-----------|
| **baseline (æ¨è–¦)** | ~1.8 | ~35 åˆ†é˜ | **17.5 å°æ™‚** |
| emergency (OOMå‚™æ¡ˆ) | ~2.5 | ~50 åˆ†é˜ | 25 å°æ™‚ |
| é™ä½è§£æåº¦ | ~2.2 | ~30 åˆ†é˜ | 15 å°æ™‚ |

**è¨»**: åŸºæ–¼ RTX 5080 16GB + 32-core CPU + batch_size=32 + FP16

#### 11.4.2 æ¨è«–é€Ÿåº¦ä¼°ç®—

| ä»»å‹™ | é€Ÿåº¦ | èªªæ˜ |
|------|------|------|
| å–®å¼µå½±åƒæ¨è«– | ~30 ms | batch=1, FP16 |
| æ‰¹æ¬¡æ¨è«– (64) | ~1.2 s | batch=64, FP16 |
| KNN æª¢ç´¢ (K=20) | ~0.5 ms | FAISS binary, GPU |
| å®Œæ•´ pipeline | ~35 ms | æ¨è«– + KNN |

**ååé‡**: ~28 images/sec (å–® GPU)

### 11.5 è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤è…³æœ¬

```bash
# scripts/monitor_training.sh
#!/bin/bash

# ç›£æ§è¨“ç·´éç¨‹çš„ GPU ä½¿ç”¨
watch -n 1 '
echo "=== GPU Status ==="
nvidia-smi --query-gpu=timestamp,memory.used,memory.free,utilization.gpu,temperature.gpu --format=csv
echo ""
echo "=== Process Info ==="
nvidia-smi pmon -c 1
'
```

---

## 12) åƒè€ƒæ–‡ç»

### æ ¸å¿ƒæ–¹æ³•
1. **SigLIP 2**: Jiasen Lu, et al. "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features". arXiv:2502.14786, 2025.

2. **MS-COCO Dataset**: Tsung-Yi Lin, et al. "Microsoft COCO: Common Objects in Context". ECCV 2014.

3. **MS-COCO Captions**: Xinlei Chen, et al. "Microsoft COCO Captions: Data Collection and Evaluation Server". arXiv:1504.00325, 2015.

### Hash æ–¹æ³•
4. **Deep Supervised Discrete Hashing**: Qi Li, et al. "Deep Supervised Discrete Hashing". NeurIPS 2017.

5. **HashNet**: Zhangjie Cao, et al. "HashNet: Deep Learning to Hash by Continuation". ICCV 2017.

6. **Learning to Hash Survey**: Jun Wang, et al. "Learning to Hash for Indexing Big Data - A Survey". Proceedings of the IEEE, 2015.

### å¤šæ¨¡æ…‹èåˆ
7. **MCB**: Akira Fukui, et al. "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding". EMNLP 2016.

8. **MUTAN**: Hedi Ben-younes, et al. "MUTAN: Multimodal Tucker Fusion for Visual Question Answering". ICCV 2017.

### KNN èˆ‡ Multi-label
9. **Ranking-based KNN**: Derek Hoiem, et al. "A Ranking-based KNN Approach for Multi-label Classification". AISTATS 2012.

### è¨“ç·´æŠ€å·§
10. **Focal Loss**: Tsung-Yi Lin, et al. "Focal Loss for Dense Object Detection". ICCV 2017.

11. **Mixed Precision Training**: Paulius Micikevicius, et al. "Mixed Precision Training". ICLR 2018.

---

## 13) é™„éŒ„

### é™„éŒ„ A: å®Œæ•´å¯¦é©— Checklist

#### ç’°å¢ƒè¨­ç½®
- [ ] GPU é©—è­‰ (RTX 5080 16GB, CUDA 13.0)
- [ ] Python 3.10+ å®‰è£
- [ ] PyTorch 2.5+ (æ”¯æ´ CUDA 13.0)
- [ ] Transformers 4.47+ (æ”¯æ´ SigLIP2)
- [ ] FAISS-GPU å®‰è£
- [ ] è³‡æ–™é›†ä¸‹è¼‰ (MS-COCO 2014)
- [ ] Karpathy split ä¸‹è¼‰

#### ç¨‹å¼ç¢¼å¯¦ä½œ
- [ ] DirectionMagnitudeDecomposer
- [ ] HadamardFusion
- [ ] HashLayer + regularization
- [ ] MultimodalHashKNN å®Œæ•´æ¨¡å‹
- [ ] è¨“ç·´è¿´åœˆï¼ˆå«è¨˜æ†¶é«”å„ªåŒ–ï¼‰
- [ ] é©—è­‰è¿´åœˆ
- [ ] FAISS index å»ºç«‹
- [ ] KNN æª¢ç´¢èˆ‡æŠ•ç¥¨
- [ ] è©•ä¼°å‡½æ•¸

#### Baseline å¯¦é©—
- [ ] SigLIP2-MLP baseline
- [ ] SigLIP2-ZeroShot baseline
- [ ] è³‡æ–™è™•ç†é©—è­‰

#### Ablation å¯¦é©—
- [ ] Tier 1: A1-A3 (æ ¸å¿ƒæ¶æ§‹)
- [ ] Tier 2: B1-B4 (è¨“ç·´ç­–ç•¥)
- [ ] Tier 3: C1-C4 (KNN è¶…åƒæ•¸)

#### åˆ†æèˆ‡å ±å‘Š
- [ ] å­¸ç¿’æ›²ç·šç¹ªè£½
- [ ] Per-class metrics åˆ†æ
- [ ] KNN é„°å±…è¦–è¦ºåŒ–
- [ ] å¤±æ•—æ¡ˆä¾‹åˆ†æ
- [ ] å¯¦é©—å ±å‘Šæ’°å¯«

### é™„éŒ„ B: æ•…éšœæ’é™¤ï¼ˆRTX 5080 å°ˆç”¨ï¼‰

#### OOM (Out of Memory)

**ç—‡ç‹€**: `RuntimeError: CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**ï¼ˆä¾åºå˜—è©¦ï¼‰:
```bash
# 1. æª¢æŸ¥ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨
python -c "import torch; print(torch.cuda.memory_summary())"

# 2. é™ä½ batch size
# åœ¨é…ç½®æ–‡ä»¶ä¸­: training.batch_size = 16

# 3. å•Ÿç”¨ gradient checkpointing
# åœ¨é…ç½®æ–‡ä»¶ä¸­: memory_optimization.gradient_checkpointing = true

# 4. æ¸…ç† GPU å¿«å–
python -c "import torch; torch.cuda.empty_cache()"

# 5. é‡å•Ÿ Python ç¨‹åº
```

#### è¨“ç·´é€Ÿåº¦æ…¢

**æª¢æŸ¥æ¸…å–®**:
```python
# 1. ç¢ºèªæ··åˆç²¾åº¦å·²å•Ÿç”¨
assert config.memory_optimization.mixed_precision == True

# 2. ç¢ºèª DataLoader ä½¿ç”¨è¶³å¤  workers
assert config.dataloader.num_workers >= 16

# 3. ç¢ºèª pin_memory å·²å•Ÿç”¨
assert config.dataloader.pin_memory == True

# 4. æª¢æŸ¥ GPU åˆ©ç”¨ç‡
# æ‡‰è©²åœ¨ 80-90% ä»¥ä¸Š
!nvidia-smi
```

#### CUDA ç‰ˆæœ¬ä¸åŒ¹é…

**ç—‡ç‹€**: `RuntimeError: CUDA error: no kernel image is available`

**è§£æ±º**:
```bash
# é‡æ–°å®‰è£æ­£ç¢ºç‰ˆæœ¬çš„ PyTorch
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130
```

### é™„éŒ„ C: é æœŸçµæœï¼ˆå‡è¨­ï¼‰

| æ–¹æ³• | mAP | F1-Micro | F1-Macro | è¨“ç·´æ™‚é–“ | è¨˜æ†¶é«” |
|------|-----|----------|----------|----------|--------|
| SigLIP2-ZeroShot | 0.32 | 0.28 | 0.24 | N/A | ~3 GB |
| SigLIP2-MLP | 0.68 | 0.71 | 0.62 | 15 å°æ™‚ | ~8 GB |
| Ours (æ–¹æ¡ˆ A) | 0.66 | 0.69 | 0.60 | 17.5 å°æ™‚ | ~10 GB |
| **Ours (æ–¹æ¡ˆ B, full)** | **0.71** | **0.74** | **0.65** | **17.5 å°æ™‚** | **~10 GB** |

### é™„éŒ„ D: ç¡¬é«”å‡ç´šå»ºè­°ï¼ˆæœªä¾†ï¼‰

å¦‚æœé ç®—å…è¨±ï¼Œä»¥ä¸‹å‡ç´šå¯ä»¥æå‡æ•ˆèƒ½ï¼š

| å‡ç´š | é æœŸæ”¹å–„ | æˆæœ¬ |
|------|---------|------|
| **RTX 5090 (32GB)** | batch_size å¯é” 64 (ç„¡éœ€ç´¯ç©) | $$$$ |
| **å¢åŠ  RAM è‡³ 64GB** | æ›´å¿«çš„è³‡æ–™é è™•ç† | $$ |
| **NVMe RAID 0** | æ›´å¿«çš„è³‡æ–™è®€å– | $$ |
| **ç¬¬äºŒå¼µ GPU** | å¹³è¡Œè¨“ç·´å¤šçµ„å¯¦é©— | $$$$ |

### é™„éŒ„ E: å¿«é€Ÿå•Ÿå‹•å‘½ä»¤

```bash
# 1. å•Ÿå‹•ç’°å¢ƒ
cd ~/projects/siglip2-multimodal-hash
source .venv/bin/activate

# 2. æª¢æŸ¥ GPU
python -c "import torch; print(torch.cuda.get_device_name(0))"

# 3. åŸ·è¡Œ baseline è¨“ç·´
python scripts/train.py \
  --config configs/hardware/rtx5080_16gb.yaml \
  experiment.name=baseline_rtx5080

# 4. ç›£æ§è¨“ç·´ï¼ˆå¦é–‹çµ‚ç«¯ï¼‰
watch -n 1 nvidia-smi

# 5. ä½¿ç”¨ wandb ç›£æ§
# é–‹å•Ÿç€è¦½å™¨: https://wandb.ai/your-username/siglip2-multimodal-hash
```

---

## çµèª

æœ¬å¯¦é©—è¨ˆç•«é‡å°ä½ çš„ **RTX 5080 16GB** ç¡¬é«”é€²è¡Œäº†å…¨é¢å„ªåŒ–ï¼š

### âœ… ä¸»è¦å„ªåŒ–é»
1. **Batch size**: 64 â†’ 32ï¼ˆé…åˆæ¢¯åº¦ç´¯ç©ï¼‰
2. **æ··åˆç²¾åº¦**: å¾å»ºè­°è®Šç‚ºå¿…é ˆï¼ˆç¯€çœ 40% VRAMï¼‰
3. **è¨˜æ†¶é«”ç›£æ§**: æ–°å¢å¯¦æ™‚è¿½è¹¤èˆ‡è­¦å‘Š
4. **CPU å„ªåŒ–**: å……åˆ†åˆ©ç”¨ 32 æ ¸å¿ƒï¼ˆ`num_workers=16`ï¼‰
5. **ç·Šæ€¥æ–¹æ¡ˆ**: æä¾› OOM æ™‚çš„é™ç´šç­–ç•¥

### ğŸ“Š é æœŸæ•ˆèƒ½
- è¨“ç·´é€Ÿåº¦: ~1.8 iter/s
- æ¯ epoch: ~35 åˆ†é˜
- å®Œæ•´è¨“ç·´: **~17.5 å°æ™‚**
- VRAM ä½¿ç”¨: **~10.2 GB / 16 GB** (å®‰å…¨ç¯„åœ)

### ğŸ¯ ä¸‹ä¸€æ­¥
1. æŒ‰ç…§ `setup_guide.md` è¨­ç½®ç’°å¢ƒ
2. ä½¿ç”¨æœ¬æ–‡ä»¶çš„å„ªåŒ–é…ç½®
3. åŸ·è¡Œç¬¬ä¸€è¼ª baseline è¨“ç·´
4. å¯†åˆ‡ç›£æ§ VRAM ä½¿ç”¨

ç¥å¯¦é©—é †åˆ©ï¼æœ‰ä»»ä½•å•é¡Œéš¨æ™‚è©¢å•ã€‚ğŸš€
